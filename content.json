[{"title":"Ubuntu安装VmWare遇到问题","date":"2020-01-01T02:06:00.000Z","path":"2020/01/01/Ubuntu安装VmWare遇到问题/","text":"一、Ubuntu安装Vmware1234567891011121314151617cd ~/下载/软件安装包#赋予可执行权限sudo chmod +x VMware-Player-15.1.0-13591040.x86_64.bundle#安装sudo ./VMware-Player-15.1.0-13591040.x86_64.bundle#创建虚拟机的时候，如果遇到vmmode is defined表示没有这个模块#还需要在bios设置，把secure boot设置为disabledsudo vmware-modconfig --console --install-allsudo apt-get install libcanberra-gtk-module#当vmware报错为Cannot open /dev/vmmon: No such file or directory. Please make sure that the kernel module `vmmon' is loaded#设置秘钥openssl req -new -x509 -newkey rsa:2048 -keyout MOK.priv -outform DER -out MOK.der -nodes -days 36500 -subj \"/CN=VMware/\"#然后在内核装sudo /usr/src/linux-headers-`uname -r`/scripts/sign-file sha256 ./MOK.priv ./MOK.der $(modinfo -n vmmon)#安装MOKsudo /usr/src/linux-headers-`uname -r`/scripts/sign-file sha256 ./MOK.priv ./MOK.der $(modinfo -n vmnet)mokutil --import MOK.der 二、打开Vmware发现报错1.、Ubuntu解决vmware Unable to start services 分析原因：由于Ubuntu18.04更新升级模块导致，所以需要下载对应VmWare版本的模块。 2、shell脚本下载对应模块12345678910111213141516#!/bin/bash#workstation-15.1.0，我自己用的版本是15.1.0的，所以跟写这个版本，然后才能从github下载对应版本的模块信息。VMWARE_VERSION=workstation-15.1.0 #This needs to be the actual name of the appropriate branch in mkubecek's GitHub repo for your purposesTMP_FOLDER=/tmp/patch-vmwarerm -fdr $TMP_FOLDERmkdir -p $TMP_FOLDERcd $TMP_FOLDERgit clone https://github.com/mkubecek/vmware-host-modules.git #Use `git branch -a` to find all available branches and find the one that's appropriate for youcd $TMP_FOLDER/vmware-host-modulesgit checkout $VMWARE_VERSIONgit fetchmakesudo make installsudo rm /usr/lib/vmware/lib/libz.so.1/libz.so.1sudo ln -s /lib/x86_64-linux-gnu/libz.so.1 /usr/lib/vmware/lib/libz.so.1/libz.so.1systemctl restart vmware &amp;&amp; vmware &amp;","link":"","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://victorblog.github.io/tags/DevOps/"}]},{"title":"Hive开发总结与问题复盘","date":"2019-12-11T02:06:00.000Z","path":"2019/12/11/开发总结与问题复盘/","text":"一、今日头条广告预警、暂停以及加量 开始时间：11-22 预计完成时间： 12-21 二、中间遇到的问题1、业务需求方面 由于广告预警，加量，每小时监控，需要的是每小时跑一次数据，而由于之前的load_toutiao_data的job的接口拉取的还是每天汇总计算好的数据，没能对头条开发者技术文档仔细研读，导致业务理解出现偏差，后来与市场部进行多次沟通交流，最终确定新的prd需求。需要每天的每个小时的明细数据，改动接口传参，重新dump头条的广告明细数据。 由于之前只是对头条广告明细数据，和广告数据进行了每小时拉取，没有对广告组数据每小时处理，数据产生偏差比较大。 广告暂停是每天跑一次，自然而然用到的是每天的数据，之前没能有深刻理解，使用的每小时的明细数据进行计算聚合，导致数据计算少，发现后及时修改逻辑。 当初自己理解为广告暂停是每天跑一次，用的是昨天的一整天的历史数据。后来又和市场再三确认，更改为计算的每个小时的明细数据然后进行累计计算。因为计算到当前时间的48小时内的累计消耗，后又进行重构更改job。耗费了大量的时间 2、SQL方面 首先是自己没能深刻认识Group By和Distinct的应用，把维度和指标的字段都进行group by分组，导致二次犯错。后经老师傅指导批评，重新学习SQL语法，sql方面问题GROUP BY和DISTINCT的使用错误，需要深刻反思。 dm_toutiao_adstat_hourly是存放的广告计划明细小时数据，需要按照广告计划ad_id和stat_datetime小时标签来进行分组后，然后计算相关指标。 1234567891011121314151617--插入数据ALTER TABLE hdp_drq_dw_db.da_toutiao_adstat_hourly drop PARTITION(cal_dt =$&#123;today&#125;,hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;') ;insert overwrite table hdp_drq_dw_db.da_toutiao_adstat_hourly partition (cal_dt =$&#123;today&#125;,hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;')selectad_id,stat_datetime,sum(cost) as cost,sum(convert) as convert,sum(case when convert&lt;=0.0 then 0 else cost end)/sum(case when convert&lt;=0.0 then 1 else convert end) as convert_cost,max(advertiser_id) as advertiser_id,max(campaign_id) as campaign_idfrom hdp_drq_dw_db.dm_toutiao_adstat_hourlywhere cal_dt = $&#123;today&#125; and hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;'group byad_id,stat_datetime; 头条公司账户，广告计划成本，广告计划状态明细汇总中间表，进行关联的时候出现错误，导致数据发散。 反思是业务逻辑理解出现问题，需要的是ad_id,advertiser_id,campaign_id三者的关系理解。首先是账户advertiser_id对应多个campagin_id，然后campaign_id对应多个广告ad_id。再者，之前是用广告组每天的数据表进行关联，由于广告的开启，暂停等状态受广告组限制，所以广告组也需要使用每小时的明细数据来进行业务逻辑处理，筛选出广告组状态为开启状态status=’CAMPAIGN_STATUS_ENABLE’ 123456789101112131415161718192021222324252627282930313233--插入数据alter table hdp_drq_dw_db.dm_toutiao_company_ad_adstat_hourly drop PARTITION(cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;',hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;') ;insert overwrite table hdp_drq_dw_db.dm_toutiao_company_ad_adstat_hourly PARTITION(cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;',hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;')selectc.company,c.name as company_name,a.ad_id,b.os,a.advertiser_id,b.status,b.opt_status,b.start_time,b.name,a.cost,a.convert_cost,b.budget,b.budget_mode,a.convert,c.operate_flag,c.monitor_flag,a.stat_datetimefrom hdp_drq_dw_db.da_toutiao_adstat_hourly aleft outer join hdp_drq_dw_db.dm_toutiao_ad_hourly b on a.ad_id=b.ad_id and b.cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125;'&#125;' and b.hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;'left outer join hdp_drq_dw_db.dw_toutiao_account c on a.advertiser_id=c.advertiser_id and c.cal_dt=$&#123;dealDate&#125;/*此处关联后发现数据量明显变少，具体问题待排查，可能是load_toutiao_data_hourly拉取的广告组数据不完整。正常逻辑是需要用广告组id是关联的，然后只有在广告组开启的状态下，才能有广告计划的开启关闭意义。*/--left outer join hdp_drq_dw_db.dm_toutiao_campaign_hourly d on a.campaign_id=d.campaign_id and d.cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125;'&#125;' and d.hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;'where a.cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;' and a.hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;'anda.stat_datetime&lt;='$bash&#123;date -d\"$&#123;todayDateTime&#125; 1 hour ago\" +\"%F %H:%M:%S\"&#125;'--and d.status='CAMPAIGN_STATUS_ENABLE'; 去重汇总广告计划的da层表da_toutiao_adstat_hourly 12345678910111213141516171819202122232425262728293031323334--插入数据ALTER TABLE hdp_drq_dw_db.da_toutiao_adstat_hourly drop PARTITION(cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;',hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;') ;insert overwrite table hdp_drq_dw_db.da_toutiao_adstat_hourly partition (cal_dt = '$bash&#123;date +%Y-%m-%d -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;',hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125; 1 hour ago'&#125;')select a.ad_id,a.stat_datetime,sum(a.cost) as cost,sum(a.convert) as convert,case when sum(a.convert)=0 then 0 else sum(a.cost)/sum(a.convert) end as convert_cost,max(a.advertiser_id) as advertiser_id,max(a.campaign_id) as campaign_idfrom ( --由于存在数据重复，所以需要用ad_id,stat_datetime,inventory字段进行group by去重 select ad_id ,stat_datetime ,inventory ,max(cost) as cost ,max(convert) as convert ,max(advertiser_id) as advertiser_id ,max(campaign_id) as campaign_id from hdp_drq_dw_db.dm_toutiao_adstat_hourly --dm_toutiao_adstat_hourly表当前时间的数据前一个小时的数据肯定是完整的。 where cal_dt = $&#123;today&#125; and hour='$bash&#123;date +%H -d '$&#123;todayDateTime&#125;'&#125;' --此处逻辑至关重要，直接过滤1 hour ago的数据存到前一个小时分区。 and stat_datetime&lt;='$bash&#123;date -d\"$&#123;todayDateTime&#125; 1 hour ago\" +\"%F %H:%M:%S\"&#125;' group by ad_id ,stat_datetime ,inventory ) agroup bya.ad_id,a.stat_datetime; 每小时监控的MiniReport的某些数据指标需要为整数，或者为百分数保留两位小数 拉取头条返回的数据里的convert_cost是不能使用的，需要我们自己去计算 使用convert_cost=sum(cost)/sum(convert) 由于convert可能会有0的情况，所以需要处理convert_cost为0 1,case when sum(COALESCE(`convert`,0))=0 then 0.0 else cast((sum(COALESCE(cost,0))/sum(COALESCE(`convert`,0))) as decimal(10,2)) end as convert_cost 1234567891011121314151617 select --广告计划的convert转化数必须为整数，所以要转为为int ,cast(round(a.convert,0) as int) as convert--其他的数据指标保留2位小数 ,round(a.convert_cost,2) as convert_cost --当日-昨日差值 ,round(a.cost-b.cost,2) as sub_cost ,cast(round(a.convert,0) as int)-cast(round(b.convert,0) as int) as sub_convert ,round(a.convert_cost-b.convert_cost,2) as sub_convert_cost --转化日环比 --环比计算公式=（当前的数量/昨天的数量-1）×100% --日环比为百分数，需要处理 ,case when b.convert=0 then '0.0%' else concat(cast (cast((a.convert/b.convert-1)*100 as decimal(18,2)) as string),'%') end as convert_rhb --转化成本日环比 ,case when b.convert_cost=0 then '0.0%' else concat(cast(cast((a.convert_cost/b.convert_cost-1)*100 as decimal(18,2)) as string),'%') end as convert_cost_rhb from xxx_table ; 补跑历史的数据问题 load_toutiao_data_hourly只需要跑一次就能把历史的全量数据给拉取下来 dump的时候也是只需要dump一次，然后再dm层表，把所有的数据都insert进行就可以。这样就是24个小时分区都是有历史24小时的消耗明细数据。然后da层表利用stat_datetime进行过滤到小时分区的数据就可以。 dump的job，从192.168.1.1:1700服务器上拉取txt数据文件，是每一个txt文件都会去建立连接，然后MR当初底层设置默认是200多个连接。直接dump的job设置为并行跑，一次性选择跑多个历史会refused connection，原因是192.168.1.1:1700服务器限制连接数不够了。 3、接口方面 首先是ToutiaoAPI.getAdReport接口的更改 需要改group by传参，按照时间，id，广告位分组，获取每个小时的明细数据。 1234if (\"hourly\".equals(flag)) &#123; request.put(\"group_by\", \"[\\\"STAT_GROUP_BY_FIELD_ID\\\",\\\"STAT_GROUP_BY_FIELD_STAT_TIME\\\",\\\"STAT_GROUP_BY_INVENTORY\\\"]\"); request.put(\"time_granularity\", \"STAT_TIME_GRANULARITY_HOURLY\"); &#125; 拉取数据的接口更改 由于每小时跑的接口，需要大量调用toutiaoAPI，串行运行会导致job拉取时间过长，一个小时跑出结果可能不行，影响后续逻辑计算的job，后改为多线程并行调度，初步使用newFixedThreadPool(30)，30个core线程数来调用，进行代码优化。但是跑的时候，后来查看log日志，出现调用头条API接口too frequently太频繁。提工单和头条技术人员请教后，每秒并发最大量为20个。后调整线程池核心线程为10个，防止再次出现错误。 并发拉取的时候，发现数据出现重复问题，以及数据没能保证所有线程池所有线程分配的任务都进行完毕再退出，直接调用executor.shutdown()，后来进行重新研究并本地测试线程池使用，在业务逻辑中加入。 12345678910111213public void saveAdStats(String date, String flag) throws Exception &#123; for (ToutiaoAccount toutiaoAccount : accounts) &#123; ... executor.submit(() -&gt; &#123; //业务逻辑 &#125;); &#125; executor.shutdown(); //如果没有终止，让线程等待100ms while(!executor.isTerminated()) &#123; Thread.sleep(100); &#125;&#125; 为了保证数据准确无误，则da层表存的数据是’$bash{date +%Y-%m-%d -d ‘${todayDateTime} 1 hour ago’}’的数据。需要单独处理23的数据，来作为第二天00点来单独进行拉取。也就是job当前的运行时间1 hour ago，实际存的00点到当前时间1 hour ago的数据集。接口需要处理这种特殊情况 首先是load_toutiao_data_hourly的job的入口传参时间 1$bash&#123;date +%Y-%m-%d-%H -d '$&#123;todayDateTime&#125;'&#125; 需要单独处理的saveAdStat2File接口的业务逻辑代码 123456789101112/** * getAdReport接口只支持日期为yyyy-MM-dd格式，需要处理 */ if (\"hourly\".equals(flag)) &#123; String hour = date.substring(11, 13); date = date.substring(0, 10); //时间为00点的时候，去拉前一天的数据作为23点的数据 if (\"00\".equals(hour)) &#123; //传入job参数的日期，防止补历史出错 date = Util.dateReduce(date,-1); &#125; &#125; saveAdState2File接口的bug修改 拉取头条广告的时候，只拉取的第一页的900条数据，可能会出现潜在性的bug，后改掉拉取totalPage的所有广告计划 12345678910111213141516171819202122232425for (List&lt;Long&gt; arrayList : lists) &#123; //首先获取第一页的数据，来获取接口返回的totalpage。来把所有的广告计划数据都进行拉取下来 JSONObject report = touTiaoAPI.getAdReport(advertiserId, date, date, 1, pageSize, arrayList, true, flag); JSONArray jsonArray = new JSONArray(); if (report == null || report.isEmpty()) &#123; continue; &#125; int code = Integer.parseInt(Optional.ofNullable(report.getString(\"code\")).orElse(\"40001\")); //根据头条接口返回的状态码进行判断，如果大于0的都为异常情况 if (code &gt; 0) &#123; log.error(\"拉取头条广告计划明细数据异常：&#123;&#125;\", report); &#125; else &#123; //把拉取的第一页广告计划处理，然后存到array里 JSONArray objects = Optional.ofNullable(report.getJSONObject(\"data\").getJSONArray(\"list\")).orElse(new JSONArray()); jsonArray.addAll(objects); //获取广告计划总页码 Integer totalPage = Optional.of(report.getJSONObject(\"data\").getJSONObject(\"page_info\").getIntValue(\"total_page\")).orElse(1); //如果有大于1页的数据，则需要拉取所有的 if (totalPage &gt; 1) &#123; for (int i = 2; i &lt;= totalPage; i++) &#123; report = touTiaoAPI.getAdReport(advertiserId, date, date, i, pageSize, arrayList, true, flag); JSONArray data = Optional.ofNullable(report.getJSONObject(\"data\").getJSONArray(\"list\")).orElse(new JSONArray()); jsonArray.addAll(data); &#125; &#125; &#125; 广告计划加量的接口逻辑 自己理解和导师给的suggestion出现了偏差，在代码里计算判断加量的逻辑，犯了形而上学的错误，后更改为在Hive里新增一个newBudget，判断逻辑进行汇总计算。然后dump到MySQL数据库，进行查询后比较来进行是否加量。 4、页面方面重新学习Thymeleaf，以及分页相关问题。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Spark Streaming的使用","date":"2018-10-05T02:06:00.000Z","path":"2018/10/05/Spark-Streaming/","text":"一、Spark Streaming1、使用背景​ 有时候需要实时处理收到的数据，比如实时追踪页面访问统计的应用，训练机器学习模型，自动检测异常等。Spark Streaming是Spark为这些应用而设计的模型。让用户使用一套和批处理非常接近的API来编写流式计算应用，可以大量重用批处理应用的技术和代码。 ​ 和Spark基于RDD的类似，Spark Streaming使用离散化流作为抽象表示，叫做DStream。而DStream是随时间推移而收到的数据的序列。DStream内部每个时间区间收到的数据都作为RDD的存在，DStream是由这些RDD所组成的序列，因此是离散化。DStream可以从各种输入源创建，常见的是Flume，Kafka，HDFS。创建出来的DStream支持2种操作，一种是转化操作Transformation，会生成新的DStream。一种是输出操作Action。可以把数据写入外部系统中。DStream提供了许多与RDD所支持的操作，同时还增加了与时间相关的新操作，滑动窗口。 ​ 与批处理Job不同的是，Spark Streaming应用需要进行额外配置来保证24小时不间断工作。比如checkpoint机制，也就是把数据存储到可靠文件系统HDFS上的机制，这也是Spark Streaming用来实现不间断工作的主要方式。 2、实例​ Spark Streaming程序最好是使用Maven 或者Sbt编译的独立应用形式运行。Spark Streaming虽然是Spark的一部分。 pom.xml依赖文件 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt; Java流计算import","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Spark的使用","date":"2018-10-04T02:06:00.000Z","path":"2018/10/04/Spark的使用/","text":"一、Spark Core的使用1、下载并安装Spark下载链接 123456789101112131415161718192021#安装spark相关命令cd /usrsudo mkdir sparkcd ~/下载/软件安装包sudo mv spark-2.4.4-bin-hadoop2.7.tgz /usr/spark/cd /usr/spark/sudo tar -zxvf spark-2.4.4-bin-hadoop2.7.tgzcd spark-2.4.4-bin-hadoop2.7/#配置spark相关文件,切换到conf目录cd conf/#复制一个模板sh文件，设置spark的cp spark-env.sh.template spark-env.sh#配置spark需要的jdk环境，以及使用的内存，核心数export JAVA_HOME=/usr/java/jdk1.8.0_201export SPARK_MASTER_IP=SparkMasterexport SPARK_WORKER_MEMORY=2gexport SPARK_WORKER_CORES=2export SPARK_WORKER_INSTANCES=1cp slaves.template slaves #配置spark的slavevim slaves 2、首先要有jdk，scala1234#修改当前用户的环境变量文件vim ~/.bashrc#保存source ~/.bashrc 3、操作spark并启动1234567891011121314151617cd /sbin#如果发现报错，拒绝连接本地，则说明没有安装ssh#启动spark的简单集群./start-all.sh#发现还是一样，拒绝连接本地ssh localhost#发现只有ssh-agent，而没有sshdps -e|grep ssh#安装sshsudo apt updatesudo apt upgradesudo apt install openssh-serverps -e|grep ssh#重新启动./start-all.sh#打开浏览器访问lhttp://victor:8081/ 启动spark-shell 12345cd bin/#python下的spark shellpyspark#scala下的spark shellspark-shell 1、简介 Spark流是对于Spark核心API的扩展，从而支持对于实时数据流的可扩展，高吞吐量和容错性流处理。数据可以由多个源取的，比如Kafka，Flume，ZeroMQ，或者TCP接口等。可以同时使用比如map，reduce，join和window这样的高层接口描述的复杂算法进行处理。最终，处理过得数据可以保存到HDFS，数据库等。 在内部，Spark Streaming接收到实时数据流同时将其划分为分批，也就是微批处理。这些数据的分批将会被Spark的引擎所处理而生成同样按批次形式的最终流。 Spark Streaming提供了被成为离散化或者DStream的高层抽象，这个高层抽象用于表示数据的连续流。 创建DStream的2种方式： 由Kafka，Flume(一个日志收集中间件)，取的数据作为输入流 在其他DStream进行的高层操作。 在内部，DStream被表达为RDDs的一个序列。 2、Spark的组件介绍2.1、Spark Core​ 实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中包含了对RDD(弹性分布式数据集)的API定义。 ​ RDD表示分布在多个计算节点上可以并行操作的元素集合，是Spark的主要编程抽象。Spark Core提供了创建和操作这些集合的多个API。 2.2、SparkSQL​ SparkSQL是Spark用来操作结构化数据的程序包。通过SparkSQL，可以使用SQL或者Hive的HQL来查询数据。SparkSQL支持多种数据源，比如Hive表，JSON等。 ​ SparkSQL除了提供一个SQL接口，还支持将SQL和传统的RDD编程的数据相结合的操作。 2.3、Spark Streaming​ Spark Streaming是Spark提供对实时数据进行流式计算的组件。比如生产环境的网页服务器日志，或者用户提交的状态更新组成的消息队列，都是数据流。Spark Streaming提供了用来操作数据流的API。与Spark Core中的RDD API高度对应。不论是操作内存或者硬盘的数据，还是操作实时的数据流，都可以应对自如。Spark Streaming支持与Spark Core同级别的容错性、吞吐量以及可伸缩性。 2.4、MLib​ Spark中的一个提供常见机器学习ML的库。提供了很多机器学习算法，包括分类、回归、聚类、协同过滤等。还提供了模型评估、数据导入等额外的支持功能功能。MLib还提供了一些更底层的机器学原语，包含一个通用的梯度下降优化算法。 2.5、GraphX​ GraphX是用来操作图的程序库，可以进行并行的图计算。与Spark Streaming和SparkSQL类似。GraphX也扩展了Spark Core的RDD API，用来创建一个顶点和边都包含任艺术型的有向图。 ​ 个人理解：有点类似TensorFlow的TensorBoard。 1.6、集群管理器​ 包括Hadoop YARN，Apache Mesos，以及Spark自带的一个简易调度器，独立调度器。这样可以高效的在一个计算节点到数千个计算节点之前伸缩设计算。如果单独安装Spark，没有任何集群管理的机器，则有子代的独立调度器。或者在装有Hadoop YARN或者Mesos的集群上安装Spark。 1.7、总结​ Spark不仅可以读取HDFS上的文件作为数据集，还支持本地文件，Amazon S3，Cassandra，Hive，HBase等数据集。总之，Hadoop不是Spark的必须必备的。 3、Spark的使用3.1、在Java中使用Spark​ 连接Spark的过程在各个语言中不一样。在Java和Scala中，只需要在pom.xml中添加spark-core的maven依赖。 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; 3.2、初始化SparkContext​ 完成应用项目与Spark的连接，需要在程序中导入Spark的包，并创建SparkContext上下文。 1234567891011121314151617181920import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;public void test3()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); //读取文件里的数据作为输入数据 JavaRDD&lt;String&gt; rdd = javaSparkContext.textFile(\"/inputFile\"); //切分为单词，以空格为分割 JavaRDD&lt;String&gt; words = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //将统计出来的单词总数存入一个文本文件。 counts.saveAsTextFile(\"/outputFile\"); //关闭spark javaSparkContext.stop(); System.out.println(javaSparkContext.version()); &#125; setMaster()：传递集群的URL，告诉Spark如何连接到集群上。 setAppName():当你连接到一个集群时，这个值可以帮你在集群管理器cluster manager的用户界面中找到你的应用。 3.3、总结​ Spark的核心编程，通过一个驱动器SparkConf创建一个SparkContext和一系列的RDD。然后进行并行操作。 4、RDD的使用​ Spark对数据的核心抽象：弹性分布式数据集(Resilient Distributed Dataset)。RDD本质就是分布式的元素集合。在Spark中，对数据的所有操作就是创建RDD，转化已有的RDD和调用RDD进行求值。Spark会自动将RDD中的数据分发到集群，操作并行化执行。 ​ RDD是Spark的核心 4.1、创建RDD​ RDD是一个不可变的分布式对象集合。每个RDD都被分成多个分区，这些分区运行在集群中不同的节点上。 ​ 可以使用2种方法创建RDD： 读取外部数据集 比如使用SparkContext.textFile()来读取文本文件作为一个字符串RDD。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * spark有2种创建RDD的方式：1、读取外部数据集 2、在驱动器程序中对一个集合进行并行化操作。 */ package spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;import java.util.Arrays;import java.util.List;import java.util.regex.Pattern;/** * @Description: * @Author: VictorDan * @Date: 19-11-6 下午2:31 * @Version: 1.0 */public class Spark &#123; private static final Pattern SPACE = Pattern.compile(\" \"); public static void main(String[] args) &#123; SparkConf conf = new SparkConf().setMaster(\"local\").setAppName(\"wc\"); JavaSparkContext context = new JavaSparkContext(conf); JavaRDD&lt;String&gt; lines = context.textFile(\"/home/victor/桌面/file.txt\"); JavaRDD&lt;String&gt; words = lines.flatMap(s -&gt; Arrays.asList(SPACE.split(s)).iterator()); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1)); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?, ?&gt; tuple : output) &#123; System.out.println(tuple._1() + \": \" + tuple._2()); &#125; //spark去重操作 JavaPairRDD&lt;String, Integer&gt; distinct = counts.distinct(); System.out.println(distinct); //spark合并 List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = distinct.collect(); collect.forEach(System.out::println); &#125;&#125; 在驱动程序里分发驱动器程序中的对象集合(list,set) 12345678910111213141516171819202122@Testpublic void parallizeDataCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); List&lt;String&gt; list=new ArrayList&lt;&gt;(); list.add(\"abc\"); list.add(\"i like apple\"); list.add(\"hello world\"); JavaRDD&lt;String&gt; rdd = context.parallelize(list); JavaRDD&lt;String&gt; cache = rdd.cache(); JavaRDD&lt;String&gt; words = cache.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //spark去重操作 JavaPairRDD&lt;String, Integer&gt; distinct = counts.distinct(); //spark合并 List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = distinct.collect(); collect.forEach(System.out::println);&#125; 4.2、操作RDD创建RDD后，支持2种类型操作： transformation转化操作 转化操作会将一个RDD生成一个新的RDD。filter() action行动操作 行动操作first()。Spark会扫描文件直到找到第一个匹配的行为为止，并不一定要读取整个文件 4.2.1、转化操作​ RDD的转化操作是返回新的RDD的操作。转化操作是惰性的，也就是不会改变已有的rdd中的数据。只有在行动操作的时候，rdd才会被计算。 1234//读取文件里的数据作为输入数据,创建rdd JavaRDD&lt;String&gt; rdd = javaSparkContext.textFile(\"/home/victor/桌面/inputFile.txt\"); //转化操作，把创建的rdd，通过filter过滤掉含有abc的，成为新的rdd JavaRDD&lt;String&gt; filter = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"abc\")); filter()操作不会改变rdd中的数据。实际上filter操作会返回一个全新的RDD。rdd在后面的程序中还可以继续使用。 123456//转化操作，把创建的rdd，通过filter过滤掉含有abc的，成为新的rdd JavaRDD&lt;String&gt; filter = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"abc\")); JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\")); //union操作 JavaRDD&lt;String&gt; union = filter.union(errorRDD); union.cache(); union()操作与filter()操作不同在于是操作两个RDD。转化操作可以操作任意数量的输入RDD。 总结：转化操作，可以从已有的RDD中派生出新的RDD，Spark会通过lineage graph谱系图来记录这些不同RDD之间的依赖关系。 4.2.2、行动操作​ 行动操作是第二种类型的RDD操作，会把最终的求得结果返回给程序，或者写入到外部存储系统里。 ​ 比如你的collect()函数操作，只有当你的整个数据集在单机的内存中放得下的时候，才可以使用collect()，不能在大规模数据集上使用。 ​ 一般我们都是把数据写到HDFS或者Amazon S3的分布式文件系统里。或者也可以通过调用saveAsTextFile()，saveAsSequenceFile()等类似外部文件里。 ​ 注意：每次调用一个新的行动操作，整个RDD都会从头开始计算。一般都是将中间结果持久化。 1234567//切分为单词，以空格为分割 JavaRDD&lt;String&gt; words = persist.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //将统计出来的单词总数存入一个文本文件。 counts.saveAsTextFile(\"/home/victor/桌面/outputFile.txt\"); 4.2.3、惰性求值​ RDD的转化操作都是惰性求值，也就是在Spark调用行动操作之前是不是从头开始计算的。 ​ 可以把RDD看做是通过转化操作构建出来的，记录着特定数据的指令列表。把数据读取到RDD的操作也是惰性的。也就是调用sc.textFile()的时候，其实数据并没有读取进来，而是在必要的时候才会读取，而且读取数据的操作可能会执行多次。 ​ Spark使用惰性求职，可以把一些操作合并到一块，从而减少计算数据的步骤。 5、向Spark传递函数5.1、Java中传递方式​ 在java中，函数需要作为实现了Spark的org.apache.spark.api.java.function包中的任一函数接口的对象来传递。 123456789//方式1：使用匿名内部类的方式进行函数传递JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\"));//方式2：通过创建一个具体类，实现接口的方式进行函数传递class ContainsError implements Function&lt;String,Boolean&gt;&#123; @Override public Boolean call(String s) throws Exception &#123; return s.contains(\"error\"); &#125; &#125; 一般大型程序，单独组织类能比较好。而一些简单的直接使用匿名内部类的方式。 6、常见的转化操作和行动操作6.1、转化操作​ 两个最常见的转化操作map()和filter()。 map()：接收一个函数，把这个函数用在RDD中的每个元素，将函数的返回结果作为结果RDD中对应元素的值。 123456789101112@Test public void mapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1,3,2,5)); //计算RDD中各个数值的平方 JavaRDD&lt;Integer&gt; map = rdd.map((Function&lt;Integer, Integer&gt;) x -&gt; x * x); System.out.println(map.collect()); &#125; filter()：接收一个函数，将RDD中满足函数的元素放入到新的RDD中 flatMap()：和map()类似，有时候我们希望对每个输入元素生成多个输出元素。 12345678910111213@Test public void flatMapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;String&gt; rdd = context.parallelize(Arrays.asList(\"hello world\",\"i love you\")); //用flatMap()将数据切分为单词 JavaRDD&lt;String&gt; word = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); String first = word.first(); System.out.println(first);//hello &#125; 集合操作 RDD本身不是严格意义上的集合，但是也支持许多集合操作。比如合并，相交等。 只要唯一的元素distinct() RDD.distinct()操作的开销很大，它需要将所有的数据通过网络进行混洗shuffle，来确保每个元素只有一份。 union,合并操作 subtract:移除一些数据。 intersection:求2个RDD共同的元素 distinct:去重 sample：对RDD采用 1234567891011121314151617181920212223@Test public void flatMapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;String&gt; rdd = context.parallelize(Arrays.asList(\"hello world\",\"i love you\",\"error\")); //去重 JavaRDD&lt;String&gt; distinct = rdd.distinct(); JavaRDD&lt;String&gt; helloRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"hello\")); JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\")); //用flatMap()将数据切分为单词 JavaRDD&lt;String&gt; word = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //移除errorRDD成为新的RDD JavaRDD&lt;String&gt; subtract = rdd.subtract(errorRDD); //求errorRDD与helloRDD的共同的元素 JavaRDD&lt;String&gt; intersection = helloRDD.intersection(errorRDD); //对RDD进行采样，以及是否替换 JavaRDD&lt;String&gt; sample = rdd.sample(false, 0.5); String first = word.first(); System.out.println(first);//hello &#125; 6.2、行动操作​ 最常见的行动操作就是reduce()，接收一个函数作为参数，操作2个RDD的元素类型的数据返回一个同样类型的新元素。 1234567891011121314151617181920212223242526272829/** * 行动操作 */ @Test public void actionCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1,3,2,5)); //计算RDD中各个数值的平方 JavaRDD&lt;Integer&gt; map = rdd.map((Function&lt;Integer, Integer&gt;) x -&gt; x * x); //reduce Integer reduce = map.reduce((Function2&lt;Integer, Integer, Integer&gt;) (a, b) -&gt; a + b); //返回RDD中所有元素 List&lt;Integer&gt; collect = rdd.collect(); //RDD中元素的个数 long count = rdd.count(); //各个元素在RDD中出现的次数 Map&lt;Integer, Long&gt; integerLongMap = rdd.countByValue(); //从RDD中返回2个元素 List&lt;Integer&gt; take = rdd.take(2); //从RDD中返回最前面的2个元素 List&lt;Integer&gt; top = rdd.top(2); //从RDD中按照提供的顺序返回最前面的2个元素 List&lt;Integer&gt; order = rdd.takeOrdered(2); System.out.println(map.collect()); &#125; 6.3、在不同RDD类型间转换12345678910111213141516171819202122public void diffTypeRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1, 3, 2, 5)); //创建DoubleRDD JavaDoubleRDD doubleRDD = rdd.mapToDouble((DoubleFunction&lt;Integer&gt;) x -&gt; x*x); //mean只能用在数值上 Double mean = doubleRDD.mean(); //mean只能用在数值上 Double variance = doubleRDD.variance(); //pairRDD的使用 JavaPairRDD&lt;Integer, Integer&gt; pairRDD = rdd.flatMapToPair((PairFlatMapFunction&lt;Integer, Integer, Integer&gt;) x -&gt; &#123; ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tpLists = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; Tuple2 tp = new Tuple2&lt;&gt;(x, i); tpLists.add(tp); &#125; return tpLists.iterator(); &#125;); &#125; 6.4、总结​ 学习了RDD运行模型和RDD的许多操作，已经学习并了解了Spark Core。我们在进行并行聚合，分组的时候，常常都是以kv的形式RDD。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"大数据离线部分","date":"2018-09-30T02:06:00.000Z","path":"2018/09/30/一、大数据离线部分总结/","text":"一、大数据离线部分总结1、HDFS HDFS的架构部分以及工作原理 NameNode：负责管理元数据，将信息保存在内存中 DataNode：保存数据，以块的形式保存，启动后需要定时的向NameNode发送心跳，报告自身存储的Block块信息。 SecondNameNode：二级节点，负责合并请求。分担NameNode工作。 2、Spark容错机制分布式数据集的容错性有2种方式：数据检查和记录数据的更新。 面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之前复制庞大的数据集，而网络带宽往往比内存带宽低很多，同事还需要消耗更多的存储资源。 3、YARN的总结YARN（Yet Another Resource Negotiator）：另一种资源协调者。 3.1、使用YARN的背景​ 旧版本MapReduce中的JobTracker/TaskTracker在可扩展性、内存消耗、可靠性和线程模型方面存在很多问题，需要开发者做很多调整来修复。后来Hadoop开发者对这些问题进行了修复，可是也因而带来的成本却越来越高，为了从根本上解决旧版本的MapReduce存在的问题，从Hadoop 0.23.0版本开始，Hadoop的MapReduce框架就大改动。Hadoop新的MapReduce框架被叫做MapReduce V2，也叫YARN。 3.2、为什么要使用YARN​ 与旧版本的MapReduce比较，YARN采用了一种分层的集群框架。 解决了NameNode的单点故障问题，可以通过配置NameNode高可用来解决 提出了HDFS联邦，通过HDFS联邦可以让多个NameNode分别管理不同的目录，从而实现访问隔离以及横向扩展。 将资源管理和应用程序管理ApplicationManager分离开。用ResourceManager管理资源，用ApplicationMaster负责管理程序。（YARN的核心思想） 具有向后兼容特点，运行在MR1上的Job不需要做任何修改就可以运行在YARN上。 YARN是一个框架管理器，用户可以将各种计算框架移植到YARN上，统一由YARNJ进行管理和资源调度。目前YARN支持的计算框架有：MapReduce，Storm，Spark，Flink等。 3.2、YARN的基本架构 YARN的核心思想：将功能分开，ResourceManager进程完成整个集群的资源管理和调度。ApplicationMaster进程负责应用程序的相关事务，比如任务调度，容错，任务监控等。 系统中所有应用资源调度的最终决定权是ResourceManager担当的。 每个应用的ApplicationMaster实际上是框架指定的库，它从ResourceManager调度资源，和Node Manager一块执行监控任务。 NodeManager通过心跳信息向ResourceManager汇报自己所在节点的资源使用情况。 在旧版本的MapReduce(MR1)中，JobTracker有2个功能：一个是资源管理，另一个是作业调度。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"sbt的相关问题","date":"2018-09-20T02:30:47.000Z","path":"2018/09/20/sbt的相关问题/","text":"一、sbt下载依赖太慢sbt类似一个maven工具，需要配置环境变量 sbt下载链接 12345678910111213141516cd /usr#创建文件夹sudo mkdir sbtcd ~/下载#移到到创建的/usr/sbt文件夹sudo mv sbt-0.13.18.tgz /usr/sbt#解压文件sudo tar -zxvf sbt-0.13.18.tgz#编辑当前用户环境变量配置vim ~/.bashrc#配置sbt环境配置export SBT_HOME=/usr/sbt/sbtexport PATH=$&#123;SBT_HOME&#125;/bin:$PATH#保存配置cd ~source .bashrc 1、可以本地设置全局代理proxychains12345678#下载并安装全局代理工具 apt install proxychains#cd ~#创建proxychains隐藏文件夹mkdir .proxychains#新建全局代理配置文件touch proxychains.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#编辑全局代理配置文件# proxychains.conf VER 3.1## HTTP, SOCKS4, SOCKS5 tunneling proxifier with DNS.# # The option below identifies how the ProxyList is treated.# only one option should be uncommented at time,# otherwise the last appearing option will be accepted##dynamic_chain## Dynamic - Each connection will be done via chained proxies# all proxies chained in the order as they appear in the list# at least one proxy must be online to play in chain# (dead proxies are skipped)# otherwise EINTR is returned to the app#strict_chain## Strict - Each connection will be done via chained proxies# all proxies chained in the order as they appear in the list# all proxies must be online to play in chain# otherwise EINTR is returned to the app##random_chain## Random - Each connection will be done via random proxy# (or proxy chain, see chain_len) from the list.# this option is good to test your IDS :)# Make sense only if random_chain#chain_len = 2# Quiet mode (no output from library)#quiet_mode# Proxy DNS requests - no leak for DNS dataproxy_dns # Some timeouts in milliseconds#tcp_read_time_out 15000#tcp_connect_time_out 8000# ProxyList format# type host port [user pass]# (values separated by 'tab' or 'blank')### Examples:## socks5 192.168.67.78 1080 lamer secret# http 192.168.89.3 8080 justu hidden# socks4 192.168.1.49 1080# http 192.168.39.93 8080 # ## proxy types: http, socks4, socks5# ( auth types supported: \"basic\"-http \"user/pass\"-socks )#[ProxyList]# add proxy here ...# meanwile# defaults set to \"tor\"socks4 127.0.0.1 9050 2、通过代理重新下载依赖切换到spark-demo项目目录下，然后一个个module模块进行编译。下载依赖。这样导入idea会比较快一点。 12345678cd ~/桌面cd source/cd spark-demo/#通过全局依赖插件，下载sbt依赖proxychains sbt log/assemblyproxychain sbt demo/assemblyproxychains sbt data/assemblyproxychains sbt test/assembly","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Linux安装常用软件","date":"2018-09-03T02:06:00.000Z","path":"2018/09/03/Linux安装常用软件/","text":"一、安装Ubuntu18.04LTS 去老毛桃官网下载软件，制作老毛桃U盘。我的笔记本比较旧，要设置BIOS setup的boot选项为Legacy Support才能进入WIN PE。 利用里面的DiskGenius磁盘分区工具，进行磁盘分区等。 利用软碟通Utral ISO软件进行镜像快捷U盘刻录 安装Ubuntu的时候，设置BIOS，在BIOS SETUP中选择boot 选项，选择boot mode为UEFI，U盘设置为Enabled 然后在Security中设置Secure boot为disabled，否则可能导致安装无线插件的时候报错。 123456#更新软件源sudo apt-get update#安装无线网卡驱动sudo apt install boradcom-sta-dkms#提示有未能满足的依赖关系执行以下命令sudo apt --fix-broken install 二、Ubuntu18.04LTS更换清华的镜像源 先是进行源码备份：cp /etc/apt/sources.list /etc/apt/sources.list.backup 然后修改源列表：sudo vim /etc/apt/sources.list 删除源列表的所有内容，复制如下内容并保存： 清华镜像源官网： 12345678910111213# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 三、Ubuntu18.04LTS安装谷歌浏览器 第一步获取linux的google chrome列表sudo wget http://www.linuxidc.com/files/repo/google-chrome.list -P /etc/apt/sources.list.d/ 第二步获取google的linux密钥wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - 第三步进行镜像源更新sudo apt update 第四步安装获取的google-chromesudo apt install google-chrome-stable 第五步：启动谷歌浏览器 /usr/bin/google-chrome-stable 四、Ubuntu18.04LTS安装Typora123456789# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAEwget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -# add Typora's repositorysudo add-apt-repository 'deb https://typora.io/linux ./'sudo apt-get update# install typorasudo apt-get install typora 五、Ubuntu18.04LTS右键怎么添加新建空白文本文档 第一步：在Terminal终端上切换到用户主目录的模板文件夹里 1cd ~/模板/ 第二步：使用gedit命令打开一个文本文件 1sudo gedit 文本文件 会打开一个空白文件窗口，不要做任何修改直接点击保存 接着在模板文件夹中会多出一个我们创建的文本文件 现在我们可以在桌面上右键，在新建文档中就有文本文件选项。 六、Ubuntu18.04LTS安装NotePad++12345678#Ununtu下的安装notepad方法：sudo add-apt-repository ppa:notepadqq-team/notepadqqsudo apt-get updatesudo apt-get install notepadqq#Ubuntu下的卸载notepad方法：sudo apt-get remove notepadqqsudo add-apt-repository --removeppa:notepadqq-team/notepadqq 七、安装JDK1.8 去Oracle官网下载，点击链接 由于从浏览器下载的东西一般默认都会在下载目录下，所以可以修改一下目录 1234567#切换到/usr目录，然后新建一个java文件夹cd /usrsudo mkdir java#切换到~/下载，然后把刚下在的jdk移动到java文件夹cd ~/下载sudo mv jdk-8u201-linux-x64.tar.gz /usr/java 切换到/usr/java文件夹,解压tar.gz包 12#解压jdksudo tar -zxvf jdk-8u201-linux-x64.tar.gz 设置环境变量 方案一：修改全局配置文件，作用于所有用户： 123456789101112131415#修改配置文件sudo vim /etc/profile#按i进入编辑模式export JAVA_HOME=/usr/java/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH#按ESC后，输入:wq回车#在终端，输入使得修改的配置立刻生效source /etc/profile#检查是否安装成功java -version 方案二：修改当前用户配置文件，只作用与当前用户： 123456789101112131415#修改配置文件vim ~/.bashrc#按i进入编辑模式export JAVA_HOME=/usr/java/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH#按ESC后，输入:wq回车#在终端，输入使得修改的配置立刻生效source ~/.bashrc#检查是否安装成功java -version 八、安装Maven 去官网下载，点击链接 由于从浏览器下载的东西一般默认都会在下载目录下，所以可以修改一下目录 1234567#切换到/usr目录，然后新建一个maven文件夹cd /usrsudo mkdir maven#切换到~/下载，然后把刚下在的移动到maven文件夹cd ~/下载sudo mv apache-maven-3.6.0-bin.tar.gz /usr/maven 切换到/usr/maven文件夹,解压tar.gz包 12#解压mavensudo tar -zxvf apache-maven-3.6.0-bin.tar.gz 设置环境变量 方案一：修改全局配置文件，作用于所有用户： 12345678910111213#修改配置文件sudo vim /etc/profile#按i进入编辑模式export M2_HOME=/usr/maven/apache-maven-3.6.0 export PATH=$&#123;M2_HOME&#125;/bin:$PATH#按ESC后，输入:wq回车#在终端，输入使得修改的配置立刻生效source /etc/profile#检查是否安装成功mvn -v 方案二：修改当前用户配置文件，只作用与当前用户： 12345678910111213#修改配置文件vim ~/.bashrc#按i进入编辑模式export M2_HOME=/usr/maven/apache-maven-3.6.0 export PATH=$&#123;M2_HOME&#125;/bin:$PATH#按ESC后，输入:wq回车#在终端，输入使得修改的配置立刻生效source ~/.bashrc#检查是否安装成功mvn -v 九、安装Intellij IDEA 去IDEA官网下载，点击链接 由于从浏览器下载的东西一般默认都会在下载目录下，所以可以修改一下目录 1234567#切换到/usr目录，然后新建一个idea文件夹cd /usrsudo mkdir idea#切换到~/下载，然后把刚下在的移动到maven文件夹cd ~/下载sudo mv ideaIU-2018.3.4.tar.gz /usr/idea 切换到/usr/idea文件夹,解压tar.gz包 1234#解压mavensudo tar -zxvf ideaIU-2018.3.4.tar.gz#赋予权限sudo chmod 755 -R idea-IU-183.5429.30/ 破解，下载破解jar包，点击链接 1234567#移动下载的jar包到idea-IU-183.5429.30/bin路径cd ~/下载sudo mv JetbrainsIdesCrack-4.2-release.jar /usr/idea/idea-IU-183.5429.30/bin#赋予权限cd /usr/idea/idea-IU-183.5429.30/binsudo chmod 755 JetbrainsIdesCrack-4.2-release.jar 编辑idea-IU-183.5429.30/bin路径下的两个配置文件 在两个文件末尾加入： 123456789#编辑idea.vmoptions配置文件sudo vim idea.vmoptions-javaagent:/usr/idea/idea-IU-183.5429.30/bin/JetbrainsIdesCrack-4.2-release.jar#点击ESC然后输入:wq保存并推出#编辑idea64.vmoptions配置文件sudo vim idea64.vmoptions-javaagent:/usr/idea/idea-IU-183.5429.30/bin/JetbrainsIdesCrack-4.2-release.jar#点击ESC然后输入:wq保存并推出 然后在idea-IU-183.5429.30/bin路径下的执行命令 1./idea.sh 点击下一步，选择激活码激活，并将激活码粘贴进去： 十、安装搜狗输入法搜狗输入法官网下载linux版本搜狗输入法 12345678#首先安装fcitx输入框架sudo apt install fcitx#解压安装cd ~/下载sudo dpkg -i sogoupinyin_2.2.0.0108_amd64.deb#安装过程有错则运行如命令sudo apt --fix-broken install 十一、安装MySQL-81234#卸载mysqlsudo apt remove mysql-*#清除mysql的缓存dpkg -l |grep ^rc|awk '&#123;print $2&#125;' |sudo xargs dpkg -P 1、Ubuntu安装mysql8官方链接:选择最底下No thanks, just start my download. 然后更新镜像源 123#切换到下载的目录cd ~/下载sudo dpkg -i mysql-apt-config_0.8.12-1_all.deb 下载mysql8 12sudo apt updatesudo apt install mysql-server 新建数据库和用户 12345678910--创建数据库create database victor;--以下都是在root用户下执行--创建用户victorcreate user 'victor'@'localhost' identified by 'root';--赋予victor数据库的所有权限grant all privileges on victor.* to victor@localhost;--删除用户drop user victor; 注意：安装的Navicat无法连接，直接报错退出，可能是版本问题，导致Navicat已经不怎么维护Linux版本了。 所以选择MySQL官网的MySQL Workbench客户端来使用就行。 2、安装MySQL5.7用sudo apt 镜像源装默认是MySQL5.7，而且mysql的控制台有中文显示bug。 12345678910#更新源sudo apt-get update#安装mysqlsudo apt-get install mysql-server#配置mysqlsudo mysql_secure_installation#检查mysql服务状态systemctl status mysql.service#登陆mysqlsudo mysql -u root -p 注意：使用Navicat，导致中文乱码，设置start_navicat文件将export LANG=”en_US.UTF-8”改为export LANG=”zh_CN.UTF-8”，保存,又会导致英文乱码。 1sudo vim start_navicat Navicat试用期到了，可以执行以下命令： 12cd ~rm -rf .navicat64 十二、安装Anaconda3和Tensorflow下载链接 1、安装步骤执行以下命令就行 1234567891011121314151617181920212223242526272829303132333435363738394041wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2018.12-Linux-x86_64.sh#赋予权限chmod +x Anaconda3-2018.12-Linux-x86_64.sh #执行shellsudo ./Anaconda3-2018.12-Linux-x86_64.sh#设置安装路径/usr/anaconda3#最后提示要不要安装vscode选择no#设置anaconda3的路径export PATH=/usr/anaconda3/bin:$PATH#路径加到$PATH就行,然后保存退出source ~/.bashrc#测试是否可以打开anaconda navigatoranaconda-navigator#查看conda的版本conda --version#查看conda自带的包conda list#查看python版本python --version#删除conda创建的虚拟环境conda remove -n tensorflow --all#查看conda的虚拟环境conda env list#设置清华conda镜像conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes#创建tensorflow的虚拟环境防止污染anaconda3conda create --name [虚拟环境名] python版本conda create --name tensorflow python=3.7#激活tensorflow虚拟环境，进入到tensorflow虚拟环境source activate tensorflow#安装tensorflowconda install tensorflow 2、喜欢在spyder上编写tensorflow和运行12anaconda-navigator选择sypder来运行就行。 在激活tensorflow的虚拟环境运行，使用spyder会报错，没有tensorflow模块等。是因为tensorflow的虚拟环境中，没有spyder。要先在tensorflow的环境中安装spyder等插件。 要Ananconda navigator的Home中选择Applications on tensorflow,以及其他包。 3、tensorboard的使用123#切换到anaconda建立的tensorflow的虚拟环境source activate tensorflowtensorboard --logdir=/tmp/tmp_4sbz6pn/ 十三、安装Node1、安装node12345678910111213141516#更新源sudo apt update#安装nodejssudo apt install nodejs#安装npmsudo apt install npm#验证版本node -vnpm -v #设置淘宝镜像来使用cnpm#因为利用npm下载，国内很慢。sudo npm install -g cnpm --registry=https://registry.npm.taobao.org#安装完成后，默认的淘宝镜像放在/usr/local/lib/node_modules/cnpm/bin/cnpm#测试cnpmcnpm -v 2、安装vue1234567891011#安装vue脚手架工具vue-clisudo cnpm install -g vue-cli#测试vue的版本vue -V#创建vue项目sudo vue init webpack vue-demo一直回车，然后选择n就可以#切换到vue-demo目录,安装项目依赖sudo cnpm install#然后运行项目sudo npm run dev 3、安装hexohexo init命令执行的时候卡死在Install Dependencies 可能还是由于使用默认的npm库，速度极慢。可以安装nrm nrm是一个npm源管理器。 12345678910111213141516#使用cnpm初始化nrm包cnpm install nrm -g#切换到淘宝npm镜像nrm use taobao#新建一个文件夹mkdir hexo#安装hexo的脚手架sudo npm install -g hexo-cli#初始化hexohexo init#生成静态页面hexo g#本地运行测试。hexo s#将public文件内容部署到github仓库hexo d -g 十四、安装Eclipse下载链接 1234567891011121314151617181920212223242526272829303132#usr路径下新建eclipse文件夹cd /usrsudo mkdir eclipse#移动到/usr/eclipse路径cd ~/下载sudo mv eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz /usr/eclipse/#解压sudo tar -zxvf eclipse-jee-2018-12-R-linux-gtk-x86_64.tar.gz#进入eclipse文件目录cd /usr/eclipse#给解压后的eclipse文件夹赋予权限sudo chown -R eclipse#建立桌面启动项cd /usr/share/applicationssudo gedit eclipse.desktop#将下面内容复制到desktop文件里保存#Exec路径是eclipse安装文件夹下的可执行文件#Icon路径是图标路径[Desktop Entry]Encoding=UTF-8Name=EclipseComment=Eclipse IDEExec=/usr/eclipse/eclipse/eclipseIcon=/usr/eclipse/eclipse/icon.xpmTerminal=falseStartupNotify=trueType=ApplicationCategories=Application;Development;#赋予可执行权限sudo chmod u+x eclipse.desktop#复制到桌面cp eclipse.desktop ~/桌面 十五、安装Tomcat下载链接 123456789101112131415161718192021222324252627282930#/usr路径下新建tomcat文件夹cd /usrsudo mkdir tomcat#把下载的tomcat移动到新建文件夹cd ~/下载sudo mv apache-tomcat-8.5.37.tar.gz /usr/tomcatcd /usr/tomcat#解压sudo tar -zxvf apache-tomcat-8.5.37.tar.gz#赋予文件夹权限sudo chmod 755 -R apache-tomcat-8.5.37#到tomcat的bin路径下cd /usr/tomcat/apache-tomcat-8.5.37/bin#修改启动sudo vim startup.sh#加入jdk路径和tomcat路径#set java environment#JAVA PATHexport JAVA_HOME=/usr/java/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH#tomcatexport TOMCAT_HOME=/usr/tomcat/apache-tomcat-8.5.37#测试启动tomcatsudo ./startup.sh#关闭sudo ./shutdown.sh 十六、安装Docker12345678910#安装之前先更新sudo apt updatesudo apt upgradesudo apt install docker.io#启用dockersudo systemctl start dockersudo systemctl enable dockerdocker -v#查看已经pull的镜像sudo docker images 1、docker下的RabbitMQ拉取12345678910111213141516171819202122232425#使用国内镜像加速安装unbuntu18.04镜像sudo docker pull registry.docker-cn.com/library/ubuntu:18.04#安装RabbitMQsudo docker pull registry.docker-cn.com/library/rabbitmq:3-management#运行RabbitMQ# -d:表示后台运行# -p：表示暴露端口号# --name myrabbitmq：指定名字# a829a97a0435：对应的image idsudo docker run -d -p 5672:5672 -p 15672:15672 --name myrabbitmq a829a97a0435#查看进程sudo docker ps#访问RabbitMQlocalhost:156720.0.0.0:15672#登录用户名：guest密码：guest#下一次启动#找到对应的Container IDsudo docker ps -a#然后启动命令sudo docker start 213a1c20682e#停止命令sudo docker stop 213a1c20682e 2、docker下的redis拉取12345#下载Redis镜像sudo docker pull registry.docker-cn.com/library/redis#启动镜像#redis默认的端口是6379，可以将虚拟机的6379映射到容器的6379sudo docker run -d -p 6379:6379 --name myredis 0f55cf3661e9 解决方法： 12345#查看docker的进程sudo docker ps -a#删除掉创建的镜像#886183d7a2cc：container id，镜像idsudo docker rm 886183d7a2cc 12#重新启动docker已经有的容器sudo docker restart myrabbitmq 十七、安装XMind8官网链接 解压压缩包 然后移动到/usr/xmind8 12345678910111213141516171819202122232425sudo mkdir xmind8cd ~/下载 sudo mv xmind-8-update8-linux /usr/xmind8/cd /usr/xmind8/xmind-8-update8-linux/#执行setup.sh文件，安装相关依赖。sudo /setup.sh #百度下载一个xmind8的图片sudo mv xmind8.png /usr/xmind8/xmind-8-update8-linux/XMind_amd64#创建桌面快捷方式sudo vim /usr/share/applications/xmind8.desktop#编辑xmind8.desktop[Desktop Entry]Type=ApplicationPath=/usr/xmind8/xmind-8-update8-linux/XMind_amd64Exec=/usr/xmind8/xmind-8-update8-linux/XMind_amd64/XMindName=XMind 8Comment=Create mind mapsGenericName=Planning ToolIcon=/usr/xmind8/xmind-8-update8-linux/XMind_amd64/xmind8.pngCategories=Officecp /usr/share/applications/xmind8.desktop ~/桌面 十八、本地文件提交到github1234cd ~/文档/Ubuntu安装软件手册git add .git commit -m \"本次更新的说明\"git push origin master 十九、安装react脚手架 123456789101112131415#更新系统sudo apt updatesudo apt upgrade#跟新npmsudo npm cache clean -fsudo npm install -g nsudo n stable#安装react的脚手架sudo cnpm install -g create-react-app#创建一个文件夹cd ~/桌面sudo mkdir react-appcd ~/桌面/react-app#用react脚手架创建react项目sudo create-react-app react-app 二十、安装vmware 下载链接 12345678910111213141516171819cd ~/下载#赋予可执行权限sudo chmod +x VMware-Player-15.1.0-13591040.x86_64.bundle#安装sudo ./VMware-Player-15.1.0-13591040.x86_64.bundle#创建虚拟机的时候，如果遇到vmmode is defined表示没有这个模块#还需要在bios设置，把secure boot设置为disabledsudo vmware-modconfig --console --install-allsudo apt-get install libcanberra-gtk-module#当vmware报错为Cannot open /dev/vmmon: No such file or directory. Please make sure that the kernel module `vmmon' is loaded#设置秘钥openssl req -new -x509 -newkey rsa:2048 -keyout MOK.priv -outform DER -out MOK.der -nodes -days 36500 -subj \"/CN=VMware/\"#然后在内核装sudo /usr/src/linux-headers-`uname -r`/scripts/sign-file sha256 ./MOK.priv ./MOK.der $(modinfo -n vmmon)#安装MOKsudo /usr/src/linux-headers-`uname -r`/scripts/sign-file sha256 ./MOK.priv ./MOK.der $(modinfo -n vmnet)mokutil --import MOK.der#重启reboot 安装vmware tools 需要吧CD/DVD设置为Auto detect设置为自动检测，然后保存，才可以出现install vmware tools的菜单栏由灰色变为可用。 安装过程中，可能需要你输入密码，这时候，你只需要输入你的ubuntu的系统管理就行。然后安装就行。 12#卸载命令sudo vmware-installer -u vmware-player 二十一、Ubuntu 安装redis12345678910111213#更新系统sudo apt update#更新软件列表sudo apt upgrade#安装redissudo apt install redis-server#打印redis版本redi-server -v#启动redis服务redis-server#启动客户端redis-cli#然后输入ping，出现PONG说明安装成功 二十二、环境变量的配置123456789101112131415161718192021222324#修改环境变量vim ~/.bashrc#Javaexport JAVA_HOME=/usr/java/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH#Mavenexport M2_HOME=/usr/maven/apache-maven-3.3.3export PATH=$&#123;M2_HOME&#125;/bin:$PATH#tomcatexport TOMCAT_HOME=/usr/tomcat/apache-tomcat-8.5.42#终端颜色配置PS1='$&#123;debian_chroot:+($debian_chroot)&#125;\\[\\033[01;35;01m\\]\\u\\[\\033[00;00;01m\\]@\\[\\033[01;35;01m\\]\\h\\[\\033[00;31;01m\\]:\\[\\033[00;00;01m\\]\\w \\[\\033[01;32;01m\\]\\$ \\[\\033[01;01;01m\\]'#Scalaexport SCALA_HOME=/usr/scala/scala-2.13.1export PATH=$&#123;SCALA_HOME&#125;/bin:$PATH#Sparkexport SPARK_HOME=/usr/spark/spark-2.4.4-bin-hadoop2.7export PATH=$&#123;SPARK_HOME&#125;/bin:$PATH#启动spark(首先确保配置好jdk，scala，以及spark才能使用spark-shell命令启动)spark-shell#保存.bashrcsource ~/.bashrc","link":"","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://victorblog.github.io/tags/DevOps/"}]},{"title":"MySQL的问题","date":"2018-08-14T02:06:00.000Z","path":"2018/08/14/开发中-MySQL的问题/","text":"MySQL的问题1、重置id问题由于在使用mysql，设计表的时候，设置了id自增，然后删除了数据后，再次新增数据时，就会出现id累计的情况，重置清空id，可以使用truncate 12#重置清空id，让id从1开始自增truncate table t_student 2、Insert ignore的使用表要求有：primary key，或者有unique索引 Insert ignore会忽略已存在的数据 1insert ignore into t_student(name,age,class) values(&quot;test&quot;,19,&quot;计算机&quot;); 3、MySQL的表数据的导入导出1234#先把测试环境的数据导出到sql文件mysqldump -h192.168.1.1 -uadmin -padmin demo user_account &gt; /opt/demo/dmp/user_account.sql#然后导入到生产环境mysql -h192.161.1.215 -uadmin -padmin demo &lt; /opt/dmp/user_account.sql","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"G1垃圾回收","date":"2018-07-21T02:06:00.000Z","path":"2018/07/21/一、G1垃圾回收器介绍/","text":"一、G1垃圾回收器介绍为了解决CMS算法产生空间碎片和其他一系列的问题缺陷，HotSpot虚拟机团队提出了另外一种垃圾回收策略，G1（Garbage First）算法。通过参数-XX:+UserG1GC来启用。这个算法在JDK7被正式推出。 G1垃圾回收算法主要应用在多CPU大内存的服务中，在满足高吞吐量的同事，尽可能的满足垃圾回收时的暂停时间，G1的设计应用场景： 垃圾收集线程和应用线程并发执行，和CMS一样。 空闲内存亚索时避免冗长的暂停时间 应用需要更多可预测的GC暂停时间 不希望牺牲太多的吞吐性能 不需要很大的java堆 1、堆内存结构：1、以往的垃圾回收算法，比如CMS，堆内存结构如下：年轻代/新生代：eden space+2个survivor 老年代：old space 持久代：1.8之前的perm space 元空间：1.8之后的metaspace(元空间) 上面这些space必须是地址连续的空间。 2、在G1算法中，采用了另一种完全不同的方式组织堆内存堆内存被划分成多个大小相等的内存块Region，每个Region是逻辑连续的一段内存。 每个Region被标记了E,S,O,H，说明每个Region在运行时都充当了一种角色，其中H是以往算法中没有的，它代表Humongous，这表示这些Region存储的是巨型对象。当新建对象大小超过Region大小一半的时候，直接在新的一个或者多个连续Region中分配，并标记为H。 3、Region堆内存中一个Region的大小可以通过-XX:G1HeapRegionSize参数指定，大小区间只能是1M,2M,4M,8M,16M,32M，总之是2的幂次方，如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实际大小。 默认把堆内存按照2048份均分，最后得到一个合理的大小。 2、GC模式G1中提供了3种模式垃圾回收模式，young GC,mixed GC和full GC。在不同的条件下被触发 1、young GC发生在年轻代的GC算法，一般对象（除了巨型对象），都是在eden region中分配内存，当所有的eden region被消耗殆尽无法再申请内存的时候，就会触发一次young GC，这种触发机制和之前的young GC差不多，执行玩一次young GC，活跃对象会被拷贝到survivor region或者晋升到old region中，空闲的region会被放入空闲列表中，等待下次被使用。 参数 含义 -XX:MaxGCPauseMillis 设置G1收集过程目标时间，默认200ms -XX:G1NewSizePercent 新生代最小值，默认值5% -XX:G1MaxNewSizePercent 新生代最大值，默认值60% 2、mixed GC当越来越多的对象晋升到老年代old region时，为了避免堆内存被消耗殆尽，虚拟机会触发一个混合的垃圾收集器，也就是mixed GC；这个算法不是一个old GC，除了回收整个young region，还会回收一部分old region。注：只是一部分老年代，而不是全部老年代，可以选择哪些old region进行收集，从而可以对垃圾回收的耗时时间进行控制。 mixed GC什么时候被触发？ 有点类似cms的触发机制 ，如果添加了以下参数： 12-XX:CMSInitiatingOccupancyFraction=80-XX:+UserCMSInitiatingOccupancyOnly 当CMS的老年代的使用率达到80%时，就会触发一次cms gc。 相对的，mixed gc中也有一个阈值参数： 1-XX:InitiatingHeapOccupancyPercent 当老年代大小占整个堆大小百分比达到该阈值时，会触发一次mixed gc。 3、mixed GC执行过程 initial remark：初始标记过程，整个过程STW（Stop-The-World，在执行垃圾回收算法的时候，Java应用程序的其他所有线程都被挂起，Java中一种全局暂停现象，全局停顿，所有Java代码停止，native代码可以执行，但是不饿能够和JVM交互）标记了从GC Root可达的对象 concurrent marking：并发标记过程，整个过程gc collector线程与应用线程可以并行执行，标记出GC Root可达对象衍生出来的存活的对象，并收集各个Region的存活对象信息。 remark：最终标记过程，整个过程STW，标记出哪些在并发标记过程中遗漏的，或者内部引用发生变化的对象。 clean up：垃圾清除过程，如果发现一个Region中没有存活对象，则把该Regioon加入到空闲列表中。 4、full GC如果对象内存分配速度过快，mixed GC来不及回收，导致老年代old Region被填满，就会触发一次full gc。 G1的full GC算法就是单线程的执行serial old gc，会导致异常长时间的暂停时间，需要进行不断的调优，尽可能的避免full GC。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"消息中间件RabbitMQ","date":"2018-07-07T07:06:00.000Z","path":"2018/07/07/消息中间件RabbitMQ/","text":"一、消息中间件RabbitMQ1、RabbitMQ介绍 RabbitMQ是一个开源并实现了高级消息队列协议(AMQP)的消息中间件，支持单一节点部署，也支持多个节点的集群部署。 RabbitMQ提供了后端管理控制台，用来实现RabbitMQ的队列，交换机，路由，消息和服务节点的管理。 RabbitMQ可以通过客户端管理相应的用户，主要是分配相应的操作权限和数据管理权限 2、RabbitMQ应用场景RabbitMQ实现了高性能存储分发消息的分布式中间件，它的作用如下图 2.1、异步通信和服务解耦1、以用户注册为场景，传统的方式用户注册过程如下 分析用户注册流程，核心业务就是判断用户注册信息的合法性以及把用户信息写入到数据库。 发送邮件，短信验证码服务不归属用户注册的核心流程，可以使用消息中间件进行异步通信，将这一块解耦出来。 2、引入RabbitMQ消息中间件后用户注册的流程 3、好处 引入RabbitMQ，可以将一条业务线走到底，系统的接口的整体响应时间明显降低，实现了低延迟，给用户带啦很好的体验效果。 2.2、接口限流和消息分发1、以电商网站商品抢购活动的传统处理流程 分析抢购活动开始的那一刻，将会产生巨大的用户抢购流量，请求几乎在同一时刻到达后端系统接口。 首先校验用户和商品信息的合法性，校验通过后 会判断当前商品的库存是否重组，如果充足，代表当前用户将能成功抢购 最后将用户抢购成功的相关数据保存到数据库 异步通知用户抢购成功，尽快进行付款 分析发现后端接口在处理用户抢购整体业务流程太长，业务处理逻辑，先取出库存然后进行判断，然后进行减1更新。在高并发情况下，会导致商品超卖，数据不一致，用户等待时间过长，系统接口直接挂了，如果是商品抢购，商品秒杀等某一时刻产生高并发的请求就不行了。 2、引入RabbitMQ的商品抢购流程 引入RabbitMQ主要用来优化系统如下： 接口限流：当前端产生高并发请求的时候，不会立刻到达后端接口，而是先将请求按照先来后到加入到RabbitMQ队列，一定程度上实现接口限流。 消息异步分发：当商品库存充足，当前抢购的用户将可以抢到商品，然后会异步发送短信通知用户抢购成功，然后告知用户付款，一定程度上实现消息异步分发。 2.3、业务延迟处理1、实际场景分析 以12306抢票来分析，春运的时候12306抢票抢到车票的时候，12306会提醒用户，请在30分钟内付款。 正常情况用户会立刻付款，然后付款成功12306会发短信通知。 然而也存在一些特殊情况，比如抢到车票的用户迟迟没有付款，过了30分钟，系统会自动取消这笔订单。 需要延迟一定的时间后再进行处理的业务在实际生产环境常见。 2、传统处理方式-采用定时器 传统处理方式采用一个定时器定时的去获取用户没有付款的订单，然后判断用户的下单时间距离当前时间是否超过30分钟，超过没有付款则设置订单失效。 分析发现，春运抢票可以看做是一个大数据量，高并发请求场景，如果定时器频繁的从数据库获取未付款状态的订单，数据量非常大，假如有大量用户在30分钟内没付款，那么数据库中获取数据量一直增长，达到一定程度，给数据库服务器和应用服务器带来巨大的压力，直接压跨服务器，导致抢票全线崩溃。 早期12306网站每次赶上春运，经常出现网站崩溃卡顿无响应等状态，因为某一时刻产生高并发，定时频繁拉取数据库得到数据量过大，导致内存，CPU，网络和数据库服务负载过高导致的。 2、引入RabbitMQ优化 优化流程看出，RabbitMQ的引入主要替代传统处理流程的定时器处理逻辑，而采用RabbitMQ的延迟队列来进行处理。 延迟队列：可以延迟一定的时间后再处理相应的业务逻辑 常见的成功抢到票30分钟没付款流程，商城抢购没有在规定时间付款，点外卖下单成功后，没有在规定时间付款的流程，都是用RabbitMQ的延迟队列来实现。 3、RabbitMQ的简单使用 由于我本地使用docker来运行RabbitMQ，如何使用docker安装RabbitMQ以及运行，请点击链接看我这篇博客 运行好RabbitMQ后，然后浏览器输入访问地址(http://localhost:15672/) 默认用户名密码都为guest/guest，即可进入RabbitMQ管理后台","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Redis应用场景之抢红包系统(四)","date":"2018-06-21T07:06:00.000Z","path":"2018/06/21/Redis应用场景之抢红包系统-四/","text":"一、Redis应用场景之抢红包系统(四)1、Jmeter压力测试高并发抢红包 Apache Jmeter是Apache组织开发的基于Java的压力测试工具。可以用来模拟生产环境中高并发产生的巨大负载。 下载链接 下载好后，解压运行Jmeter 12345cd ~/下载/软件安装包tar -zxvf apache-jmeter-5.2.1.tgzcd apache-jmeter-5.2.1cd bin./jmeter.sh 首先用Postman先新建一个用户userId假设为10030，生成一个新的红包全局id 测试计划–&gt;右键–&gt;添加–&gt;线程(用户)–&gt;线程组 线程组面板，设置线程属性，设置线程数为1000个 线程组–&gt;右键–&gt;添加–&gt;取样器–&gt;HTTP请求 线程组–&gt;右键–&gt;添加–&gt;配置原件–&gt;CSV数据文件设置 假设录入新的用户userId是从10030~10035的data.csv数据文件 然后点击Jmeter上面的三角符号运行按钮进行测试 线程组–&gt;右键–&gt;添加–&gt;监听器–&gt;查看结果树 打开MySQL查询抢红包的明细表发现已经抢过红包的用户后续又抢到红包，比如10030已经抢过，后又抢到红包，说明是高并发引起的线程安全问题 2、问题分析 当某一时刻的同一个用户在疯狂的点红包，如果前端不加以控制，同一时间的同一个账户将发起多个抢红包请求。 后端接口接收到这些请求后，很可能同时在redis中判断是否有红包，并成功通过，然后导致一个用户抢到多个红包。 后端接口并没有考虑到高并发请求，原因在于当前请求还没有处理完核心业务逻辑，其他同样的请求过来，导致后端接口几乎来不及做重复判断 3、优化方案 同一时刻多个并发的线程对共享资源进行了访问操作，导致最终出现数据不一致或者结果并非自己所预料的现象，就是多线程高并发时出现的并发安全问题。 传统的单体Java应用，为了解决高并发，最常见的方法是在核心的业务逻辑代码中加锁，也就是Synchronized关键字。 在微服务、分布式系统架构，这样做法行不通，因为Synchronized关键字是跟单一服务节点所在的JVM相关联，而分布式系统架构的服务一般是部署在不同的节点服务器上，从而当出现高并发请求时，Synchronized同步操作力不从心。 为了保证单一节点核心业务代码的同步控制，也要保证当扩展到多个节点部署同样的实现核心业务逻辑，就出现了分布式锁。 分布式锁只是一种解决方案，主要是为了解决分布式系统高并发请求时出现并发访问共享资源导致的并发安全问题； 分布式锁的实现有多种 基于数据库级别的乐观锁和悲观锁 基于Redis的原子操作实现分布式锁 基于Zookeeper实现分布式锁 4、Redis分布式锁实战4.1、redis的底层分析 由于Redis底层架构是采用单线程进行设计的，所以它提供的这些操作都是单线程的，也就是操作具有原子性。 原子性就是在同一时刻只能有一个线程来处理核心业务逻辑，当有其他线程对应的请求过来的时候，如果前面的线程没有处理完毕，那么当前线程将进入等待(阻塞)状态，直到前面的线程处理完毕。 4.2、分布式锁实现 因为抢红包的核心业务逻辑在于拆红包操作。 可以通过redis的原子操作setIfAbsent()方法对业务逻辑加分布式锁 setIfAbsent()方法： 如果当前的key不存在redis中，就设置对应的value，然后返回true 如果当前key存在，则设置value失败，返回false 方法具有原子性单线程操作，所以多个并发的线程同一时刻调用setIfAbsent()，redis底层会将线程加入队列排队处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Override public BigDecimal robByDistributeLock(Integer userId, String redId) throws Exception &#123; ValueOperations valueOperations = redisTemplate.opsForValue(); /** * 处理用户抢红包之前，需要先判断一下当前用户是否已经抢过红包 * 如果抢过，则直接返回红包金额 */ Object data = valueOperations.get(redId + userId + \":rob\"); if (data != null) &#123; BigDecimal redPacket = new BigDecimal(data.toString()); log.info(\"从redis中获取数据，当前用户抢到红包了：userId=&#123;&#125; key=&#123;&#125; 金额=&#123;&#125;\", userId, redId, redPacket); return redPacket; &#125; Boolean res = clilck(redId); //如果为true代表redis中仍然还有红包，也就是红包个数&gt;0 if (res) &#123; //上分布式锁：一个红包每个人只能抢到一次随机金额，也即是永远保证一对一关系 final String lockKey = redId+userId+\"-lock\"; //调用setIfAbsent()方法，其实就是间接实现了分布式锁 Boolean lock = valueOperations.setIfAbsent(lockKey, redId); //设置分布式锁的过期时间为24h redisTemplate.expire(lockKey,24L,TimeUnit.HOURS); try &#123; //表示当前线程获取到了该分布式锁 if(lock) &#123; //从小红包随机金额列表弹出一个随机金额 Object value = redisTemplate.opsForList().rightPop(redId); if (value != null) &#123; /** * 更新redis中剩余的红包个数，也就是红包个数减1 */ String redTotalKey = redId + \":total\"; Object total = valueOperations.get(redTotalKey); Integer curTotal = 0; if (total != null) &#123; curTotal = (Integer) total; &#125; valueOperations.set(redTotalKey, curTotal - 1); //直接处理红包的单位变为分，如果此处不处理，需要前端处理也是一样的 BigDecimal redPacket = new BigDecimal(value.toString()).divide(new BigDecimal(100)); //将抢到红包的用户信息异步保存到数据库 redService.recordRobRedPacket(userId, redId, new BigDecimal(value.toString())); //将当前用户抢到红包的用户设置到redis中，表示当前用户已经抢过红包了，设置过期时间24h valueOperations.set(redId + userId + \":rob\", redPacket, 24L, TimeUnit.HOURS); log.info(\"当前用户抢到红包了：userId=&#123;&#125; key=&#123;&#125; 金额=&#123;&#125;\", userId, redId, redPacket); return redPacket; &#125; &#125; &#125;catch (Exception e)&#123; throw new Exception(\"系统异常-抢红包-加分布式锁失败！\"); &#125; &#125; //null表示当前用户没有抢到红包 return null; &#125; 4.3、测试结果 首先利用postman调用发红包接口，重新生成一个红包全局id 然后配置到Jmeter的http请求的redId上 然后利用Jmeter测试步骤同样的，最后查看抢红包明细表发现，没有出现同一个账户又重复抢到红包的情况，从而实现分布式锁控制 5、总结与分析 代码的核心业务逻辑已经加上分布式锁，也能扛得住秒级高并发的请求，但是从业务层来看，我们只设置了10030~10035这6个用户来抢红包，发红包我们设置了10个红包，那么剩下4个红包的剩余金额还没处理，首先需要归还个发红包的人，这块业务逻辑就不在写。 多线程高并发时产生的并发安全问题，需要对核心业务处理逻辑加同步控制操作，也就是加上分布式锁","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Redis应用场景之抢红包系统(三)","date":"2018-06-20T06:06:00.000Z","path":"2018/06/20/Redis应用场景值抢红包系统-三/","text":"一、Redis应用场景之抢红包系统(三)1、业务模块分析 抢红包业务对应的后端接口需要频繁的访问Redis，用来获取红包剩余个数和随机金额列表，来判断用户点红包、拆红包是否成功。 用户每次成功抢到红包之后，后端接口需要及时更新缓存系统中红包剩余个数，将相应的信息保存到数据库 2、开发流程介绍​ 抢红包系统处理用户请求的过程和数据流向如下： 3、抢红包模拟实战 前端用户发起抢红包请求，需要带上红包全局唯一标识串redId和当前用户userId。 后端接口根据redId去查询Redis中获取红包剩余个数和随机金额列表。 3.1、发红包的控制器RedPacketController1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.victor.controller;import com.victor.model.redpacket.RedPacketRequest;import com.victor.model.redpacket.BaseResponse;import com.victor.model.redpacket.StatusCode;import com.victor.service.redpacket.IRedPacketService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.validation.BindingResult;import org.springframework.validation.annotation.Validated;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;/** * @Description: 红包处理逻辑Controller * @Author: VictorDan * @Version: 1.0 */@RestController@Slf4jpublic class RedPacketController &#123; @Autowired private IRedPacketService redPacketService; private static final String PREFIX = \"red/packet\"; /** * 抢红包-请求方式为Get * @param userId * @param redId * @return */ @GetMapping(value = PREFIX + \"/rob\") public BaseResponse rob(@RequestParam Integer userId, @RequestParam String redId) &#123; BaseResponse&lt;BigDecimal&gt; response = new BaseResponse&lt;&gt;(StatusCode.Success); /** * 调用抢红包，返回最终的红包金额，如果为null表示已被抢完 */ BigDecimal data = redPacketService.rob(userId, redId); if (data != null) &#123; response.setData(data); &#125; else &#123; response = new BaseResponse&lt;&gt;(StatusCode.Fail.getCode(), \"红包已被抢完！\"); &#125; return response; &#125;&#125; 2.3、抢红包的核心逻辑Service 抢红包接口IRedPacketService 12345678910111213141516171819202122232425262728package com.victor.service.redpacket;import com.victor.model.redpacket.RedPacketRequest;import java.math.BigDecimal;/** * @Description: * @Author: VictorDan * @Version: 1.0 */public interface IRedPacketService &#123; /** * 发红包核心业务逻辑 * @param request * @return * @throws Exception */ String handOut(RedPacketRequest request) throws Exception; /** * 抢红包 * @param userId * @param redId * @return */ BigDecimal rob(Integer userId,String redId);&#125; 抢红包核心逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.victor.service.redpacket.impl;import com.victor.model.redpacket.RedPacketRequest;import com.victor.service.redpacket.IRedPacketService;import com.victor.service.redpacket.IRedService;import com.victor.util.RedPacketUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Service;import java.math.BigDecimal;import java.util.List;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Service@Slf4jpublic class RedPacketServiceImpl implements IRedPacketService &#123; @Autowired private RedisTemplate redisTemplate; /** * 红包业务逻辑处理过程记录到数据库的服务 */ @Autowired private IRedService redService; //存储到Redis中定义key前缀 private static final String KEY_PREFIX = \"redis:red:packet:\"; /** * 抢红包 * @param userId 当前抢红包的用户id * @param redId 红包的全局唯一标识符 * @return */ @Override public BigDecimal rob(Integer userId, String redId) &#123; ValueOperations valueOperations = redisTemplate.opsForValue(); /** * 处理用户抢红包之前，需要先判断一下当前用户是否已经抢过红包 * 如果抢过，则直接返回红包金额 */ Object data = valueOperations.get(redId + userId + \":rob\"); if (data != null) &#123; BigDecimal redPacket = new BigDecimal(data.toString()); log.info(\"从redis中获取数据，当前用户抢到红包了：userId=&#123;&#125; key=&#123;&#125; 金额=&#123;&#125;\", userId, redId, redPacket); return redPacket; &#125; Boolean res = clilck(redId); //如果为true代表redis中仍然还有红包，也就是红包个数&gt;0 if (res) &#123; //从小红包随机金额列表弹出一个随机金额 Object value = redisTemplate.opsForList().rightPop(redId); if (value != null) &#123; /** * 更新redis中剩余的红包个数，也就是红包个数减1 */ String redTotalKey = redId + \":total\"; Object total = valueOperations.get(redTotalKey); Integer curTotal = 0; if (total != null) &#123; curTotal = (Integer) total; &#125; valueOperations.set(redTotalKey, curTotal - 1); //直接处理红包的单位变为分，如果此处不处理，需要前端处理也是一样的 BigDecimal redPacket = new BigDecimal(value.toString()).divide(new BigDecimal(100)); //将抢到红包的用户信息异步保存到数据库 redService.recordRobRedPacket(userId, redId, new BigDecimal(value.toString())); //将当前用户抢到红包的用户设置到redis中，表示当前用户已经抢过红包了，设置过期时间24h valueOperations.set(redId + userId + \":rob\", redPacket, 24L, TimeUnit.HOURS); log.info(\"当前用户抢到红包了：userId=&#123;&#125; key=&#123;&#125; 金额=&#123;&#125;\", userId, redId, redPacket); return redPacket; &#125; &#125; //null表示当前用户没有抢到红包 return null; &#125; /** * 点红包的业务逻辑，如果为true代表还有红包，否则代表红包已经抢光 * * @param redId * @return */ private Boolean clilck(String redId) &#123; ValueOperations valueOperations = redisTemplate.opsForValue(); String redIdKey = redId + \":total\"; /** * 获取redis中红包的剩余个数 */ Object total = valueOperations.get(redIdKey); /** * 判断红包剩余个数total是否大于0，如果大于0，则返回true代表还有红包，否则false */ return total != null &amp;&amp; Integer.valueOf(total.toString()) &gt; 0; &#125;&#125; 2.4、异步保存红包业务逻辑处理过程数据到数据库 红包业务逻辑处理过程数据接口 12345678910111213141516171819202122232425262728package com.victor.service.redpacket;import com.victor.model.redpacket.RedPacketRequest;import java.util.List;/** * @Description: 红包业务逻辑处理过程数据记录 * @Author: VictorDan * @Version: 1.0 */public interface IRedService &#123; /** * 记录发红包 * @param dto * @param redId * @param list */ void recordRedPacket(RedPacketRequest dto, String redId, List&lt;Integer&gt; list); /** * 将抢到红包的用户信息以及红包金额异步保存到数据库 * @param userId * @param redId * @param data */ void recordRobRedPacket(Integer userId, String redId, BigDecimal data);&#125; 红包业务逻辑处理过程数据实现类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.victor.service.redpacket.impl;import com.victor.model.redpacket.RedPacketRequest;import com.victor.model.redpacket.RedDetail;import com.victor.model.redpacket.RedRecord;import com.victor.repository.redpacket.RedDetailRepo;import com.victor.repository.redpacket.RedRecordRepo;import com.victor.repository.redpacket.RedRobRecordRepo;import com.victor.service.redpacket.IRedService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import java.math.BigDecimal;import java.util.Date;import java.util.List;/** * @Description: 红包业务逻辑处理过程数据记录 * @Author: VictorDan * @Version: 1.0 */@Slf4j@Servicepublic class RedServiceImpl implements IRedService &#123; @Autowired private RedRecordRepo redRecordRepo; @Autowired private RedDetailRepo redDetailRepo; @Autowired private RedRobRecordRepo redRobRecordRepo; /** * 将成功抢到红包的用户信息以及红包金额异步保存到数据库 * @param userId 用户账号 * @param redId 红包全局唯一标识串 * @param data 抢到的红包金额 */ @Override @Async @Transactional(rollbackFor = Exception.class) public void recordRobRedPacket(Integer userId, String redId, BigDecimal data)&#123; RedRobRecord redRobRecord = new RedRobRecord(); redRobRecord.setAmount(data); redRobRecord.setUserId(userId); //红包的全局唯一标识串 redRobRecord.setRedPacket(redId); redRobRecord.setRobTime(new Date()); redRobRecord.setIsActive(true); //将抢到红包的信息保存到数据库 redRobRecordRepo.saveAndFlush(redRobRecord); &#125;&#125; 2.5、测试结果​ 分别输入10个userId，假设输入的从10010到10019,然后当输入到10011就会提示红包抢光 使用PostMan测试接口结果如下： 查看数据库记录 1234#查看发红包表SELECT * FROM victor.red_record;#查看红包明细表，是否对应总金额SELECT sum(amount) FROM victor.red_detail where record_id=1 and is_active=1; 查看抢红包表 汇总红包明细表的金额与总金额对比 4、开发总结4.1、缺陷 首先这些请求都是串行的，因为在PostMan测试的时候，都是一个一个账户的请求，是有时间间隔的。 没有达到生产环境中秒级高并发请求。 高并发就是，在同一时刻突然有成千上万甚至百千万的数量级请求到达后端接口。 这里没有考虑高并发抢红包的情况，高并发请求的本质是高并发多线程，多线程的高并发如果出现抢占共享资源而不加以控制，会造成数据不一致。","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Redis应用场景之抢红包系统(二)","date":"2018-06-19T05:06:00.000Z","path":"2018/06/19/Redis应用场景之抢红包系统-二/","text":"一、Redis应用场景之抢红包系统(二)1、开发流程介绍​ 抢红包系统处理用户请求的过程和数据流向如下： 1.1、发红包请求处理 后端主要根据用户输入的金额和个数预生成相应的红包随机金额列表，然后将红包的总个数和对应的随机金额列表缓存到Redis中。 将红包的总金额、随机金额列表和红包全局唯一标识串信息异步记录到对应的数据库表中。 1.2、抢红包请求处理 后端接口先接收到前端用户账号以及红包的全局唯一标识串等请求信息，假设用户账号合法性等信息全部校验通过，然后开始处理用户点开红包的逻辑 主要是从Redis中获取当前剩余的红包个数，根据红包个数是否大于0，则开始处理用户拆红包的逻辑。 拆红包逻辑主要是从Redis中的随机金额队列中弹出一个红包金额，并根据金额是否为null判断是否成功抢到红包 1.3、状态码设计 保证系统整体开发流程规范，约定处理用户请求信息后将返回统一的响应格式。 BaseResponse类 123456789101112131415161718192021222324252627282930313233343536373839404142package com.victor.model.redpacket;import lombok.Data;import lombok.ToString;/** * @Description: 响应信息类 * @Author: VictorDan * @Version: 1.0 */@Data@ToStringpublic class BaseResponse&lt;T&gt; &#123; /** * 状态码 */ private Integer code; /** * 描述信息 */ private String msg; /** * 响应数据-采用泛型表示可以接受通用的数据类型 */ private T data; public BaseResponse(Integer code, String msg, T data) &#123; this.code = code; this.msg = msg; this.data = data; &#125; public BaseResponse(StatusCode statusCode) &#123; this.code = statusCode.getCode(); this.msg = statusCode.getMsg(); &#125; public BaseResponse(Integer code, String msg) &#123; this.code = code; this.msg = msg; &#125;&#125; StatusCode类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.victor.model.redpacket;import lombok.ToString;/** * @Description: 状态码类 * 注：状态码类是枚举类，所以不能使用@Data，需要自己生成getter/setter方法 * @Author: VictorDan * @Version: 1.0 */@ToStringpublic enum StatusCode &#123; /** * 以下是暂时设定的几种状态码 */ Success(0,\"成功\"), Fail(-1,\"失败\"), InvalidParams(201,\"非法参数！\"), InvalidGrantType(202,\"非法的授权类型\"); /** * 状态码 */ private Integer code; /** * 描述信息 */ private String msg; StatusCode(Integer code,String msg) &#123; this.code=code; this.msg=msg; &#125; public Integer getCode() &#123; return code; &#125; public String getMsg() &#123; return msg; &#125; public void setCode(Integer code) &#123; this.code = code; &#125; public void setMsg(String msg) &#123; this.msg = msg; &#125;&#125; 1.4、红包金额随机生成算法 抢红包系统的核心部分主要在于抢红包的逻辑处理，而能否抢到红包，主要取决于红包个数和红包随机金额列表。 红包随机金额列表采用预生成的方式，也就是通过给定红包的总金额M和总人数N，采用随机数算法生成红包随机金额列表，并将它放到Redis中，用来拆分红逻辑的处理 1、随机数算法要求 所有人抢到的金额之和等于红包金额 每个人至少抢到0.01元 要保证所有人抢到金额的记录相等。 采用蒙特卡洛方法，主要构造一个数学模型，将若干个随机变量和统计分析的方法求出若干个随机数。 2、二倍均值法 实际中红包随机金额的生成算法有许多，二倍均值法是比较典型。 根据每次剩余的总金额M和剩余人数N，执行M/N，然后在乘以2,所得到的数E，在0到E的区间内，随机产生一个随机金额。 重复上面的步骤，直到剩余最后一个人。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.victor.util;import java.util.ArrayList;import java.util.List;import java.util.Random;/** * @Description: 发红包工具类 * @Author: VictorDan * @Version: 1.0 */public class RedPacketUtil &#123; /** * 发红包算法，二倍均值法 * * @param totalAmount 红包总金额，单位为分 * @param totalPeopleNum 总人数 * @return */ public static List&lt;Integer&gt; divideRedPacket(Integer totalAmount, Integer totalPeopleNum) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); /** * 判断总金额和总个数参数的合法性 */ if (totalAmount &gt; 0 &amp;&amp; totalPeopleNum &gt; 0) &#123; //记录剩余总金额，初始化为红包总金额 Integer restAmount = totalAmount; //记录剩余总人数，初始化为指定的总人数 Integer restPeopleNum = totalPeopleNum; //产生随机数 Random random = new Random(); //不断循环遍历，更新迭代产生随机金额，直到N-1&gt;0 for (int i = 0; i &lt; totalPeopleNum - 1; i++) &#123; //随机范围:[1,剩余人均金额的两倍),随机金额为分，至少一个人得到1分钱 int amount = random.nextInt(restAmount / restPeopleNum * 2 - 1) + 1; //更新剩余的总金额 restAmount = restAmount - amount; //更新剩余的人数 restPeopleNum--; //将产生的随机金额加入到list list.add(amount); &#125; //循环完毕，将剩余的金额也即是最后一个随机金额，加入到list list.add(restAmount); &#125; return list; &#125;&#125; 2、发红包模拟实战 发红包模块的核心处理逻辑在于接受前端发红包设定的总金额M和总个数N，后端接口根据这2个参数，然后采用二倍均值算法生成N个随机金额的红包，最后将红包个数N和随机金额list存到缓存，然后把相关的数据异步记录到数据库中。 后端接口在接收到前端用户发红包请求的时候，可以采用时间戳作为红包全局唯一标识串，然后将这个串返回给前端，后续用户发红包的时候，会带上这个参数，目的为了给发出的红包打标记，把这个标记作为key去缓存中查询红包个数和随机金额列表等。 2.1、发红包请求参数RedPacketRequest123456789101112131415161718192021222324252627282930package com.victor.model.redpacket;import lombok.Data;import lombok.ToString;import javax.validation.constraints.NotNull;/** * @Description: 处理发红包的请求参数 * @Author: VictorDan * @Version: 1.0 */@Data@ToStringpublic class RedPacketRequest &#123; /** * 用户账号id */ private Integer userId; /** * 红包个数 */ @NotNull private Integer total; /** * 总金额-单位为分 */ @NotNull private Integer amount;&#125; 2.2、发红包的控制器RedPacketController123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.victor.controller;import com.victor.model.redpacket.RedPacketRequest;import com.victor.model.redpacket.BaseResponse;import com.victor.model.redpacket.StatusCode;import com.victor.service.redpacket.IRedPacketService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.validation.BindingResult;import org.springframework.validation.annotation.Validated;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;/** * @Description: 红包处理逻辑Controller * @Author: VictorDan * @Version: 1.0 */@RestController@Slf4jpublic class RedPacketController &#123; @Autowired private IRedPacketService redPacketService; private static final String PREFIX = \"red/packet\"; /** * 发红包-请求方式为post，数据格式采用JSON交互 * * @param request * @param result * @return */ @PostMapping(value = PREFIX + \"/hand/out\") public BaseResponse handOut(@Validated @RequestBody RedPacketRequest request, BindingResult result) &#123; //参数校验 if (result.hasErrors()) &#123; return new BaseResponse(StatusCode.InvalidParams); &#125; BaseResponse&lt;String&gt; response = new BaseResponse&lt;&gt;(StatusCode.Success); String redId; try &#123; //核心业务逻辑处理，然后返回红包的唯一标识串 redId = redPacketService.handOut(request); response.setData(redId); &#125; catch (Exception e) &#123; log.error(\"发红包发生异常，请求request=&#123;&#125;,异常信息为：&#123;&#125;\", request, e.getMessage()); return new BaseResponse(StatusCode.Fail.getCode(), e.getMessage()); &#125; return response; &#125;&#125; 2.3、发红包的核心逻辑Service 发红包接口IRedPacketService 12345678910111213141516171819202122232425262728package com.victor.service.redpacket;import com.victor.model.redpacket.RedPacketRequest;import java.math.BigDecimal;/** * @Description: * @Author: VictorDan * @Version: 1.0 */public interface IRedPacketService &#123; /** * 发红包核心业务逻辑 * @param request * @return * @throws Exception */ String handOut(RedPacketRequest request) throws Exception; /** * 抢红包 * @param userId * @param redId * @return */ BigDecimal rob(Integer userId,String redId);&#125; 发红包核心逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.victor.service.redpacket.impl;import com.victor.model.redpacket.RedPacketRequest;import com.victor.service.redpacket.IRedPacketService;import com.victor.service.redpacket.IRedService;import com.victor.util.RedPacketUtil;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Service;import java.math.BigDecimal;import java.util.List;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Service@Slf4jpublic class RedPacketServiceImpl implements IRedPacketService &#123; @Autowired private RedisTemplate redisTemplate; /** * 红包业务逻辑处理过程记录到数据库的服务 */ @Autowired private IRedService redService; //存储到Redis中定义key前缀 private static final String KEY_PREFIX = \"redis:red:packet:\"; /** * 发红包 * * @param request * @return * @throws Exception */ @Override public String handOut(RedPacketRequest request) throws Exception &#123; if (request.getTotal() &gt; 0 &amp;&amp; request.getTotal() &gt; 0) &#123; List&lt;Integer&gt; list = RedPacketUtil.divideRedPacket(request.getAmount(), request.getTotal()); //生成红包全局唯一标识串 String timestamp = String.valueOf(System.nanoTime()); //根据缓存key的前缀与其他信息拼接成一个新的用来存储红包总数的key String redId = KEY_PREFIX + request.getUserId() + \":\" + timestamp; //将随机金额写入到redis的list中（从队列左边全部push进去） redisTemplate.opsForList().leftPushAll(redId, list); //将红包总个数写入redis中 String redTotalKey = redId + \":total\"; redisTemplate.opsForValue().set(redTotalKey, request.getTotal()); //异步记录红包的全局标识符，红包个数与随机金额列表到数据库 redService.recordRedPacket(request, redId, list); //返回红包的全局唯一标识符给前端 return redId; &#125; else &#123; throw new Exception(\"系统异常---&gt;分发红包参数不合法\"); &#125; &#125; @Override public BigDecimal rob(Integer userId, String redId) &#123; return null; &#125;&#125; 2.4、异步保存红包业务逻辑处理过程数据到数据库 红包业务逻辑处理过程数据接口 1234567891011121314151617181920package com.victor.service.redpacket;import com.victor.model.redpacket.RedPacketRequest;import java.util.List;/** * @Description: 红包业务逻辑处理过程数据记录 * @Author: VictorDan * @Version: 1.0 */public interface IRedService &#123; /** * 记录发红包 * @param dto * @param redId * @param list */ void recordRedPacket(RedPacketRequest dto, String redId, List&lt;Integer&gt; list);&#125; 红包业务逻辑处理过程数据实现类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.victor.service.redpacket.impl;import com.victor.model.redpacket.RedPacketRequest;import com.victor.model.redpacket.RedDetail;import com.victor.model.redpacket.RedRecord;import com.victor.repository.redpacket.RedDetailRepo;import com.victor.repository.redpacket.RedRecordRepo;import com.victor.repository.redpacket.RedRobRecordRepo;import com.victor.service.redpacket.IRedService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.annotation.Async;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;import java.math.BigDecimal;import java.util.Date;import java.util.List;/** * @Description: 红包业务逻辑处理过程数据记录 * @Author: VictorDan * @Version: 1.0 */@Slf4j@Servicepublic class RedServiceImpl implements IRedService &#123; @Autowired private RedRecordRepo redRecordRepo; @Autowired private RedDetailRepo redDetailRepo; @Autowired private RedRobRecordRepo redRobRecordRepo; /** * 发红包记录-异步方式 * @param dto 红包总金额+个数 * @param redId 红包全局唯一标识串 * @param list 红包随机金额列表 */ @Transactional(rollbackFor = Exception.class) @Async @Override public void recordRedPacket(RedPacketRequest dto, String redId, List&lt;Integer&gt; list)&#123; RedRecord redRecord = new RedRecord(); redRecord.setUserId(dto.getUserId()); redRecord.setAmount(BigDecimal.valueOf(dto.getAmount())); redRecord.setTotal(dto.getTotal()); redRecord.setRedPacket(redId); redRecord.setIsActive(true); redRecord.setCreateTime(new Date()); //保存到发红包记录表 redRecordRepo.save(redRecord); RedDetail redDetail; //遍历随机金额列表，将金额等信息设置到对应字段 for (Integer data:list) &#123; redDetail = new RedDetail(); redDetail.setRecordId(redRecord.getId()); redDetail.setAmount(BigDecimal.valueOf(data)); redDetail.setCreateTime(new Date()); redDetail.setIsActive(true); //保存红包明细金额表 redDetailRepo.saveAndFlush(redDetail); &#125; &#125;&#125; 2.5、测试结果 使用PostMan测试接口结果如下： 查看数据库记录 1234#查看发红包表SELECT * FROM victor.red_record;#查看红包明细表，是否对应总金额SELECT sum(amount) FROM victor.red_detail where record_id=1 and is_active=1; 查看发红包表 查看红包明细表 汇总红包明细表的金额与总金额对比","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Redis应用场景之抢红包系统(一)","date":"2018-06-18T02:06:00.000Z","path":"2018/06/18/Redis应用场景之抢红包系统/","text":"一、Redis应用场景之抢红包系统(一)1、业务流程 信息流：包括用户操作背后的请求通信和红包信息在不同用户与用户群中的流转等 业务流：主要包括发红包、点红包和抢红包等业务逻辑 资金流：主要包括红包背后的资金转账和入账等流程 1.1、业务系统流程图 1.2、业务流程分析​ 系统整体业务流程包括2大业务组成：发红包和抢红包。其中的拆红包又可以拆分2个小业务，用户点红包和用户拆红包。 1.2.1、发红包整体业务流程 1.2.2、抢红包整体业务流程 2、业务模块划分 缓存中间件Redis模块：主要用来发红包时缓存红包个数和由随机数算法产生的红包随机金额列表，同时将借助Redis单线程特性和操作的原子性实现抢红包时锁的操作 引入Redis一方面是大大减少高并发情况下频繁查询数据库的操作，减少数据库的压力。 另一方面是提供系统整体响应性能和保证数据的一致性。 3、数据库表设计与环境搭建主要有3张表，发红包记录红包的信息表，发红包时对应的随机金额信息表，抢红包时用户抢到红包的金额记录表 3.1、实体类Model 发红包记录表RedRecord 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.victor.model.redpacket;import com.fasterxml.jackson.annotation.JsonFormat;import lombok.Data;import lombok.ToString;import javax.persistence.*;import java.math.BigDecimal;import java.util.Date;/** * @Description: 发红包记录表 * @Author: VictorDan * @Version: 1.0 */@Data@ToString@Entity@Table(name = \"red_record\")public class RedRecord &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Integer id; /** * 用户id */ private Integer userId; /** * 红包全局唯一标识串 */ private String redPacket; /** * 红包指定可以抢的总人数 */ private Integer total; /** * 红包总金额 */ private BigDecimal amount; /** * 是否有效 */ private Boolean isActive; @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\",timezone = \"GMT+8\") private Date createTime;&#125; 红包明细金额表RedDetail 12345678910111213141516171819202122232425262728293031323334353637383940package com.victor.model.redpacket;import com.fasterxml.jackson.annotation.JsonFormat;import lombok.Data;import lombok.ToString;import javax.persistence.*;import java.math.BigDecimal;import java.util.Date;/** * @Description: 红包明细金额表 * @Author: VictorDan * @Version: 1.0 */@Data@ToString@Entity@Table(name = \"red_detail\")public class RedDetail &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Integer id; /** * 红包记录id */ private Integer recordId; /** * 红包随机金额 */ private BigDecimal amount; /** * 是否有效 */ private Boolean isActive; @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\",timezone = \"GMT+8\") private Date createTime;&#125; 抢红包记录表RedRobRecord 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.victor.model.redpacket;import com.fasterxml.jackson.annotation.JsonFormat;import lombok.Data;import lombok.ToString;import javax.persistence.*;import java.math.BigDecimal;import java.util.Date;/** * @Description: 抢红包记录表 * @Author: VictorDan * @Version: 1.0 */@Data@ToString@Entity@Table(name = \"red_record\")public class RedRobRecord &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Integer id; /** * 用户id */ private Integer userId; /** * 红包全局唯一标识串 */ private String redPacket; /** * 抢到红包的金额 */ private BigDecimal amount; /** * 是否有效 */ private Boolean isActive; /** * 抢到时间 */ @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\",timezone = \"GMT+8\") private Date robTime;&#125; 3.2、持久层Repository 发红包记录表RedRecordRepo 1234567891011121314package com.victor.repository.redpacket;import com.victor.model.redpacket.RedRecord;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Repositorypublic interface RedRecordRepo extends JpaRepository&lt;RedRecord,Long&gt; &#123;&#125; 红包明细金额RedDetailRepo 1234567891011121314package com.victor.repository.redpacket;import com.victor.model.redpacket.RedDetail;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Repositorypublic interface RedDetailRepo extends JpaRepository&lt;RedDetail,Long&gt; &#123;&#125; 抢红包记录表RedRobRecordRepo 1234567891011121314package com.victor.repository.redpacket;import com.victor.model.redpacket.RedRobRecord;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Repositorypublic interface RedRobRecordRepo extends JpaRepository&lt;RedRobRecord,Long&gt; &#123;&#125;","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Redis实战场景分析总结","date":"2018-06-17T02:06:00.000Z","path":"2018/06/17/Redis实战与总结/","text":"一、Redis实战之场景1：缓存穿透1、正常流程​ 项目中使用缓存Redis查询数据的正常流程，如下图 前端用户要访问获取数据时，后端首先会在Redis中查询 如果能查询到数据，则直接将数据返回给用户，流程结束 如果查不到数据，就前往数据库中查询，如果能查到数据，将数据返回给用户，同时将数据塞入缓存Redis中，流程结束 如果在数据库中没有查询到数据，则返回null，同时流程结束。 2、问题分析​ 当查询数据库的时候如果没有查询到数据，就直接返回null给前端用户，流程结束，如果前端频繁发起访问请求，恶意提供数据库中不存在的Key，则此时数据库中查询到的数据永远为null。由于null数据是不存入到Redis中，所以每次访问请求时将查询数据库，如果此时有恶意攻击，发起“洪流”式的查询，则很有可能会对数据库造成极大的压力，甚至压垮数据库。这个叫缓存穿透，就好像永远越过了缓存而直接永远的访问数据库。 3、解决方案​ 当查询数据库时，如果没有查询到数据，将null返回给前端用户，同时将该null数据塞入Redis，并对对应的Key设置一定的过期时间，流程结束，具体过程如下图 4、实战过程1、建立数据库连接1234567891011121314151617#redis#spring.redis.database=0spring.redis.host=localhostspring.redis.port=6379#spring.redis.password=#spring.redis.jedis.pool.max-active=8#spring.redis.jedis.pool.max-wait=-1#spring.redis.jedis.pool.max-idle=8#spring.redis.lettuce.pool.min-idle=0#spring.redis.timeout=1000server.port=8080#mysqlspring.jpa.show-sql=truespring.datasource.url=jdbc:mysql://localhost/victorspring.datasource.username=rootspring.datasource.password=rootspring.datasource.driver-class-name=com.mysql.jdbc.Driver 2、引入依赖12345678910111213141516&lt;!-- jpa--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt;&lt;!-- lombok--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;!-- redis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 3、实体类Person12345678910111213141516171819202122232425262728293031package com.victor.model;import com.fasterxml.jackson.annotation.JsonFormat;import lombok.Data;import lombok.ToString;import javax.persistence.*;import java.io.Serializable;import java.util.Date;/** * @Description: * 使用JPA查询必须要有@Entity和@Table两个注解，否则报错 * @Author: VictorDan * @Version: 1.0 */@Data@ToString@Table(name = \"person\")@Entitypublic class Person implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Integer id; private String userName; private String password; @JsonFormat(pattern = \"yyyy-MM-dd HH:mm:ss\",timezone = \"GMT+8\") private Date createTime; private String status;&#125; 4、操作数据库的PersonRepository1234567891011121314151617package com.victor.repository;import com.victor.model.Person;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;import java.util.List;/** * @Description: * @Author: VictorDan * @Version: 1.0 */@Repositorypublic interface PersonRepo extends JpaRepository&lt;Person,Long&gt; &#123; Person findByUserName(String name);&#125; 5、操作Redis的CachePersonService12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.victor.service;import com.fasterxml.jackson.databind.ObjectMapper;import com.victor.model.Person;import com.victor.repository.PersonRepo;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.ValueOperations;import org.springframework.stereotype.Service;import java.util.concurrent.TimeUnit;/** * @Description: 缓存穿透实战Controller * @Author: VictorDan * @Version: 1.0 */@Service@Slf4jpublic class CachePersonService &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private PersonRepo personRepo; @Autowired private ObjectMapper objectMapper; private static final String keyPrefix = \"person:cache\"; public Person getPerson(String userName) throws Exception &#123; Person person = new Person(); final String key = keyPrefix + userName; ValueOperations valueOperations = redisTemplate.opsForValue(); //先从redis中获取，如果没有则查MySQL，并把查询的数据写入到redis if (redisTemplate.hasKey(key)) &#123; log.info(\"---------获取个人信息---&gt;从Redis缓存中------&gt;姓名为：&#123;&#125;\", userName); Object result = valueOperations.get(key); if (result != null &amp;&amp; result.toString() != null &amp;&amp; !\"\".equals(result.toString())) &#123; person = objectMapper.readValue(result.toString(), Person.class); &#125; &#125; else &#123; log.info(\"--------获取个人信息---&gt;Redis中不存在---&gt;从MySQL中查询----&gt;姓名为：&#123;&#125;\", userName); person = personRepo.findByUserName(userName); if (person != null) &#123; //如果数据库张查询到该人信息，将它序列化后写入到Redis中 String str = objectMapper.writeValueAsString(person); valueOperations.set(key, str); &#125; else &#123; //如果数据库中查不到就将key设置过期时间为30s，实际情况根据实际业务决定 valueOperations.set(key, \"\", 30L, TimeUnit.MINUTES); &#125; &#125; return person; &#125;&#125; 6、前端控制器CachePersonController1234567891011121314151617181920212223242526272829303132333435363738394041package com.victor.controller;import com.victor.model.Person;import com.victor.service.CachePersonService;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.RestController;import java.util.HashMap;import java.util.Map;/** * @Description: 缓存穿透实战Controller * @Author: VictorDan * @Version: 1.0 */@RestController@Slf4jpublic class CachePersonController &#123; @Autowired private CachePersonService cachePersonService; @GetMapping(value =\"/cache/person\") public Map&lt;String,Object&gt; getPerson(@RequestParam String userName)&#123; Map&lt;String,Object&gt; map=new HashMap&lt;&gt;(); map.put(\"code\",0); map.put(\"msg\",\"success\"); try &#123; //调用缓存穿透处理Service得到返回结果，并添加到map中 Person person = cachePersonService.getPerson(userName); map.put(\"data\",person); &#125; catch (Exception e) &#123; log.error(\"查询失败：&#123;&#125;\",e.getMessage()); map.put(\"code\",-1); map.put(\"msg\",\"failed\"+e.getMessage()); &#125; return map; &#125;&#125; 二、Redis实战之场景2：缓存雪崩 在某个时间点，缓存中的key集体发生过期失效使得大量查询数据的请求都落到了DB上，导致数据库负载过高，压力暴增，甚至有可能压垮数据库。 原因分析： 大量的key在某个时间点或者某个时间段过期失效导致。 为了避免这种问题发生，一般的做法是给这些key设置不同的过期时间，随机的TTL，从而错开Redis中的key失效时间点，可以在某种成都上减轻数据库的压力。 三、Redis实战之场景3：缓存击穿 Redis中某个频繁被访问的key或者叫热点key，在不停地扛着前端的高并发请求，当这个key突然在某个时间过期失效的是偶，持续的高并发访问请求就会穿破缓存，直接请求数据库，导致数据库压力在某一瞬间暴增。 原因分析： 主要是热点的key过期失效 实际开发中，既然这个key被当做频繁访问，那么就给这个key设置永不过期，这样前端的请求将几乎永远不会落在数据库上。 四、总结​ 不管是缓存穿透，缓存雪崩，缓存击穿，其实它们最终导致的后果几乎都是一样的，给数据库造成压力，甚至压垮数据库。解决方案也都有一个共同特征，就是加强防线，尽量让高并发的读请求落在缓存中，从而避免直接跟数据库打交道。","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"缓存常识总结","date":"2018-06-16T02:06:00.000Z","path":"2018/06/16/缓存常识总结/","text":"一、缓存穿透，缓存击穿，缓存雪崩1、缓存穿透缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的 并且出于容错考虑，从存储层查不到数据则不写入缓存，这将导致这个 不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大的时候，可能DB就 挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 解决方案 有很多种方法可以有效的解决缓存穿透的问题，最常见的就是布隆过滤器 将所有可能存在的数据hash到一个足够大的bitmap位示图中，一个一定不存在的数据会 被这个bitmap过滤掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单 粗暴的方法(我们采用的就是这种)，如果一个查询返回的数据为空(不管是数据不存在，还是系统故障) 仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过5min 2、缓存雪崩缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求 全部发到DB，DB瞬时压力过重雪崩。 解决方案 缓存失效时的雪崩效应对底层系统的冲击非常可怕，大多数系统设计者考虑用加锁 或者队列的方式保证缓存的单线程(进程)写，从而避免失效时大量的并发请求落到 底层存储系统上，这里分享一个简单方案：将缓存失效时间分散开，比如我们可以在原有失效 时间基础上增加一个随机值，比如1-5min随机，这样每一个缓存的过期时间的重复率就会降低 就很难引发集体失效的事件。 3、缓存击穿对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发的访问，是一种 非常热点数据，这个时候，需要考虑一个问题：缓存被击穿的问题，这个和缓存雪崩的区别 在于，缓存击穿针对某一key缓存，缓存雪崩是很多key 缓存在某个时间点过期的时候，恰好在这时间点对这个key有大量的并发请求过来，这些请求 发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把 后端DB压垮。 解决方案 1、使用互斥锁mutex key 常用的做法是：使用mutex，就是在缓存失效的时候，判断拿出来的值为空，不是立即去load db 而是先使用缓存工具的某些带成功操作返回值得操作，比如redis的setnx 去set一个mutex key。当操作返回成功时，在进行load db的操作，并回设缓存，否则，就重试 整个get缓存的方法。 setnx==set if not exists的缩写。也就是只有不存在的时候才设置，可以利用它来实现缓存击穿 12345678910111213141516public String get(String key)&#123; String value=redis.get(key); if(value==null)&#123; //设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db if(redis.setnx(key_mutex,1,3*60)==1)&#123;//代表设置成功 vlaue=db.get(key); redis.set(key,value,expire_secs); redls.del(key_mutex); &#125;else&#123;//这个时候代表同时候的其他线程已经load db并回设到缓存了，这个时候，重试获取缓存值就行。 sleep(50); get(key);//重试 &#125; &#125;else&#123; return vlaue; &#125;&#125; 4、为什么要用redis缓存Redis就是一个数据库，不过传统数据库不同是Redis数据是存在内存中。 Redis被广泛用于缓存方向，Redis也经常用来做分布式锁。 5、为什么要用Redis而不用map/guava做缓存？缓存分本地缓存和分布式缓存。Java中使用自带的map或者guava实现的是本地的缓存，轻量快速，生命周期随着JVM的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用Redis或者memcached之类是分布式缓存，在多实例情况下，各实例共用一份缓存数据，缓存具有一致性。 缺点：Redis需要保持服务的高可用，整个程序架构比较复杂。 6、Redis和Memcached的区别 Redis支持更丰富的数据类型，也就意味着支持更复杂的应用场景 不仅仅是支持简单的key、value数据，同时还提供list，set，zset，hash等数据的存储。memached支持简单的数据类型，string Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用，而Memecached把数据全部存在内存之中，重启就没了。 集群模式：Memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。大师Redis是原生支持cluster模式的 Memcache是多线程，非阻塞IO复用的网络模型 Redis使用单线程的多路IO复用模型 7、Redis常见数据结构以及使用场景分析 String 常用命令：set,get,decr,incr,mget String数据结构是简单的key-value，value可以是String，也可以是数字。 常规key-value缓存应用，常规计数，微博数，粉丝数。 Hash 常用命令：hget，hset，hgetall hash是一个string类型的field和value的映射表 hash特别适合用来存储对象。可以用hash数据结构来存储用户信息，商品信息 1234567key=javatest12343value=&#123; 'id':1, 'name':'vitor', 'age':12, 'locatoin':'shanghai'&#125; List 常用命令：lpush，rpush，lpop，lrange list就是链表，Redis list的应用场景非常多，比如微博的关注列表，粉丝列表，消息列表等功能都是用Redis的list结构来实现 Redis list的实现是一个双向链表，既可以支持反向查找和遍历，但是带了额外的内存开销。 还可以利用list实现简单的高性能分页。 Set 常用命令：sadd,spop,smembers,sunion等 set对外提供的功能与list类似是一个列表功能。只不过set可以自动去重 Sorted Set 常用命令：zadd,zrange,zrem,zcard 和set相比，sorted set增加了一个权重参数score，使得集合的元素能够按照score进行排序。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息。 8、Redis设置过期时间Redis中有个设置时间过期的功能，对存储在Redis数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的，如我们一般项目中的token或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样会严重影响项目性能。 所以，我们set key的时候，都可以给一个expire time过期时间，通过过期时间我们可以指定这个key可以存活的事件。如果假设你设置了一批key只能存活1个小时，那么接下来1个小时后，redis怎么删除key？ 1、定期删除+惰性删除 定期删除:redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查是否过期，如果过期就删除。 这里是随机抽取的，因为随机抽取，假如redis存了几十万的key，每隔100ms就遍历所有的设置过期时间的key话，就会给cpu带来很大复杂 惰性删除：定期删除可能会导致很多过期key到了时间并没有被删除掉。就有了惰性删除。假如你的过期key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个key，才会被redis给删除掉。这就是所谓的惰性删除。 但是仅仅通过设置过期时间还是有问题的，如果定期删除漏掉了很多过期的key，然后也没有及时去查，也就没有惰性删除。如果大量过期key堆积在内存里，导致redis内存耗尽，redis内存的淘汰机制就出现了。 2、Redis内存淘汰机制MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据？ Redis.conf中有相关注释 Redis提供6中数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰。 volatile-ttl:从已设置过期时间的数据集（server.db[i].expires)中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集中任意选择数据进行淘汰 allkeys-lru:当内存不足以容纳新写入数据的时候，在key空间中，移除最近最少使用的key（这是最常用的） allkey-radom：从数据集（server.db[i].dict）中任意选择数据淘汰 volatile-lfu:从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰 allkeys-lfu:当内存不足容纳新写入数据，在key空间，移除最不经常使用的key ​ 注：redis设置过期时间以及内存淘汰机制 3、Redis持久化机制（怎么保证Redis挂掉之后再重启数据可以进行恢复）很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器，机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置 Redis不同于Memcached重点是Redis可以持久化，支持两种不同持久化操作。 快照（Snapshotting RDB）： Redis可以通过创建快照来获取存储在内存里面的数据在某个时间点的副本。Redis创建快照后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提供Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 123456#在900s之后，如果至少有1个key发生变化，redis就会自动触发BGSAVE命令创建快照save 900 1#在300s之后，如果至少有10个key发生变化，redis就会自动触发BGSAVE命令save 300 10#在60s之后如果有1w个key发生变化，redis就会自动触发BGSAVE命令save 60 10000 追加文件（append-only file,AOF） 与快照持久化相比，AOF持久化的实时性更好，因此也成为主流的持久化方案。 默认情况下redis没有开启AOF方式持久化 1appendonly yes 开启AOF持久化后每执行一条会更改redis中的数据的命令，redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件位置相同都是通过dir参数设置 默认文件名为appendonly.aof 123456#每次有数据修改发生都会写入aof，这样会严重降低redis的速度appendfsync always#每秒钟同步一次，显式的将多个写命令同步到硬盘appendfsync everysec#让操作系统决定何时同步appendfsync no 为了兼顾数据和写入性能，用户可以考虑appendfsync everysec选项，让redis每秒同步一次AOF文件，redis性能几乎没收到任何影响。即使系统崩溃，用户也最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作，redis还会放慢自己的速度来适应硬盘的最大写入速度。 Redis4.0对于持久化机制的优化 4.0开始支持RDB和AOF的混合持久化（默认关闭，可以通过配置项aof-use-rdb-preamble开启） 混合持久化开启，AOF重写的时候就会直接把RDB的内容写到AOF文件开头。这样的好处可以结合RDB和AOF的优点，可以快速加载同时避免丢失过多的数据。 缺点是：AOF里面的RDB部分也不是AOF的压缩格式，可读性差。 AOF重写 AOF重写可以产生一个新的AOF文件，这个新的文件和原有的AOF文件所保存的数据库状态一样，但是体积更小 AOF重写是一个有歧义的名字，功能是通过读取数据库中的键值来实现。程序无需对现有的AOF文件进行任何读入，分析重写， 4、Redis事务redis通过multi，exec，watch等命令来实现事务transaction功能。 事务提供了一种将多个命令请求打包，然后一次性，按顺序执行多个命令的机制。 5、解决Redis的并发竞争key的问题所谓redis的并发竞争key的问题也就是多个系统同时对一个key进行操作，但是最后执行的顺序和我们期望的顺序不同。也就导致了结果的不同 解决方案： ​ 分布式锁zookeeper和redis都可以实现分布式锁。如果不存在redis的并发竞争key问题及，不要使用分布式锁，这样影响性能。 ​ 基于zookeeper临时有序结点可以实现的分布式锁 大致思想：每个客户端对某方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序结点。 判断是否获取锁的方式很简单，只需要判断有序结点中序号最小的一个。当释放所得时候，只要将这个瞬时结点删除就行。同时也避免服务挂了导致锁无法释放，从而产生死锁问题。 什么是RedLock特性： 安全性：互斥访问，也就是永远只有一个client能拿到锁 避免死锁：最终client都可能拿到锁，不会出现死锁的情况，原本锁住某资源的client crash或者出现了网络分区 容错性：只要大部分redis结点存活就可以正常提供服务。","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"分布式锁的使用","date":"2018-06-15T02:06:00.000Z","path":"2018/06/15/分布式锁的3种实现/","text":"一、分布式锁1、为什么要使用分布式锁​ 如果需要对一个共享变量进行多线程同步访问的时候，可以使用java多线程进行运行。 ​ 如果是单机应用，也就是所有的请求都会分配到当前服务器的JVM内部，然后映射为操作系统的线程进行处理，而这个共享变量只是在这个JVM内部的一块内存空间。 ​ 后续业务发展，需要做集群，一个应用需要部署好几台机器然后做负载均衡。 一个共享变量A，存在JVM1，JVM2，JVM3，这3个JVM内存中，这个变量A主要体现是在一个类中的一个成员变量，是一个有状态的对象。 比如UserController中的一个int类型的成员变量。如果不加任何控制的话，变量A同时都会在JVM分配一块内存，3个请求发过来，同时对这个变量操作，显然结果是不对的。即使不是同时发过来，3个请求分别操作3个不同JVM内存区域的数据，变量A之间不共享的话，也不具有可见性的话，处理结果也是不对的。 2、解决问题​ 为了保证一个方法或者属性在高并发情况下的同一时间只能被同一个线程运行，在传统单体应用单机部署的情况下，可以使用Java并发处理相关的API（比如ReentrantLock，Synchronized）进行互斥控制。在单机环境中，Java中提供了很多并发处理的API。但是随着业务发展的需要，原来的单机部署的系统被演化为分布式集群系统后，由于分布式系统多线程，多进程并且分布在不同机器上，这将使得原单机部署情况下的并发控制锁策略失效，单纯的Java API并不能呢提供分布式锁的能力。为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题。 3、分布式锁具备的条件 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行。 高可用的获取锁与释放锁 高性能的获取锁与释放锁 具备可重入特性 具备锁失效机制，防止死锁 具备非阻塞锁特性，也就是没有获取到锁将直接返回获取锁失败。 4、分布式锁的3种实现方式​ 目前大型网站应用都是分布式部署，分布式场景中的数据一致性问题一直是一个重要的问题。出现了CAP定理。任何一个分布式系统都无法同时满足一致性(Consistency)，可用性(Availablility)，分区容忍性(Partition tolerance)，只能同时满足2个。 ​ 目前在绝大多数场景中，都是需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性就行。也就是只要这个最终时间是在用户可以接受的范围内就行。 ​ 在很多场景中，我们为了保证数据的最终一致性，需要很多技术支持。比如分布式事务，分布式锁。需要保证一个方法在同一个时间内只能被同一个线程执行。 基于数据库实现分布式锁 基于缓存Redis等，实现分布式锁 基于Zookeeper实现分布式锁 1、基于数据库实现排它锁(互斥锁)1.1、解决方法1：使用唯一索引约束。12345678DROP TABLE IF EXISTS `method_lock`;CREATE TABLE `method_lock` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL COMMENT '锁定的方法名', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 12--获取锁insert into method_lock(method_name,desc) values('xxxService','methodName';) 这种解决方法，对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功。 1.2、解决方法2：先获取锁的信息，然后更新状态。12345678910DROP TABLE IF EXISTS `method_lock`;CREATE TABLE `method_lock` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL COMMENT '锁定的方法名', `state` tinyint NOT NULL COMMENT '1:未分配；2：已分配', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `version` int NOT NULL COMMENT '版本号', `PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 123456--先获取锁的信息select id,method_name,state,version from method_lock where state=1 and method_name='methodName';--占有锁update method_lock set state=2,version=2,update_time=now()where method_name='methodName' and state=1 and version=2;--如果没有更新影响到的这一行数据，说明这个资源已经被别人占位了。 1.3、这2种方案都是有弊端 缺点： 这把锁很依赖数据库的可用性，如果数据库是单个节点，一旦数据库挂了，则业务系统不可用了。 这把锁还没有失效时间，一旦解锁失败(更新state状态失败)，就会导致锁记录一直在数据库中，其他线程无法获取锁。 这把锁是非阻塞的，因为数是insert操作，一旦插入失败就会直接报错，没有获取锁的线程就不会进入到等待队列，要想再次获取锁就必须再次触发获取锁的操作。 这把锁是不可重入的，同一个线程在没有释放锁之前无法再次得到这个所，因为数据已经在数据库中存在了。 解决方案： 数据库是单点的话，可以数据库集群，然后数据之前进行同步，一旦挂掉，快速切换到备份库上。 没有失效时间，可以搞一个定时任务，每隔一段时间把数据库中的超时的数据delete。 他是非阻塞的，可以搞一个while循环，直到insert成功后再返回。 它是不可重入的，可以在数据库表上加一个字段，记录当前获取锁的机器的主机信息和线程信息，那么下次获取锁的时候可以先查询数据库，如果当前及其的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就行了。 2、基于Redis实现分布式锁12## 获取锁使用的命令：SET resource_name_value NX PX 30000 12345678910111213141516171819202122232425262728293031@Autowiredprivate RedisTemplate redisTemplate;private static final String LOCK=\"distribute_key\";public boolean test(String lockKey)&#123; Boolean lock=false; /** * 加锁的时候，在try块中，尝试获取锁，在finaly块中释放锁。 */ try &#123; //如果key不存在，则新增，如果存在则不改变已经有的值 lock = redisTemplate.opsForValue().setIfAbsent(lockKey, LOCK); log.info(\"test方法是否获取到锁：\" + lock); if (lock) &#123; //todo 业务逻辑 redisTemplate.expire(lockKey, 1, TimeUnit.MINUTES); return true; &#125; else &#123; log.info(\"test方法没有获取到锁，不执行任务！\"); &#125; &#125;finally &#123; if(lock)&#123; redisTemplate.delete(lockKey); log.info(\"test方法任务结束，释放锁！\"); &#125;else&#123; log.info(\"test方法没获取到锁，不需要释放锁！\"); &#125; return false; &#125;&#125; 2.1、弊端首先是这主从结构存在明显的竞争状态： 客户端A从master获取到锁，在master将锁同步到slave之前，master挂掉了，slave节点被晋级为master节点，客户端B取的了同一个资源被客户端A已经获取到的另外一个锁。安全失效。 3、基于Zookeeper实现​ Zookeeper中有节点的概念，Zk的数据存储结构就像一棵树，这棵树由节点组成，节点叫Znode，有点类似Linux的文件系统结构。 ​ Znode有4种类型： 持久节点Persistent 默认的节点类型，创建节点的客户端与zk断开连接后，这个节点仍然存在。 持久节点顺序节点Persistent_Sequential 顺序节点，就是在创建节点的时候，zk根据创建的时间顺序给该节点名称进行编号。 临时节点Ephemeral 和持久节点相反，当创建节点的客户端与zk断开后，临时节点会被删除。 临时顺序节点（Ephemeral_Sequential） 临时顺序节点就是结合了临时节点和顺序节点的特点：在创建临时节点的时候，zk根据创建的时间顺序给节点名称进行编号，当创建节点的客户端与zk断开连接后，临时顺序节点被删除。 3.1、Zookeeper分布式锁的原理Zk分布式锁使用的是临时顺序节点，也就是客户端断开连接后，会删除该节点，然后客户端创建节点的时候，会按照时间顺序来对临时节点命名。 获取锁 首先在zk中创建一个持久节点ParentLock，当第一个客户端想要获取锁的时候，需要在ParentLock这个节点下面创建临时顺序节点Lock1。 Client1查找ParnentLock下面所有的临时顺序节点并排序，然后判断自己创建的节点Lock1是不是顺序最靠前的一个节点。如果是第一个节点，就成功获取锁。 这个时候，再有一个客户端Client2前来获取锁，首先在ParentLock下再创建一个临时顺序节点Lock2 Client2查找ParnentLock下面所有的临时顺序节点并排序，判断它自己创建的节点Lock2是不是顺序最靠前的一个，结果发现节点Lock2并不是最小的。于是，Client2向排序仅仅比他靠前的节点Lock1注册Watcher，用来监听Lock1节点是否存在。这意味者此时Client2抢锁失败，进入了等待状态。 假如这个时候，又有一个客户端Client3前来获取锁，则在ParentLock下再创建一个临时顺序节点Lock3 Client3查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock3是不是顺序最靠前的一个，结果同样发现节点Lock3并不是最小的。 于是，Client3向排序仅仅比他靠前的节点Lock2注册Watcher，用来监听Lock2节点是否存在，这意味这Client3同样抢锁失败，进入了等待状态。 这样就形成了，Client1获取了锁，Client2监听了Lock1，Client3监听额Lock2。从而形成了一个等待队列，很像Java中ReentrantLock所依赖的。 释放锁 1、任务完成，客户端显示释放 当任务完成后，Client1会显示调用删除节点Lock1的指令。 2、任务执行过程中，客户端崩溃 得到锁的Client1在执行任务过程中，突然挂了，则会断开与Zk服务端的连接，根据临时节点的特性，它自己创建的节点也会被自动删除。 由于Client2一直监听者Lock1的存在状态，当Lock1节点被删除后，Client2会立刻得到通知，这时候Client2会再次查ParentLock下面的所有节点，确认自己创建的节点Lock22是不是目前最小的节点，如果是最小，则Client2就成功获取了锁。 同理，如果Client2也因为任务完成或者节点崩溃而删除了Lock2，那么Client3会收到通知。 最终，Client3成功获取锁。 3.2、实现方法可以直接使用zk第三方库Curator客户端，这个客户端封装了一个可重入的锁服务。 123456789101112131415161718192021222324252627/** * InterProcessMutex是分布式锁的实现，acquire方法用户获取锁，release释放锁 */ @Autowired private InterProcessMutex interProcessMutex; public boolean tryLock(long timeout,TimeUnit unit)&#123; try &#123; interProcessMutex.acquire(timeout,unit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; @Autowired private ExecutorCompletionService executorService; public boolean unLock()&#123; try &#123; interProcessMutex.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //executorService.schedule(new Cleaner(client,path),delayTimeForClean,TimeUnit.MILLISECONDS); &#125; return true; &#125; 缺点： ​ 性能上，并没有缓存那么高，因为每次在创建锁和释放锁的过程中，都需要动态创建按，销毁临时顺序节点来实现锁功能。zk中创建和删除节点只能通过leader服务器来执行，然后将数据同步到所有的follwer服务器上。 ​ 使用zk也有可能带来并发问题，只是并不常见。由于网络抖动，客户端与zk集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。也有可能产生并发问题，并不常见，因为zk有重试机制，一旦zk集群检测不到客户端的心跳，你就会重试，Curator客户端（操作zk的客户端）支持多种重试策略 ，多次重试之后还是不行的话，就会删除临时节点。因此，选择一个合适的重试策略也很重要，要在锁的粒度和并发之前找一个平衡。 5、总结 分布式锁 优点 缺点 Zk 1、有封装好的框架，容易实现。2、有等待锁的队列，大大提升抢锁效率。 添加和删除节点性能比较低 Redis Set和Del执行的性能比较高 1、实现复杂，需要考虑超时，原子性，误删的情况。2、没有等待锁的队列，只能在客户端自旋来等待，效率比较低。 6、3种方案比较使用3种方法，就如同CAP一样，在复杂性，可靠性，性能等方面无法同事满足。应该根据不同应用场景，选择最合适的分布式锁。 6.1、从理解的难易程度上，从低到高数据库——-&gt;Redis——-&gt;Zk 6.2、从实现的复杂程度上，从低到高zk————-&gt;Redis——-&gt;数据库 6.3、从性能角度，从低到高数据库————–&gt;zk———&gt;Redis 6.4、从可靠性角度，从低到高zk—————&gt;Redis—————&gt;数据库","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"分布式锁","date":"2018-06-14T02:06:00.000Z","path":"2018/06/14/一、分布式锁/","text":"一、分布式锁在单机场景下，可以使用Java里的内置所来实现进行同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。 阻塞所通常使用互斥量来实现： 互斥量mutex为0表示有其他进程在使用锁，此时处于锁定状态。 互斥量mutex为1表示未锁定状态。 1和0可以用一个整形值表示，也可以用某个数据是否存在表示。 1、数据库的唯一索引获取锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否在锁定状态。 存在以下几个问题： 锁没有失效时间，解锁失败的话，其他进程无法再获得该锁。 只能是非阻塞锁，插入失败直接就报错，无法重试。 不可重入，已经获得锁的进程也必须重新获取锁。 2、Redis的SETNX指令使用SETNX（set if not exist）指令插入一个键值对，如果key已经存在，那么会返回false，否则插入成功并返回true。 SETNX指令和数据库的唯一索引类似，保证了只存在一个key的键值对，那么可以用一个key的键值对是否存在来判断是否存于锁定状态。 EXPIRE指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引方式中释放锁失败的问题。也就是数据库的唯一索引解决方案的升级而已，只是可以设置过期时间。 3、Redis的RedLock算法使用了多个Redis实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。多个Redis实例实现，也就是即使一台Redis挂了，还可以有其他的服务可用。 尝试从N个互相独立的Redis实例中获取锁。 计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数(N/2+1)，半数以上的实例上获取锁，那么就认为获取锁成功了。 如果锁获取失败，就到每个实例上释放锁。 4、Zookeeper的有序节点1、Zookeeper抽象模型Zookeeper提供了一种树型结构的命名空间，/app1/p_1节点的父节点为app1。有点类似linux系统文件系统，文件目录树 2、节点类型 永久节点：不会因为会话结束或者超时而消失。 临时节点：如果会话结束或者超时就会消失。 有序节点：会在节点名的后面加上一个数字后缀，并且是有序的。例如生成的有序节点为/lock/node-0000000000，它的下一个有序节点就是/lock/node-0000000001，以此类推。 3、监听器为一个节点注册监听器，在节点状态发生改变的时候，会给客户端发送消息 。 4、分布式锁实现 创建一个目录/lock。 当一个客户端需要获取锁时，在/lock下创建临时的且有序的子节点。 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁。否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直到获取锁为止。 执行业务代码，完成后，删除对应的子节点。 5、会话超时如果一个已经获的锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其他会话就可以获取锁。可以看到zookeeper分布式锁不会出现数据库的唯一索引的分布式锁释放锁失败的问题。 6、羊群效应一个节点未获得锁，只需要监听自己前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其他所有子节点都会收到通知(羊群效应)，而我们秩序网它的后一个子节点收到通知。","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"JVM相关","date":"2018-06-13T02:46:34.000Z","path":"2018/06/13/JVM相关/","text":"一、JVM相关的笔记简历上可以写，熟悉JVM架构与GC垃圾回收机制，以及相应的参数调优，有过在Linux上进行系统优化的经验。 面试官让你谈谈jvm优化，我是根据淘宝周志明那哥们写的那本深入理解java虚拟机书，99*%的优化的是堆Heap，1%的优化的是方法区Method Area，虚拟机栈Stack是私有的，不可能优化，最多是栈溢出StackOverFlow，调整一下栈的大小-xss参数静态变量+常量+类信息+运行时常量池存在方法区。方法区一般放的是类的构造函数和接口的代码。所有定义的方法的信息都保存在方法区。引用存在虚拟机栈里。实例变量存在堆里面。 1、栈​ 栈，也叫栈内存，主管java程序的运行，是在线程创建是创建，它的生命周期跟随线程的生命周期，线程结束栈内存也就释放，对于栈来说，不存在垃圾回收，生命周期和线程一致，是线程私有的。 基本类型变量和对象的引用变量，都是在函数的栈内存里分配。 1.1、栈存储的是什么？ 栈是先进后出，比如你main方法调用test1，test1调用test2，等等，所以main在最底下，可以看作一个栈帧对应一个方法。 栈帧里面存3种类型变量： 本地变量（local variables），输入参数和输出参数，以及方法内的变量。 栈操作（ operand stack）出栈和入栈的操作 栈帧数据（Frame data）：包括类文件，方法等，言下之意，每一个方法就是一个栈帧。 1234567891011public class&#123; //test的栈帧在上面，直接如此调用，把栈给撑爆了，所以栈溢出 public void test()&#123; test(); &#125; //main方法的栈帧在最底部 public static void main(String[] Args)&#123; test(); &#125;&#125; //死循环，或者循环递归调用,出现的异常java.lang.StackOverflowError 2、堆Heap​ 一个JVM实例只存在一个堆内存，堆内存大小是可以调节的。类加载器读取了类文件后，需要把类、方法、成员变量存在堆内存中，保存所有引用类型的真实信息，以方便执行器执行，堆内存分为三个部分： 2.1、Young generation space 新生代 young新生代分为3个区： Eden space伊甸区 Survivor 0 space 幸存0区 Survivor 1 space 幸存1区 2.2、Tenure generation space 养老区 Old2.3、Permanent space 永久区 Perm二、3种JVM SUN公司的HotSpot虚拟机 BEA公司的JRockit虚拟机 IBM公司的J9VM虚拟机","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"分布式常识总结","date":"2018-06-13T02:06:00.000Z","path":"2018/06/13/分布式常识总结/","text":"一、基础知识1、分布式基础理论1、什么是分布式系统？《分布式系统原理与范型》定义： 分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统。 分布式系统(distributed system)是建立在网络之上的软件系统 eg：比如我们在使用京东商城进行购物的时候，我们登陆京东商城完成整个购物。对于我们的用户使用来说，我们体会到的是一个完整的京东商城系统。实际上京东商城后面有成千上万台独立的计算机为我们提供服务。这些所有计算机合起来它构成了一个完整的京东商城系统。京东商城它是一个分布式系统。 2、为什么要有分布式系统？随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已经无法应对，分布式服务架构以及流动计算架构势在必行，亟需一个治理系统确保架构有条不紊的演进。 eg：比如我们的应用是淘宝级别的，可能没有任何一台服务器能抵挡得住像双11那么大的用户量，每秒要处理几十万的数据。有几个亿的实时数据处理。面对这种情况怎么办呢？我们既然有一个超大型系统，把这个大型系统划分成一个个的小功能模块，把这些功能模块分布在各个计算机上，那这么多的计算机就合起来组成一个完整的系统为我们提供服务。这就是我们分布式的架构思想。毕竟三个臭皮匠，顶个诸葛亮，这么多的计算机合起来就能抵御住非常大的流量。在这种情况下，我们把大型系统，分成一个个小模块(小服务)。而这些小模块(小服务)之间也有着千丝万缕的关系。比如A可能要用到B。而B呢可能要用到C，C可能要用到D，D回过头又得用到B等等。这么复杂的关系我们亟需要一个治理系统来维护我们这个复杂的关系，保证我们系统有条不紊的进行。而我们的Dubbo就能维护和治理这些复杂的关系。那么有了dubbo，我们就能轻而易举的使用分布式的思想来架构我们一个应用。 3、应用架构的发展演变 单一应用架构 当网站流量很小时，只需要一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 比如：超市的收银系统，公司的一些管理系统，一些小型的网站应用等等。比如我们有一个小型的购物网站，它只能少量的人来提供服务，流量很小。比如网站里面有非常多的功能：用户，商品，订单。由于我们网站访问量还是很小的，我们没必要把它做成一个大型的分布式应用。所以说，我们可以将所有的功能都放在一个应用里面。然后将整个应用打包部署放在服务器里。开发简单，部署也简单。某一天网站的访问量大，服务器受不了了。我们可以再来一个服务器，同样都来跑这个应用。两个服务器一起来分担流量压力。 但是单一应用会带来一些缺点：1、扩展不容易，比如某个功能模块修改了，我们要把整个应用重新打包部署到服务器上。服务器如果有多台，我们就扩展不容易。2、我们协同开发也不容易，大家都去改这一个应用，可能都会改乱，不利于我们应用的维护。 垂直应用架构 当我们的应用规模不断扩大。比如我们以前的应用只要10Mb 我们应用不断扩大到了500Mb，还是单体应用放在一个服务器里面。单台服务器跑这么大的应用，压力也是很大，光靠增加每一个服务器，已经不能带来性能上的极大提升了。那么接下来就要重新考虑一下我们应用的架构方式。我们就来到第二阶段，叫垂直的应用架构(Vertical Application)。 我们可以将那么大的单体应用，垂直拆分成几个互不相干的小应用。这些小应用独立部署在每一个服务器上。哪一个小应用的功能增加了，我们只改这些小应用，包括这些小应用流量很大了，我们把它多放在几台服务器上。垂直拆分的好处： 1、我们分工合作很容易，每一个人负责开发和维护不同的应用。这样就互不干扰。 2、性能扩展也是很容易的，比如我们的用户应用访问量增大了。我们就把它多复制上几份放到几台服务器上。我们三五台服务器上一起来跑。我们扩展是很容易的。不像是之前的单体应用，一扩展就带上整个应用。我们只是扩展小应用，想要扩展哪一部分就扩展这一个应用就行了，其他的应用先不用动。但是呢？即使是这样的架构也是有一些缺点的：因为每一个小的应用都是完整的，包含界面，业务逻辑，数据库等等。但是呢？我们市场上对界面的要求会经常发生变化，美工天天会改这个页面，一个页面一改，我们整个应用都要重新部署，现在没法做到：1、界面和业务逻辑的实现分离。2、我们应用后来也会增多。垂直应用从用户模块，商品模块，订单模块，增多到支付模块，物流模块等等，这种情况下，我们不能理想情况下，让应用跟应用直接会互相独立。我们的订单可能会用到用户，因为我们创建订单的时候，可能会访问到用户，包括订单还要用到商品的信息。而物流，支付可能会用到订单的一些内容。我们会发现，应用与应用之间可能会进行交互。 3、应用不可能完全的独立。大量的应用之间需要交互。 接下来我们怎么办呢？可以采取分布式的应用架构(Distributed Service)，当我们的垂直应用越来越多，应用之间还会进行交互，我们可以把它们的核心业务单独的抽取出来。 分布式服务架构(Distributed Service) 用户的应用抽取出用户的界面web和用户的业务逻辑。订单的界面web和订单的业务逻辑等等。这样做的好处是业务逻辑不变的情况下，我们只想改界面，那界面的服务器重新启动一下，我们的核心的业务逻辑还在其他服务器上。比如我们的用户界面，它要展示我们的用户信息，还要展示商品信息。只需要调对应的服务业务就行了。包括业务之间也可能需要互调。但是现在还要一个最大的问题：我们的用户的Web可能在A服务器上，用户的业务可能放在B服务器上，订单放在C服务器上等。如果我们的A服务器要去调B服务器的功能，我们以前写在一个应用里面，A调B，那直接A调B的方法就行了，我们叫进程内通讯，因为他们都在一个服务器上，都是同一个tomcat,同一个进程内通讯。但是现在我们发现A跟B已经分割两地了，它在两处了，这两处我们代码之间如何互调呢?我们把这个调用称为RPC(远程过程调用)。也就是说分布式应用架构下，最核心的难点是RPC远程过程调用，以及如何拆分业务，提升业务的复用程度，那么此时一个好的分布式服务框架(RPC)，帮我们来解决远程过程调用的这个框架，就能极大的简化我们的开发。那么这样的架构就会高枕无忧了吗？随着我们的业务不断增加，我们分拆的服务也越来越多，有成千上万的服务器，再来跑各种不同的服务，而出现的一些资源浪费情况，就尤为严重，比如我们的用户业务，这块的访问量比较小，有100台服务器在跑，而我们的商品业务，它的访问量比较大却只有10台服务器在跑，那我们呢就应该有一个能基于我们访问压力的调度中心，能帮我们实时的来监控这些数据，来动态的调度，提高我们这个系统的资源利用率，你跑的服务器有点多了，我们来减上几台，让更多的服务器取来跑业务量更大的这些业务，这个时候呢，就可以采用流动计算架构(Elastic Computing)。 流动计算架构(Elastic Computing) 服务调度中心，负责维护服务之间的复杂关系以及实时管理我们整个服务集群。比如说A服务器访问量大了，我给A冬天的多来几台服务器。第一台有100个请求，第二台2个请求，第三台有10000个请求，那么下次进来的时候，就找比较闲的服务器来处理请求，比如第二台，提高我们整个服务集群的利用率。 4、RPC 什么叫RPC RPC(Remote Procedure Call)：是指远程过程调用，是一种进程间通信方式，它是一种技术思想，而不是规范。它允许程序调用另一个地址空间(通常是共享网络的另一台机器)的过程或者函数。而不是程序员显式的编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 RPC基本原理 过程：A想要调用B服务器的应用，A先跟B服务器在网络建立一个Socket连接，然后将我们要调用B的一些信息，B的哪个方法，包括方法名，用的参数是什么，把这些信息传递给B服务器，然后B服务器上Server stub这个小助手收到这些信息，一看A想要调用我里面的信息，它就将这个方法一调用，包括调这个方法用到的参数，也通过网络传递了过来，那我们调方法的时候也将参数传过去，我们B服务器里面的Server stub将方法调用完，调用完以后，方法会有一个返回值，然后这个返回值也通过网络传输给A，A收到返回值以后，A的client stub小助手把这个返回值给我们Client functions。 整个RPC的核心：A，B两个服务器之间建立网络之间，它们之间进行通信，这样的话A想要用B里面的服务，就像A给B发送一个请求一样，B的方法将响应的结果，交给A。 5、dubbo​ dubbo是一款高性能、轻量级的开源Java RPC框架，提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 ​ register：服务注册中心。 ​ provider：服务提供者 ​ consumer：服务消费者，消费我们提供的服务。 ​ container：指的是dubbo框架容器。 ​ monitor：是监控中心，服务提供者provider，服务消费者的一些监控信息都会发到监控中心。 6、zookeeper注册中心7、dubbo的Monitor监控中心GitHub地址 然后用mvn clean package打成一个jar包，然后在控制台用java -jar的命令启动就行。 注意：启动监控中心，之前必须启动zookeeper，否则，导致监控中心失败。","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"JVM探究","date":"2018-06-12T02:06:00.000Z","path":"2018/06/12/JVM的运行过程：/","text":"JVM的运行过程：​ Java源文件，通过编译器，能够生产相应得.class字节码文件。而字节码文件又通过JVM中的解释器，编译成特定机器上的机器码。 Java源文件–&gt;编译器–&gt;字节码文件 字节码文件–&gt;JVM–&gt;机器码 每一种平台的解释器是不同的，但是实现的虚拟机是相同的，这也就是Java为什么可以跨平台的原因，当一个程序从开始运行，这时虚拟机就开始实例化了，多个程序启动就会存在多个虚拟机实例。程序退出或者关闭，则虚拟机实例消亡，多个虚拟机实例之间数据不能共享。 1、线程​ 线程指的是程序执行过程中的一个线程实体。JVM允许一个应用并发执行多个线程。Hotspot JVM中的Java线程与原生操作系统线程有直接的映射关系。当线程本地存储、缓冲区分配、同步对象、栈、程序计数器等准备好之后，就会创建一个操作系统原生的线程(守护线程Daemon)。Java线程结束，原生线程随之被回收。操作系统负责调度所有线程，并把它分配到任何可用的CPU上。当原生线程初始化完毕，就会调用Java线程的run()方法。当线程结束时，会释放原生线程和Java线程的所有资源。 Hotspot JVM后台运行的系统线程主要有下面几个： 线程 解释 虚拟机线程（VM thread） 这个线程等待JVM到达安全点操作出现。这些操作必须要在独立的线程里执行，因为当堆修改无法进行时，线程都需要JVM位于安全点。这些操作的类型有：stop-the-world垃圾回收、线程栈dump、线程暂停、线程偏向锁（biased locking）解除 周期性任务线程 这线程负责定时器事件（也就是中断），用来调度周期性操作的执行 GC线程 这些线程支持JVM中不同的垃圾回收活动 编译器线程 这些线程在运行时将字节码动态编译成本地平台相关的机器码 信号分发线程 这个线程接收发送到JVM的信号并调用适当的JVM方法处理 3、JVM内存3.1、线程私有Thread Local 程序计数器PC 指向虚拟机字节码指令的位置 唯一一个无OOM的区域（OOM：Out Of Memory内存溢出） 虚拟机栈VM Stack 虚拟机栈和线程的生命周期相同 一个线程中，每调用一个方法创建一个栈帧（Stack Frame） 栈帧的结构 本地变量表Local Variable 操作数栈Operand Stack 对运行时常量池的引用 Runtime Constant Pool Reference 异常 线程请求的栈深度大于JVM所允许的深度StackOverflowError 若JVM允许动态扩展，若无法申请到足够内存OutOfMemoryError 本地方法栈Native Method Stack 异常 线程请求的栈深度大于JVM所允许的深度StackOverflowError 若JVM允许动态扩展，若无法申请到足够内存OutOfMemoryError 线程共享Thread Shared 方法区（永久代）Method Area 运行时常量池Runtime Constant Pool 类实例区（Java堆）Objects 新生代 eden from survivor to survivor 老年代 异常OutOfMemoryError 直接内存Direct Memory 不受JVM GC管理 3.2、JVM内存区域解释JVM内存区域主要分为： 线程私有区域：程序计数器、虚拟机栈、本地方法栈 线程共享区域：Java堆，方法区 直接内存 线程私有数据区域生命周期与线程相同，依赖用户线程的启动/结束，而创建/销毁在Hotspot JVM内，每个线程都与操作系统的本地线程直接映射，因此这部分内存区域的存/否跟随本地线程的生/死对应。 线程共享区域随虚拟机JVM的启动/关闭而创建/销毁 直接内存并不是JVM运行时数据区的一部分，但也会被频繁的使用：在JDK1.4引入的NIO提供了基于Channel与Buffer的IO方式，它可以使用Native函数库直接分配堆外内存，然后使用DirectByteBuffer对象作为这块内存的引用进行操作，这样就避免了在Java堆和Native堆中来回复制数据，因此在一些场景中可以显著提供性能。 1、程序计数器（线程私有）​ 一块较小的内存空间，是当前线程所执行的字节码的行号指示器，每条线程都要有一个独立的程序计数器，这类内存也称为“线程私有”的内存。 ​ 正在执行Java方法的话，计数器记录的是虚拟机字节码指令的地址（当前指令的地址）。如果还是Native方法，则为空。 ​ 这个内存区域是唯一一个在虚拟机中没有规定任何OutOfMemoryError情况的区域。 2、虚拟机栈（线程私有）​ 是描述Java方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧（Stack Frame）,用来存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 ​ 栈帧（Frame）是用来存储数据和部分过程结果的数据结构，同时也被用来处理动态链接（Dynamic Linking）、方法返回值和异常分派(Dispatch Exception)。栈帧随着方法调用而创建，随着方法结束而摧毁—无论方法是正常完成还是异常完成（抛出了在方法内未被捕获的异常）都算作方法结束。 当前线程： 当前栈帧Current Stack Frame 局部变量表 操作栈 动态链接 返回地址 3、本地方法区（线程私有）​ 本地方法区和Java Stack作用类似，区别是虚拟机栈为执行Java方法服务，而本地方法栈则为Native方法服务，如果一个VM实现使用C-linkage模型来支持Native调用，那么该栈将会是一个C栈，但是HotSpot VM直接就把本地方法栈和虚拟机栈合二为一。 4、堆（Heap-线程共享）-运行时数据区​ 是被线程共享的一块内存区域，创建的对象和数组都保存在Java堆内存中，也是垃圾收集器进行垃圾收集的最重要的内存区域。由于现代VM采用分代收集算法，因此Java堆从GC的角度还可以细分为：新生代(Eden区，From Survivor区、To Survivor区)和老年代。 5、方法区/永久代（线程共享）​ 我们常说的永久代（Permanent Generation），用于存储被JVM加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。HotSpot VM把GC分代收集扩展到方法区，也就是使用Java堆的永久代来实现方法区，这样HotSpot的垃圾收集器就可以像管理Java堆一样管理这部分内存，而不必为方法区开发专门的内存管理器（永久代的内存回收的主要目标是针对常量池的回收和类型的卸载，因此收益一般很小） ​ 运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池(Constant Pool Table)，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。Java虚拟机对Class文件的每一部分（自然也包括常量池）的格式都有严格的规定，每一个字节用于存储哪种数据都必须符合规范上的要求，这样才会被虚拟机认可、装载和执行。 3.3、JVM运行时内存Java堆从GC的角度还可以细分为：新生代（Eden区、From Survivor区和To Survivor区）和老年区 堆（Heap） 新生代（1/3）堆空间 ：Eden(8/10)，From Survivor(1/10)，To Survivor(1/10) 老年代(2/3)堆空间 1、新生代​ 用来存放新生的对象。一般占据堆的1/3空间。由于频繁创建对象，所以新生代会频繁触发MinorGC进行垃圾回收。新生代又分为Eden区、From Survivor区，To Survivor三个区。 Eden区 Java新对象的出生地 （如果新创建的对象占用内存很大，则直接分配到老年代）。当Eden区内存不够的时候就会触发MinorGC，对新生代区进行一次垃圾回收。 From Survivor区 上一次GC的幸存者，作为这一次GC的被扫描者 To Survivor区 保留了一次MinorGC过程中的幸存者 MinorGC的过程 复制—&gt;清空—&gt;互换 MinorGC采用复制算法。 1、 Eden、From Survivor复制到To Survivor，年龄+1​ 首先，把Eden和From Survivor区域中存活的对象复制到To Survivor区域(如果有对象的年龄以及达到了老年的标准，则赋值到老年区)，同时把这些对象的年龄+1（如果To Survivor不够位置了就放到老年区） 2、清空Eden、From Survivor​ 然后，清空Eden和From Survivor中的对象 。 3、To Survivor和From Survivor互换​ 最后，To Survivor和From Survivor互换，原To Survivor成为下一次GC的From Survivor区。 2、老年代​ 主要存放应用程序中生命周期长的内存对象。 ​ 老年代的对象比较稳定，所以MajorGC不会频繁执行。在进行MajorGC前一般都先进行了一次MinorGC，使得有新生代的对象晋身入老年代，导致空间不够用时才触发。当无法找到足够大的连续空间分配给新创建的较大对象的时候，也会提前触发一次MajorGC进行了垃圾回收腾出空间。 ​ MajorGC采用标记清除算法：首先扫描一次所有老年代，标记出存活的对象，然后回收没有标记的对象。MajorGC的耗时比较长，因为要扫描再回收 。MajorGC会产生 内存碎片，为了减少内存损耗，我们一般需要进行合并或者标记出来方便下次直接分配。当老年代也满了装不下的时候，就会抛出OOM(OutOfMemory)异常。 3、永久代​ 内存的永久保存区域。主要存放Class和Meta(元数据)的信息，Class被加载的时候被放入永久区域，它和存放实例的区域不同，GC不会再逐层序运行期对永久区域进行清理。所以这也导致了永久代的区域会随着加载的Class的增多而胀满，最终抛出OOM异常。 1、Java8与Meta元数据​ 在Java8中，永久代已经被移除，被一个成为元数据区(元空间)的区域所取代。元空间的本质和永久代类似，元空间与永久代之间最大的区别是：元空间并不在虚拟机中，而是使用本地内存。在默认情况下，元空间的大小仅受本地内存限制，类的元数据放入Native Memory，字符串池和静态变量放入Java堆中，这样可以加载多少类的元数据就不再由MaxPermSize控制，而由系统的实际可用空间来控制。 4、垃圾回收与算法","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Java的锁分类","date":"2018-06-11T02:06:00.000Z","path":"2018/06/11/Java锁/","text":"一、Java锁分类1、乐观锁 乐观锁是一种乐观思想，认为读多写少，遇到并发写的可能性低，每次拿数据的时候都认为别人不会修改，所以不会上锁。 但是在更新的时候会判断一下在期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作。比较跟上一次的版本号，如果一样则更新，如果失败则要重复读-比较-写的操作。 Java的乐观锁基本都是通过CAS操作实现的。CAS是一种更新的原子操作，比较当前值是否一样，一样则更新，否则失败。 2、悲观锁 悲观锁就是悲观思想，认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block直到拿到锁。java中的悲观锁就是 synchronized，AQS框架下的锁则是先尝试CAS乐观锁去获取锁，获取不到才会转换为悲观锁，比如RetreenLock。 3、自旋锁 自选锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进行阻塞挂起状态，他们只需要等一等（自旋），等持有锁的线程释放锁后就可以立即获取到锁，这样就避免了用户线程和内核的切换消耗。 线程自旋需要消耗CPU，说白了就是让CPU再做无用功，如果一直获取不到锁，那么线程也不能一直占用CPU自旋做无用功，所以需要设定一个自旋等待的最大时间。 如果持有锁的线程执行的时间超过自旋等待的最大时间仍没有释放锁，就会导致其他的线程在最大等待时间内还是获取不到锁，这时其他线程就会停止自旋进入阻塞状态。 自旋锁是一种思想，一般需要配合CAS使用 java.util.concurrent.atomic包下的原子类是自旋锁 4、可重入锁 不可重入的话，一个锁在嵌套中使用会把自己锁死 synchronized和ReentrantLock都是可重入锁，可以放心用 5公平锁/非公平所 synchronized是非公平锁，ReentrantLock默认构造函数也是非公平锁 非公平锁的性能比公平锁性能高很多 6、互斥锁/共享锁 互斥与共享的概念简单，任何语言都存在 7、偏向锁/轻量级锁/重量级锁 synchronized底层优化，偏向锁、轻量级锁是针对重量级锁做优化而提出来的定义 这些优化大部分情况下对于开发来讲是透明的，默认开启的 8、分段锁 分段锁不是一种实际的锁，而是一种思想 ConcurrentHashMap是实现锁分段机制 9、锁优化9.1、减少锁持有时间只用在要求线程安全的程序上加锁 9.2、减小锁粒度将大对象(被多线程访问的对象)，拆分成小对象，大大增加并行度，降低锁竞争。降低锁竞争，偏向锁，轻量级锁成功率才会提高。最典型的减小锁粒度的例子就是：分段锁ConcurrentHashMap。 10、锁分离 常见的锁分离就是读写锁ReadWriteLock，根据功能进行分离成读锁和写锁，这样读读不互斥，读写互斥，写写互斥，保证了线程安全，还提高了性能。 LinkedBlockingQueue从头部取出数据，从尾部放数据。 11、锁粗化为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量短，也就是在使用公共资源后，应该立即释放锁。如果对同一个锁不停地进行请求，同步和释放，本身也会消耗系统的资源，反而不利于性能的优化 12、锁消除锁消除是在编译器级别的事情，如果发现不可能被共享的对象，就可以消除这些对象的加锁操作，多数是因为编码不规范引起的。 二、Java常用锁 synchronized ReentrantLock ReadWriteLock Semaphore synchronized 按照加锁范围大小，分为类锁和对象锁 按加锁方法，分为代码块加锁和方法加锁 注：对象锁只会影响单个对象，类锁会影响这个类下的所有对象 1、分析 每个对象都有一个monitor对象，加锁就是在竞争monitor对象 代码块加锁是在前后分别加上monitorenter和monitorexit指令来实现的 方法加锁是通过一个标记位来判断的 2、误解 JDK1.5，synchronized是一个重量级加锁操作，需要调用操作系统的接口，则导致性能低，给线程加锁消耗的时间比有用操作消耗时间要多。 JDK1.6，synchronized进行了锁优化，有自旋锁，锁消除 ，锁粗化，轻量级锁，偏向锁等，效率有了提高。而之后的JDK1.7,1.8都对关键字的实现原理实现了优化。 3、总结 引入了偏向锁和轻量级锁，都是在对象头中有标记位，不需要经过操作系统加锁 锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。这种升级过程叫做锁膨胀 JDK1.6中默认是开启偏向锁和轻量级锁，可以通过-XX:UseraBiasedLocking来禁用偏向锁 ReentrantLock1、使用 lock.lock()和lock.unlock() 12345678910111213141516171819202122232425262728293031323334353637383940414243package lock;import java.util.concurrent.locks.ReentrantLock;/** * @Description: * @Author: VictorDan * @Date: 18-11-20 下午4:15 * @Version: 1.0 */public class LockTest extends Thread &#123; public static ReentrantLock lock = new ReentrantLock(); public static int flag = 0; public LockTest(String name) &#123; super(name); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10000; i++) &#123; try&#123; lock.lock(); flag++; &#125;finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; LockTest lockTest1=new LockTest(\"thread1\"); LockTest lockTest2=new LockTest(\"thread1\"); lockTest1.start(); lockTest2.start(); lockTest1.join(); lockTest2.join(); /** * 如果不加锁，则flag为小于20000的任意数 */ System.out.println(flag);//2000 &#125;&#125; lock.tryLock(long timeout, TimeUnit unit) 尝试获取锁，在时间范围内没有拿到就会返回false，不会永久构成死锁。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.anjuke.ai;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;/** * @Description: * @Author: VictorDan * @Date: 18-11-20 下午4:24 * @Version: 1.0 */public class TryLock extends Thread &#123; public static ReentrantLock lock = new ReentrantLock(); public TryLock(String name)&#123; super(name); &#125; @Override public void run() &#123; try &#123; if (lock.tryLock(5, TimeUnit.SECONDS)) &#123; Thread.sleep(6000); &#125; else &#123; System.out.println(this.getName() + \" get lock failed\"); &#125; &#125; catch (Exception e) &#123; &#125; finally &#123; if (lock.isHeldByCurrentThread()) &#123; System.out.println(\"lock.isHeldByCurrentThread: \" + this.getName()); lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) &#123; TryLock t1 = new TryLock(\"TryLockTest1\"); TryLock t2 = new TryLock(\"TryLockTest2\"); t1.start(); t2.start(); &#125;&#125;//TryLockTest2 get lock failed//lock.isHeldByCurrentThread: TryLockTest1 公平锁： 一般锁是不公平的，不一定先来的线程先得到锁，后来的锁就后得到锁，不公平锁可能会产生饥饿现象 公平锁就是先来先服务。不会产生饥饿现象，但是公平所性能比费公平所性能差很多 1public static ReentrantLock lock = new ReentrantLock(true); 2、分析 ReentrantLock是基于AQS实现的，而AQS的基础是CAS。所以搞定AQS，就搞定了ReentrantLock。 ReentrantLock分为公平锁和非公平锁 而ReentrantLock，CountDownLatch，Semaphore都是通过AQS实现的 3、总结 synchronized能做的，ReentrantLock都能做，并且还能做很多，但是synchronized仍有用武之地 ReentrantLock相比synchronized的优势是可中断，公平锁，多个锁。这种情况下使用ReentrantLock。只要是synchronized能做到的，还是使用synchronized","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Redis的基本使用","date":"2018-05-11T02:06:00.000Z","path":"2018/05/11/Redis的基本使用/","text":"一、Redis的基本使用总结1、RedisTemplate与StringRedisTemplate的使用 两个操作组件都可以用来操作存储字符串类型数据信息 对于RedisTemplate，还可以用来操作List，Set，SortedSet，Hash等2、Redis的List列表 和Java的List类型很类似，用来存储一系列具有相同类型的数据，底层对于数据的存储和读取可以理解为一个数据队列，往List中添加数据，相当于从队列中的队尾插入，获取数据从队头获取数据。 总结：使用List存入对象的时候，JavaBean必须要实现Serializable接口序列化，因为Spring会将对象先序列化后再存入Redis，否则会报错。3、Redis的Set集合 Redis的Set和Java里的Set集合类似性质，存储的数据相同类型且不重复，也就是Redis集合中的数据是唯一的，底层的数据结构是通过Hash表来实现的。所以对它增、删、查操作的复杂度都是O(1)。 使用场景：在实际开发中，set常用来解决重复提交，剔除重复id等业务场景4、Redis的有序集合Zset Redis的Zset和Set具有某些相同特征，也就是存储的数据不重复，不同之处在Zset可以通过底层的Score(分数/权重)值对数据进行排序，实现存储的集合数据有序不重合。 使用场景：Zset常用来充值排行榜，积分排行榜，成绩排名等应用场景。5、Redis的Hash存储 Redis的Hash有点类似Java的HashMap，底层数据结构是Key-Value的哈希表。 使用场景：当需要存入redis中的对象信息具有某种共性，为了减少缓存中key的数量，应考虑采用hash存储。6、使用Redis总结 如果已经用Redis将JavaBean保存到集合类型中，如果后续再修改JavaBean的类型，重新插入，会报错local class incompatible: stream classdesc serialVersionUID = 72892319061181, local class serialVersionUID = -3998150864330771094 出现这个问题后，不论是在model类里面添加private static final long serialVersionUID = -6743567631108323096L;还是重新Build都还是有这些问题。是因为第一次保存数据到Redis进入需要进行序列化，而修改JavaBean的属性类型，会导致序列化的UID不一样，自然无法操作。如果想要重新插入数，需要把所有的对应的key清空把redis服务关闭或者清空后再登录就好了。 因为redis存的是字节数据，所以model必须要序列化，改变model的属性值类型，序列化UID会发生变化。7、Redis中Key的失效与判断是否存在 在有些业务场景下，redis的key对应的数据不需要永久保留，这个时候就需要对缓存中的key进行清理。 redis缓存架构中，delete与expire操作都可以用来清理key。 区别： delete操作是人为手动触发， expire只需要提供一个ttl过期时间，key就自动失效，会被自动清理7.1、在调用SETEX方法的时候，指定key的过期时间 方法1：valueOperations.set(key,”expire操作”,10L, TimeUnit.SECONDS); 方法2：valueOperations.set(key,”expire操作”);redisTemplate.expire(key,10L,TimeUnit.SECONDS); 7.2、判断key是否存在 使用redisTemplate.hasKey(key)判断key是否存在 二、Redis使用的实例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315package com.victor.redis;import com.fasterxml.jackson.databind.ObjectMapper;import com.victor.AbstractTest;import com.victor.model.Account;import com.victor.model.User;import lombok.extern.slf4j.Slf4j;import org.junit.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.*;import java.util.ArrayList;import java.util.List;import java.util.Map;import java.util.Set;import java.util.concurrent.TimeUnit;@Slf4jpublic class RedisTest extends AbstractTest &#123; /** * 由于之前已经自定义注入RedisTemplate组件，所以可以直接自动装配 */ @Autowired private RedisTemplate redisTemplate; /** * redis的set简单测试 */ @Test public void getDataFromRedis() &#123; log.info(\"-------------开始RedisTemplate操作组件实战-------------\"); final String content = \"你淡爷爷\"; final String key = \"redis:template:victor:string\"; //redis通用的操作组件 ValueOperations valueOperations = redisTemplate.opsForValue(); log.info(\"写入redis中的内容：&#123;&#125;\", content); //将字符串信息写入到redis中 valueOperations.set(key, content); Object result = valueOperations.get(key); log.info(\"从redis中读取的内容：&#123;&#125;\", result); &#125; /** * json的序列化与反序列化框架 */ @Autowired private ObjectMapper objectMapper; @Test public void testUser() &#123; log.info(\"-------------开始RedisTemplate操作组件实战-------------\"); User user = new User(1, \"victor\", \"你淡爷爷\"); ValueOperations valueOperations = redisTemplate.opsForValue(); final String key = \"redis:template:victor:object\"; String content; try &#123; content = objectMapper.writeValueAsString(user); valueOperations.set(key, content); log.info(\"写入redis中的对象信息为：&#123;&#125;\", user); Object result = valueOperations.get(key); if (result != null) &#123; User value = objectMapper.readValue(result.toString(), User.class); log.info(\"从redis中读取内容并反序列化的结果为：&#123;&#125;\", value); &#125; &#125; catch (Exception e) &#123; log.error(\"json序列化失败：&#123;&#125;\", e.getMessage()); &#125; &#125; /** * RedisTemplate的特列，专门用来处理缓存中value的数据类型为String类型。 */ @Autowired private StringRedisTemplate stringRedisTemplate; /** * StringRedisTemplate的简单测试 */ @Test public void testStringRedisTemplate() &#123; log.info(\"-------------开始StringRedisTemplate操作组件实战-------------\"); final String content = \"StringRedisTemplate的使用\"; final String key = \"redis:string\"; ValueOperations&lt;String, String&gt; valueOperations = stringRedisTemplate.opsForValue(); log.info(\"写入redis的内容：&#123;&#125;\", content); valueOperations.set(key, content); String result = valueOperations.get(key); log.info(\"从redis中获取的内容为：&#123;&#125;\", result); &#125; /** * 采用StringRedisTemplate将对象信息系列化为json格式字符串后写入redis * 然后从redis中读取出来，最后反序列化 */ @Test public void testStringRedisTemplateObject() &#123; log.info(\"-------------开始StringRedisTemplate操作组件实战-------------\"); User user = new User(1, \"victor\", \"你淡爷爷\"); ValueOperations&lt;String, String&gt; valueOperations = stringRedisTemplate.opsForValue(); final String key = \"redis:string:object\"; String content; try &#123; content = objectMapper.writeValueAsString(user); valueOperations.set(key, content); log.info(\"写入redis中的对象信息为：&#123;&#125;\", user); Object result = valueOperations.get(key); if (result != null) &#123; User value = objectMapper.readValue(result.toString(), User.class); log.info(\"从redis中读取内容并反序列化的结果为：&#123;&#125;\", value); &#125; &#125; catch (Exception e) &#123; log.error(\"json序列化失败：&#123;&#125;\", e.getMessage()); &#125; &#125; /** * Redis中的List的测试使用 */ @Test public void testRedisList() &#123; log.info(\"-----------Redis中的List数据类型测试使用-----------------\"); List&lt;User&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; User user = new User(i, i + \"号男嘉宾\", \"张\" + i); list.add(user); &#125; log.info(\"构造已经排好序的User的列表为：&#123;&#125;\", list); final String key = \"redis:list\"; ListOperations listOperations = redisTemplate.opsForList(); for (User user : list) &#123; //往redis的list中添加数据---&gt;从list队尾添加 listOperations.leftPush(key, user); &#125; log.info(\"---------获取Redis中List的数据---&gt;从队尾中获取----------\"); Object data = listOperations.rightPop(key); User user; while (data != null) &#123; user = (User) data; log.info(\"当前获取到的数据为：&#123;&#125;\", user); data = listOperations.rightPop(key); &#125; &#125; /** * Redis的Set的测试使用 */ @Test public void testRedisSet() &#123; log.info(\"-----------Redis中的Set数据类型测试使用-----------------\"); List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"hello\"); list.add(\"victor\"); list.add(\"你淡爷爷\"); list.add(\"redis\"); list.add(\"list\"); list.add(\"set\"); list.add(\"hello\"); list.add(\"victor\"); log.info(\"待处理的用户姓名列表：&#123;&#125;\", list); final String key = \"redis:set\"; SetOperations setOperations = redisTemplate.opsForSet(); /** * 遍历访问list，并剔除相同的名字存入的Redis的set */ for (String str : list) &#123; setOperations.add(key, str); &#125; //从Redis的set中获取对象的集合 Object result = setOperations.pop(key); while (result != null) &#123; log.info(\"当前获取到的数据为：&#123;&#125;\", result); result = setOperations.pop(key); &#125; &#125; /** * Redis的zset的操作使用 */ @Test public void testRedisZset() &#123; log.info(\"-----------Redis中的Zset数据类型测试使用-----------------\"); List&lt;Account&gt; list = new ArrayList&lt;&gt;(); for (int i = 101; i &lt; 108; i++) &#123; Account account = new Account(i + \"\", i + Math.random()); list.add(account); &#125; log.info(\"待处理的用户姓名列表：&#123;&#125;\", list); final String key = \"redis:zset\"; //获取有序集合zset的操作组件 ZSetOperations zSetOperations = redisTemplate.opsForZSet(); /** * 遍历访问list，并剔除相同的名字存入的Redis的set */ for (Account account : list) &#123; zSetOperations.add(key, account, account.getMoney()); &#125; //从Redis的zset中获取对象的集合的size大小 Long size = zSetOperations.size(key); log.info(\"-------------从小到大排名--------------\"); Set&lt;Account&gt; range = zSetOperations.range(key, 0L, size); for (Account data : range) &#123; log.info(\"从redis中读取money排序列表，当前获取到的数据为：&#123;&#125;\", data); &#125; log.info(\"-------------从大到小排名--------------\"); Set&lt;Account&gt; reverseRange = zSetOperations.reverseRange(key, 0L, size); for (Account data : reverseRange) &#123; log.info(\"从redis中读取money排序列表，当前获取到的数据为：&#123;&#125;\", data); &#125; &#125; /** * Redis的Hash测试使用 */ @Test public void testRedisHash() &#123; log.info(\"-----------Redis中的Hash数据类型测试使用-----------------\"); List&lt;User&gt; userList = new ArrayList&lt;&gt;(); List&lt;Account&gt; accountList = new ArrayList&lt;&gt;(); for (int i = 100010; i &lt; 100016; i++) &#123; User user = new User(i, \"张三\" + i, \"user\" + i); userList.add(user); &#125; for (int i = 101; i &lt; 106; i++) &#123; Account account = new Account(i + \"\", i + Math.random()); accountList.add(account); &#125; final String userKey = \"redis:hash:user\"; final String accountKey = \"redis:hash:account\"; //获取Hash存储的操作组件 HashOperations hashOperations = redisTemplate.opsForHash(); /** * 分别遍历userList和accountList加入的redis */ for (User user : userList) &#123; hashOperations.put(userKey, user.getId().toString(), user); &#125; for (Account account : accountList) &#123; hashOperations.put(accountKey, account.getUserId(), account); &#125; /** * 获取user对象列表和account对象列表 */ Map&lt;String, User&gt; userMap = hashOperations.entries(userKey); log.info(\"获取userList的数据：&#123;&#125;\", userMap); Map&lt;String, Account&gt; accountMap = hashOperations.entries(accountKey); log.info(\"获取accountList的数据：&#123;&#125;\", accountMap); //获取指定的user对象 String userId = \"100010\"; User user = (User) hashOperations.get(userKey, userId); log.info(\"获取指定的user对象：&#123;&#125;-&gt;&#123;&#125;\", userId, user); //获取指定的account对象 String accountId = \"101\"; Account account = (Account) hashOperations.get(accountKey, accountId); log.info(\"获取指定的user对象：&#123;&#125;-&gt;&#123;&#125;\", accountId, account); &#125; /** * redis中存数据的时候，给key设置过期测试使用 * @throws Exception */ @Test public void testRedisKeyExpire1() throws Exception&#123; final String key=\"redis:expire:key\"; ValueOperations valueOperations = redisTemplate.opsForValue(); /** * 方法1：在往redis中set数据的时候，提供一个TTL，ttl时间一到，redis中的key自动失效，会被清理 */ valueOperations.set(key,\"expire操作\",10L, TimeUnit.SECONDS); /** * 等待5秒判断key是否还存在 */ Thread.sleep(5000); Boolean hasKey = redisTemplate.hasKey(key); Object value = valueOperations.get(key); log.info(\"等待5秒----&gt;判断key是否存在：&#123;&#125;，对应的值为：&#123;&#125;\",hasKey,value); /** * 再等待5秒判断key是还存在 */ Thread.sleep(5000); hasKey = redisTemplate.hasKey(key); value = valueOperations.get(key); log.info(\"再等待5秒----&gt;判断key是否存在：&#123;&#125;，对应的值为：&#123;&#125;\",hasKey,value); &#125; /** * redis中存数据的时候，给key设置过期测试使用 * 使用redisTemplate的expire方法 * @throws Exception */ @Test public void testRedisKeyExpire2() throws Exception&#123; final String key=\"redis:expire:key\"; ValueOperations valueOperations = redisTemplate.opsForValue(); /** * 方法2：在往redis中set数据的时候，使用redisTemplate的expire方法给key设置过期时间 */ valueOperations.set(key,\"expire操作\"); redisTemplate.expire(key,10L,TimeUnit.SECONDS); /** * 等待5秒判断key是否还存在 */ Thread.sleep(5000); Boolean hasKey = redisTemplate.hasKey(key); Object value = valueOperations.get(key); log.info(\"等待5秒----&gt;判断key是否存在：&#123;&#125;，对应的值为：&#123;&#125;\",hasKey,value); /** * 再等待5秒判断key是还存在 */ Thread.sleep(5000); hasKey = redisTemplate.hasKey(key); value = valueOperations.get(key); log.info(\"再等待5秒----&gt;判断key是否存在：&#123;&#125;，对应的值为：&#123;&#125;\",hasKey,value); &#125;&#125;","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"UDF自定义函数","date":"2018-04-13T02:06:00.000Z","path":"2018/04/13/一、UDF的使用手册/","text":"一、UDF的使用手册1、简介Hive中，提供了丰富的内置函数，比如trim(),cast(),max(),count(),coalesce()等之外， 还允许用户用java开发自定义的UDF函数。 1.1、开发自定义UDF函数的2种方式： 继承org.apache.hadoop.hive.ql.exec.UDF; 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF; 1.2、总结： 针对简单数据类型：String,Integer等，可以使用UDF。 针对复杂数据类型：Array,Map,Struct等，可以使用GenericUDF GenericUDF还可以在函数开始之前和结束之后做一些初始化和关闭的处理操作。 2、UDF使用UDF实现对String类型的字符串取HashMD5 3、GenericUDF","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Netty","date":"2018-02-15T02:10:28.000Z","path":"2018/02/15/Netty/","text":"一、Netty1、Netty原理​ Netty是一个高性能、异步事件驱动的NIO框架，基于Java NIO提供的API实现。它提供了TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。 2、Netty高性能​ 在IO编程中，当需要同时处理多个客户端接入请求时，可以用多线程或者IO多路服用技术进行处理。IO多路服用技术通过把多个IO的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程、多进程模型相比，IO多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降低了系统的维护工作量，节省了系统资源。 ​ 与Socket类和ServerSocket类相对应，NIO也提供了SocketChannel和ServerSocketChannel两种不同的套接字通道实现。 2.1、多路复用通讯方式​ Netty架构按照Reactor模式设计和实现。 1、服务端通信 NIO Server打开ServerSocketChannel 绑定监听地址InetSocketAddress Reactor Thread创建Selector，启动线程 NIO Server将ServerSocketChannel注册到Selector，监听SelectionKey.OP_ACCEPT Selector轮询就绪的Key handleAccept()处理新的客户端介入 IOHandler设置新建客户端连接的Socket 向Selector注册监听读操作SelectionKey.OP_READ handleRead()异步读请求消息到ByteBuffer IOHandler开始decode请求 然后异步写ByteBuffer到SocketChannel 2、客户端通信 NIO Client打开SocketChannel 设置SocketChannel为非阻塞模式，同时设置TCP参数 异步连接服务端Server 判断连接结果，如果连接成功，则向多路复用器注册读事件OP_READ，如果连接失败，则向Reactor线程的多路复用器注册OP_CONNECT事件 然后Reactor Thread创建Selector，启动线程 Selector轮询就绪的Key handlerConnect()连接IOHandler 判断连接是否完成，如果完成则执行向多路复用器注册读事件OP_READ handlerRead()异步读请求消息到ByteBuffer 然后在decode请求消息 异步写ByteBuffer到SocketChannel Netty的IO线程NioEventLoop由于聚合了多路复用器Selector，可以同时并发处理成百上千个客户端Channel，由于读写操作都是非阻塞的，这就可以充分提升IO线程的运行效率。避免由于频繁IO阻塞导致的线程挂起。 2.2、异步通讯NIO​ 由于Netty采用异步","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"大数据","date":"2018-02-05T02:06:00.000Z","path":"2018/02/05/一、大数据BigData学习路线/","text":"一、大数据BigData1、简介​ 大数据本质也是数据，但是有了新的特征，包括数据来源广阔，数据格式多样化(结构化数据，非结构化数据，Excel文件，文本文件等) 数据量大(最少也是TB级别的，甚至可能是PB级别)，数据增长速度快。 2、需要考虑的问题 数据来源广，数据如何采集汇总？ 对应出现了Sqoop，Cammel，Datax等工具。 数据采集之后，如何存储？ 对应出现了GFS，HDFS，TFS等分布式文件存储系统 由于数据增长速度块，数据存储就必须水平扩展，于是出现集群 数据存储之后，该如何通过运算快速转化为统一的格式，该如何快速运算出自己想要的结果？ 对应的MapReduce，来解决这样的问题；但是写MapReduce需要写大量Java代码，所以出现了Hive，Pig等将SQL转化为MapReduce的解析引擎。 但是普通的MapReduce处理数据只能一批一批的执行处理Job跑脚本，时间延迟太长，是离线计算，跑的都是前一天的数据。为了实现实时处理，每输入一条数据就能得到结果，于是出现了Storm/JStorm这样的低延时的流式计算框架。 但是如果同时需要批处理和流处理，按照上面的，就需要搭建2个集群，一个Hadoop集群(包括HDFS+MapReduce+Yarn)和Storm集群。不便于管理，所以出现了Spark这样的一站式的计算框架，既可以进行批处理，也可以进行流处理(实质上是微批处理)。如今又进一步发展为Flink(快速流式处理计算框架)。 业务处理的通用架构是什么样的？ Lambda架构 Kappa机构出现 为了提高工作效率，加快运行速度，又出现了一些辅助工具 Ozzie，Azkaban：定时任务调度的工具 Hue，Zepplin：图形化任务执行管理，结果查看工具。类似公司的云窗系统 Scala语言：编写Spark程序的最佳语言，当然也可以使用Python或者Java Allluxio，Kylin：通过对存储的数据进行预处理，就快了运算速度的工具，能够快速的查询得到结果。 3、大数据的必备技能 日志收集：Flume，Fluentd，ELK 流式计算：Storm/JStorm，Spark Streaming，Flink Streaming 编程语言：Java，Python，Scala 机器学习库：Mahout，MLib 消息队列：Kafka，RabbitMQ 数据分析/数据仓库：Hive，SparkSQL，FlinkSQL，Pig，Kylin Hadoop家族：Zookeeper，HBase，Hue，Sqoop，Oozie 资源调度：Yarn，Mesos。 分布式数据存储：HDFS 大数据通用处理平台：Hadoop，Spark，Flink 4、必须掌握的技能 Java高级开发：JVM，并发 Linux基本操作 Hadoop（HDFS+MapReduce+Yarn） HBase(JavaAPI操作+Phoenix) Hive（Hql基本操作和原理理解） Kafka Storm Python Spark（Core+SparkSQL+Spark Streaming） 辅助小工具(Sqoop/Flume/jOzzie/Hue等)","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Hive","date":"2018-01-01T02:06:00.000Z","path":"2018/01/01/Hive/","text":"一、Hive基本概念1.1、什么是HiveHive：由Facebook开源用于海量结构化日志的数据统计（Hive是分析框架，它不能存储数据的） Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。 本质：将HQL转化成MapReduce程序 数据仓库通过SQL进行统计分析 将SQL语言中常用的操作（select,where,group等）用MapReduce写成很多模板 所有的MapReduce模板封装在Hive中 client客户端，用户根据业务需求编写相应的SQL语句 然后会去Hive中找对应的MapReduce模板 通过Hive框架匹配出相应的MapReduce模板 运行MapReduce程序，生成相应的分析结果 Hive可以看做是Hadoop的客户端，装一个就够了啊，Hive它有不存储数据，不做计算，它就将HQL转化为MapReduce Hive处理的数据存储在HDFS Hive分析数据底层的实现是MapReduce 执行程序运行在Yarn上 1.2、Hive的优缺点1.2.1、优点 操作接口采用类SQL语法，提供快速开发的能力（简单，容易上手） 避免了去写MapReduce，减少开发人员得 学习成本 HIve的执行延迟比较高，因此HIve常用数据分析，对实时性要求不高的场合 比如你凌晨一两点，启动一个定时脚本，跑一下数据，实时性不高，不是那种你来一条数据，就立马处理的那种，就是说它可以处理，但是给反馈，交互出结果比较慢，因为它跑MR，而MR整个启动，提交啊，切片啊非常的慢，所以它应用的场景一般是离线 Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高，启动太慢 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 1.2.2、缺点1、Hive的HQL表达能力有限 迭代式算法无法表达 数据挖掘方面不擅长 2、Hive的效率比较低 Hive自动生成的MapReduce作业，通常情况下不够智能化 Hive调优比较困难，粒度比较粗 1.3、Hive架构原理 CLI：命令行接口，以命令行的形式输入SQL语句进行数据操作，有点类似shell Meta store：元数据存储，就是存的是表跟数据之间的位置对应关系，有点类似索引，Hive不会存储Meta store，会存在mysql或者Derby中 Meta store作用：客户端连接Meta store服务，Meta store再去连接Mysql数据库来存取元数据，有了Meta store服务，就可以有多个客户端可以同时连接，而切这些客户端不需要知道MySQL的数据库用户名和密码，只需要连接Meta store服务就行。 因为Meta store的元数据不断的修改，更新，所以Hive元数据不适合存在HDFS中，一般存在RDBMS中 HIve没有专门的数据存储格式，也没有为数据建立索引，Hive中所有数据都存储在HDFS中 1.4、使用Hive 直接使用MapReduce的问题： 人员学习成本太高 项目周期要求太短 MapReduce实现复杂查询逻辑的开发难度太大 使用Hive 更友好的接口：操作借口采用类SQL的语法，快速开发能力 学习成本低：避免了写MapReduce，减少开发学习成本 更好的扩展性：可自由扩展集群规模而不需要重启服务，用于还可以自定义函数 Hive使用场景 Hive与传统的关系型SQL不同，支持绝大多数的语句DDL，DML以及聚集函数，连接查询，条件查询。 Hive不适合联机事务处理，不提供实时查询功能，适用于基于大量不可变的批处理作业。 Hive的使用 不支持Insert into，Update，Delete操作 不支持等值连接 123select * from table a,table b where a.id=b.id;/*Hive中*/select * from table a join table b on a.id=b.id; 1.5、HiveQL1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859--Hive是一种数据库技术，可以定义数据库和表来分析结构化数据。--主题结构化数据分析是以表方式存储数据，并通过查询来分析。--创建数据库是用来创建create schema test;--------------------------------------------------------------Hive分区---------------------------------------------------------------------------------------Hive组织表到分区，它是将一个表到基于分区列，如日期，城市和部门的值相关方式。使用分区，很容易对数据进行部分查询--表或者分区是细分成桶，以提供额外的结构，可以使用更高效的查询的数据，桶的工作是基于表的一些列的散列函数值。--添加分区alter table table_name add PARTITION partition_spec-- 重命名分区alter table table_name PARTITION partition_spec RENAME TO PARTITION partition_new;--删除分区alter table table_name DROP PARTITION partition_spec,PARTITION partition_new-----------------------------------------------------------------Hive聚合函数-------------------------------------------------------------------------------Hive支持以下内置聚合函数，用法类似SQL聚合函数--1、count(*)返回检索行的总数:return type: BIGINTselect count(*) from table_name;--2、sum(col_name),sum(DISTINCT col_name):返回该组或者该组中的列的不同值的分组和所有元素的总和;return type:DOUBLEselect sum(comm_id) from table_name group by comm_id;--3、avg(col_name),avg(DISTINCT col_name):返回组或者组中列的不同值的元素的平均值，return type:DOUBLEselect avg(score) from table_name group by score;--4、min(col_name):返回该组中的列的最小值，return type :DOUBLE--5、max(col_name)：返回该组中的列的最大值,return type:DOUBLE-----------------------------------------------------------------------------Hive的视图和索引------------------------------------------------------------根据用户的需求可以创建视图，可以将任何结果集数据保存为一个视图，视图在Hive的用法和SQL视图用法相同，他是一个标准的--RDBMS概念，可以在视图上执行所有的DML操作。--1、创建一个视图，可以创建一个视图，在执行SELECT语句的时候create VIEW view_name AS SELECT * from table where cal_dat=\"201907\"and salary&gt;3000;--2、删除一个视图DROP VIEW view_name;--3、创建索引--索引也就是一个表上的一个特定列的指针，创建索引意味着创建一个表上的一个特定列的指针。CREATE INDEX index_name ON TABLE table_name(salary)AS 'org.apache.hadoop.hive.sql.index.compack.CompactIndexHandler';--4、删除索引DROP INDEX index_name on table_name;------------------------------------------------------------------------------Hive的Select Where---------------------------------------------------------1、select的单查询SELECT * FROM table_name WHERE salay&gt;2000;--2、select的order by查询SELECT id,score,name from table_name ORDER BY score;--3、select的group by查询SELECT dept,count(*) from table_name GROUP BY dept;--4、select的join查询,有点类似SQL的inner joinSELECT a.id,b.name,a,score FROM user_tab join score_tab bON a.id=b.id;--5、select的left outer join查询，以左表为核心，返回左表所有的行，右表没匹配的给null--6、select的right outer join查询，以右表为核心，返回右表所有的行，左表没匹配的列给null--7、select的full outer join查询，连接表包含两个表所有的记录，如果两侧缺少匹配的给null--查看表结构以及字段desc hdp_drq_dw_db.test;--查看表的分区字段以及值show PARTITIONS hdp_drq_dw_db.test;","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"git开发规范","date":"2017-11-03T07:23:33.000Z","path":"2017/11/03/git开发规范/","text":"1、配置SSH12345678910111213#配置用户名，邮箱git config --global user.name \"xxx\"git config --global user.email \"xxx@163.com\"#设置pull代码为rebasegit config --global --bool pull.rebase true#设置git log的格式alias gl='git log --graph --pretty=format:'\\''%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset'\\'' --abbrev-commit --date=relative'#生成秘钥ssh-keygen -t rsa #查看公钥cat ~/.ssh/id_rsa.pub#把key赋值到git的settings的SSH keys 2、rebase代码​ 除了master分支，其他分支拉最新master代码，要加上rebase，保证代码整洁性，都在一条线上。 123456789101112131415161718#切换到当前你自己的分支，获取最新的代码git branch consult#rebase远程master分支的最新代码，合并到你自己的分支git rebase origin/master#如果出现冲突，则需要解决冲突git diff -w#解决冲突文件vim 冲突文件git rebase origin/mastergit rebase --continuegit add .git log#然后fetch到当前分支git fetch#然后push到自己的分支git push origin consult#push不上去的时候，直接强push(慎用)git push -f origin consult 3、合并自己的分支代码到master​ 合并自己分支的代码到master的时候，需要git merge –no-ff xxx 1234567891011121314#切换分支到mastergit checkout master#合并自己分支代码到master分支git merge --no-ff consult#合并完之后，查看下日志，有没有自己分支的代码git log#查看分支状态git status#合并都增加准备提交git add .#本地提交git commit -m \"合并分支consult\"#然后远程push到master分支git push origin master","link":"","tags":[{"name":"Git","slug":"Git","permalink":"https://victorblog.github.io/tags/Git/"}]},{"title":"Docker的使用","date":"2017-11-01T02:06:00.000Z","path":"2017/11/01/docker使用/","text":"一、docker的使用1、docker添加镜像加速文件123456789cd /etc/docker#如果没有daemon.json文件，需要新建一个sudo touch daemon.jsonsudo vim daemon.json#daemon.json添加以下配置，然后保存&#123;\"registry-mirrors\":[\"https://registry.docker-cn.com\"]&#125;#重启服务sudo systemctl daemon-reloadsudo systemctl restart docker 2、查看docker的版本1234567docker version#使用docker查找镜像sudo docker search mysql#查看本地下载的镜像sudo docker images#查看本地正在运行的镜像sudo docker ps -a 3、使用docker拉取镜像12345sudo docker pull redis:5.0.6sudo docker pull nginxsudo docker pull mysql:8.0.18#删除拉取的镜像sudo docker image rm mysql:8.0.18 4、使用docker安装并运行nginx12345678910111213141516sudo docker search nginx#pull的时候，后面不追加冒号和版本号，默认是最新的latest版本sudo docker pull nginx#运行nginxsudo docker run --name victor-nginx-test -p 8081:80 -d nginx#victor-nginx-test是容器名称-d，设置容器在后台一直运行-p，端口进行映射，将本地的8081端口映射到容器内部的80端口#执行玩上面那条命令后，会生成一串字符串45faac1c8dcac4c356e4fd8f8dc0ee204385feca7e851cbe874805db7b811f89，这个表示容器的id，一般作为日志的文件名来查看启动信息等sudo docker logs 45faac1c8dcac4c356e4fd8f8dc0ee204385feca7e851cbe874805db7b811f89#停止nginxsudo docker ps#复制对应容器的id,然后停止镜像sudo docker stop 45faac1c8dca#删除镜像sudo docker rm 45faac1c8dca 5、使用docker部署项目12345678910111213141516171819#首先创建一个文件夹，里面包含3个子文件夹sudo mkdir -p ~/桌面/nginx/www ~/桌面/nginx/logs ~/桌面/nginx/conf#首先你要直到容器的id，sudo docker ps -a#复制docker容器里的nginx默认配置文件信息，到你的本地刚创建的conf文件夹目录下sudo docker cp76d9d74d071e:/etc/nginx/nginx.conf ~/桌面/nginx/conf#部署项目，到docker里的nginxsudo docker run -p 8082:80 --name victor-nginx-web -v ~/桌面/nginx/www:/usr/share/nginx/html -v ~/桌面/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/桌面/nginx/logs:/var/log/nginx -d nginx#命令说明-p 8082:80 是把你容器里的nginx的80端口映射到你本地主机的8082端口--name victor-nginx-web 是把容器重命名我们自己喜欢的名字-v ~/桌面/nginx/www:/usr/share/nginx/html 是把我们本地创建的项目 的www目录挂在到容器的/usr/share/nginx/html-v ~/桌面/nginx/conf/nginx.conf:/etc/nginx/nginx.conf 是吧我们创建的nginx.conf挂在到容器的/etc/nginx/nginx.conf-v ~/桌面/nginx/logs:/var/log/nginx 是把我们自己创建的logs挂在到容器的/var/log/nginx-d nginx :是设置容器一直后台运行#执行完命令后，会生成一串字符串，用这个字符串表示日志id，可以使用sudo docker logs 76d9d74d071e8b2d8499a56691587eef3e2f19d4fddeb5a3e26e5d9338856e10 6、使用docker安装mysql并运行123456789101112131415161718192021222324252627#拉去mysql的镜像sudo docker pull mysql:5.7#本地创建一个文件夹，用来映射容器的mysqlsudo mkdir -p ~/桌面/mydata/mysql/log ~/桌面/mydata/mysql/conf ~/桌面/mydata/mysql/data#给创建好的文件夹开权限sudo chmod 777 -R ~/桌面/mydata/#使用docker命令启动mysqlsudo docker run -p 3305:3306 --name mysql -v ~/桌面/mydata/mysql/log:/var/log/mysql -v ~/桌面/mydata/mysql/data:/var/lib/mysql -v ~/桌面/mydata/mysql/conf:/etc/mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7#查看镜像，如果存在，或者端口被占用，则换为3305映射容器的端口sudo docker ps -a#先删除被占用名字的容器镜像id，然后重新执行启动命令sudo docker rm3bf96e1f8a95#进入运行mysql的docker容器sudo docker exec -it mysql /bin/bash########以下是mysql的命令操作#使用mysql 命令打开客户端mysql -uroot -proot --default-character-set=utf8#查看数据库show databases;create database test character set utf8;use test;create table ();#退出mysql命令操作\\q#退出mysql运行的docker容器exit 7、使用docker安装并运行redis12345678910111213141516#docker拉取redissudo docker pull redis:3.2#本地创建一个redis文件夹存放数据sudo mkdir -p ~/桌面/mydata/redis/data#更改文件件执行权限sudo chmod 777 -R ~/桌面/mydata/redis#使用docker命令启动redissudo docker run -p 6379:6379 --name redis -v ~/桌面/mydata/redis/data:/data -d redis:3.2 redis-server --appendonly yes#进入redis容器使用redis-cli命令进行连接sudo docker exec -it redis redis-cli#####以下为redis的命令pingset a 100get a#退出redisexit 8、使用docker安装并运行rabbitmq123456789101112131415161718192021#拉取docker镜像sudo docker pull rabbitmq:3.7.15#使用docker命令启动rabbitmqsudo docker run --name rabbitmq --publish 5671:5671 --publish 5672:5672 --publish 4369:4369 --publish 25672:25672 --publish 15672:15672 --publish 15671:15671 -d rabbitmq:3.7.15#Ubuntu的防火墙暴露端口15672sudo ufw allow 15672#开启防火墙sudo ufw enable#重新加载sudo ufw reload#查看防火墙状态sudo ufw status#进入到rabbitmq的连接sudo docker exec -it rabbitmq /bin/bash#然后输入如下命令，开启rabbitmq的管理后台rabbitmq-plugins enable rabbitmq_management#浏览器输入rabbitmq的管理后台页面http://localhost:15672用户名密码：guest guest#退出exit","link":"","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://victorblog.github.io/tags/DevOps/"}]},{"title":"HBase","date":"2017-10-03T03:24:39.000Z","path":"2017/10/03/HBase/","text":"一、引言1、分布式结构化存储系统​ 大数据中，除了直接以文件形式保存的数据外(HDFS)，还有大量结构化和半结构化的数据，这种数据一般需要支持更新操作，随机插入，删除。使得HDFS无法满足要求。为了方便用户存取海量结构化和半结构化数据，HBase诞生了。是一个分布式列存储系统，有很好的扩展性、容错性、易用的API。HBase是构建在HDFS之上的，支持随机插入和删除的列簇式存储系统，可以理解成一个具有持久化能力的分布式有序映射表。 ​ 虽然HBase随机读写性能比较高，但是数据scan扫描速度较慢，难以适用于OLAP场景。后来Cloudera设计出了Kudu，很好的兼顾吞吐率和延迟。 二、背景​ 由于关系型数据库用来解决数据存储和维护有关的问题。大数据出现后，大多公司实现处理大数据，并开始选择Hadoop的解决方案。Hadoop使用分布式文件系统HDFS，用来存储大数据，使用MapReduce来处理数据。 ​ Hadoop只能执行批量处理，而且是按顺序访问数据。也就是表明必须搜索整个数据集。如果想要处理结果在另一个庞大的数据，也是按照顺序处理一个巨大的数据集。为此，需要访问数据中的任何点的单元，也就是随机访问，出现新的解决方案。 ​ Hadoop随机存取数据库的应用，HBase，Cassandra，CouchDB，Dynamo，MongoDB都是一些存储大量数据和随机访问访问的非关系型数据库。 海量数据存储成为瓶颈，单机无法负载大量数据 单机IO读写请求成为海量数据存储，高并发，大规模的瓶颈 随着数据规模越来越大，大量业务场景开始考虑数据存储横向水平扩展，让存储服务可以增加/删除 ，而大部分关系型数据库更专注在一台机器。 三、HBase简介​ HBase是BigTable的开源版本，使用Java编写的。是Apache Hadoop的数据库，建立在HDFS上，用来提供，高可靠，高性能，列存储，可伸缩，多版本的NoSQL的分布式数据库。可以实现对大型数据的实时、随机的读写访问。 HBase依赖HDFS做底层的数据存储，BigTable依赖Google GFS做数据存储 HBase依赖MapReduce进行数据计算，BigTable依赖Google MapReduce做数据计算 HBase依赖Zookeeper做服务协调，BigTable依赖Google Chubby做服务协调 1、非关系型和关系型数据库 NoSQL：HBase，Redis，MongoDB RDBMS：MySQL，Oracle，SQL Server 2、HBase重点 HBase介于NoSQL和RDBMS之间，只能通过主键rowkey和主键的range来检索数据 HBase查询数据功能很简单，不支持join等关键查询复杂操作，可以通过Hive支持来实现多表join HBase不支持复杂的事务，只支持行级事务 HBase中纯支持的数据类型，byte[]，底层所有数据的存储都是字节数组 HBase主要来存储结构化和半结构化的松散数据 结构化数据：数据结构字段清晰，比如数据库的表结构 半结构化：具有一定结构，语义不太确定，比如html网页的数据。 3、HBase的表 数据量大：一张表可以有上十亿行，上百万列的数据 面向列：列（族）的存储和权限控制，列簇独立检索 稀疏数据：对于null的空列，并不会占用存储空间，在设计表的时候可以非常稀疏 无模式：每行都有一个可以排序的主键rowkey和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列 Rowkey ColumnFamily :CF1 ColumnFamily :CF2 TimeStamp Column:Name Column:Alias Column:Age Column:Sex rk001 zhangsan shanghai 22 M T1 rk002 lisi beijing 25 F T2 3.1、RowKey的含义3.1.1、rowkey​ HBase中的rowkey和MySQL中的主键是完全一样的，来唯一的区分某一行的数据。 3.1.2、HBase支持3种查询方式： 基于Rowkey单行查询 基于Rowkey的范围扫描 全表扫描 3.1.3、rowkey分析 rowkey对HBase的性能影响很大，rowkey的设计也很重要，设计的时候需要考虑基于rowkey单行查询，还要考虑rowkey的范围查询。 rowkey行主键，可以是任意字符串，最大长度为64kb，实际中一般是10~100b，最好是16。HBase内部，rowkey保存为字节数组。HBase会对表中的数据按照rowkey进行字典序排序 3.2、Column的含义 列，可以理解为MySQL的列 3.3、ColumnFamily的含义 列族，HBase引入的定义 HBase通过列族划分数据的存储，列族下面可以包含若干个列，实现灵活的数据存取。就如同一个家族一样，列族是由一个个列组成。 HBase的表在创建的时候必须指定列族，就像你在MySQL创建表的时候必须指定具体的列一样。 HBase的列族并不是越多越好，一般列族最好是小于等于3个，一般常用1个列族 3.4、TimeStamp的含义 TimeStamp是实现HBase多版本的关键。HBase中使用不同的TimeStamp来标识相同rowkey行对应的不同版本的数据。类似Hive表中的时间分区字段。 HBase中通过rowkey和column确定一个存储单元为cell，每个cell保存同一份数据的多个版本。版本通过时间戳来索引。时间戳可以在HBase写入数据的时候自动赋值。为了避免数据版本冲突，必须自己生成具有唯一性的时间戳。一般每个cell中，不同版本的数据按照时间倒序，最新的数据在最前面。 为了避免数据存太多版本造成存储和索引的负担，HBase还提供了2种版本回收方式 保存数据的最后n个版本 保存最近一段时间内的版本(可以设置数据的生命周期TTL) 3.5、单元格Cell 由rowkey，column，version唯一确定的单元。Cell中的数据是没有类型的，全部是字节码byte[]形式存储。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Hadoop命令","date":"2017-09-13T02:06:00.000Z","path":"2017/09/13/Hadoop命令/","text":"1、文件操作12345678910#在HDFS上创建目录hadoop fs -mkdir &lt;hdfs_path&gt;#查看文件hadoop fs -ls &lt;hdfs_path&gt;#显示文件的大小hadoop fs -du &lt;hdfs_path&gt;#只删除文件或者空目录hadoop fs -rm &lt;hdsf_path&gt;#递归删除目录，命令rm的递归版本，会递归删除目录下的各级目录以及文件hadoop fs -rmr &lt;hdfs_path&gt; 2、查看文件内容1234#显示文件内容hadoop fs -cat &lt;hdfs_path&gt;#将文件尾部1KB字节的内容输出到stdouthadoop fs -tail [-f] URI 3、文件复制123456789101112#将文件源路径移动到目标路径hadoop fs -mv &lt;src1&gt; &lt;src2&gt; &lt;dest&gt;#将文件从源路径复制到目标路径hadoop fs -cp &lt;src1&gt; &lt;src2&gt; &lt;dest&gt; #将本地文件放置在HDFS上hadoop fs -copyFromLocal &lt;local_path&gt; &lt;remote_path&gt; #将HDFS上的文件复制到本地文件系统中hadoop fs -copyToLocal &lt;remote_path&gt; &lt;local_path&gt; #复制文件到本地文件系统hadoop fs -get &lt;src&gt; &lt;localdst&gt; #从本地文件系统中复制单个或多个源路径到目标文件系统中hadoop fs -put &lt;localsrc&gt;... &lt;dst&gt; 4、高权限123scp victor@192.168.10.1:/home/victor/py/test.py /usr/opt/script/#去前1000行记录到新文件head -1000 basic.dat &gt; test1.dat","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Java并发编程","date":"2017-09-13T02:06:00.000Z","path":"2017/09/13/一、Java并发编程/","text":"一、Java并发编程 多线程程序包含两个或者多个可以同事运行的部分，每个部分可以同事处理不同的任务，从而能更好的利用可用资源，如果CPU是多核，多线程可以写入多个task，在同一个程序同时进行操作处理。 多任务是多个进程共享，比如CPU处理公共资源 多线程：将多任务的概念扩展到可以将单个应用程序中的特定操作细分为单个线程的应用程序 ，每个线程可以并行运行。 1、线程的生命周期线程在生命周期中经历多个阶段：线程诞生，启动，运行，然后死亡。 新线程New：新线程在新的状态下开始其生命周期，直到程序启动线程为止，保持在这种状态，也叫出生线程。 可运行Runnable：新诞生的线程启动以后，start，该线层可以运行，状态的线程被认为正在执行其任务。 等待Waiting：有时线程回转换到等待状态sleep,wait()，而线程等待另一个线程执行任务。只有当另一个线程发信号通知notify()/notifyAll()等待线程才能继续执行时，线程才转回到可运行状态。 定时等待Timed Waiting：可运行的线程可以在指定的时间间隔内进入定时等待状态，当该时间间隔sleep(100)到期或者发生等待的事件的时候，此状态的线程将转换到可运行状态。 终止dead：可执行线程在完成任务或者以其他方式终止的时候进入到终止状态。 2、线程优先级每个java线程都有一个优先级，可以帮助操作系统安排线程的顺序。Java线程优先级在MIN_PRIORITY(1),和MAX_PRIORITY(10)之间的范围内，默认情况下的，每个线程优先级都是NORM_PRIORITY(5) 具有较高的优先级的线程相对于一个程序来说更重要，应该在低优先级线程之前分配处理器时间。然而，线程优先级并不能呢保证线程执行的顺序。 3、通过实现Runnable接口创建线程 需要实现Runnable接口提供的run()方法。run()方法为线程提供一个入口，可以把完整的业务逻辑放到run()方法体。此方法是没有返回值的 123public void run()&#123; ...service....&#125; 然后在通过Thread构造函数，去创建一个对象 对象调用start()方法来启动线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package multi_thread;/** * @Description:通过实现Runnable接口的方式创建线程 * 1、首先是需要实现Runnable的接口的run方法，run()作为线程提供一个入口，完成的业务逻辑写在run()里 * 2、通过Thread构造函数，去创建一个实例对象 * 3、对象调用start()方法来启动线程 * @Author: VictorDan * @Date: 19-8-12 下午3:58 * @Version: 1.0 */public class TestRunnable implements Runnable &#123; private Thread thread; private String threadName; public TestRunnable(String threadName) &#123; this.threadName = threadName; System.out.println(\"创建线程：\"+threadName); &#125; /** * 实现Runnable接口，需要重写run方法 * run方法里一般写的业务逻辑代码 */ @Override public void run() &#123; System.out.println(\"创建线程：\"+threadName); try &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"线程：\"+threadName+\",\"+i); Thread.sleep(50); &#125; &#125;catch(InterruptedException e)&#123; System.out.println(\"线程：\"+threadName+\"被打断\"); &#125; System.out.println(\"线程:\"+threadName+\"执行完退出\"); &#125; public void start()&#123; System.out.println(\"开启线程：\"+threadName); if(thread==null)&#123; thread=new Thread(this,threadName); thread.start(); &#125; &#125; public static void main(String[] args) &#123; TestRunnable thread1 = new TestRunnable(\"Thread-1\"); thread1.start(); TestRunnable thread2 = new TestRunnable(\"Thread-2\"); thread2.start(); &#125;&#125; 4、通过继承Thread类创建一个线程5、线程池的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107 public static void main(String[] args) &#123; /** * 使用Executors可以创建线程池，但是你方便的同时，隐藏了复杂性，而埋下隐患，比如OOM，线程耗尽等。 * 所以阿里开发规范，创建线程池要手动创建，而不允许Executors直接调用的方式。 */ //1、创建一个不限制线程数上限的线程池，任何提交的任务都立即执行，最大上限是Integer.MAX_VALUE,会使用 Executors.newCachedThreadPool(); //2、创建一个固定个数大小的线程池 Executors.newFixedThreadPool(10); //3、创建一个只有1个线程的线程池 Executors.newSingleThreadExecutor(); //4、创建一个定时的线程池，工作队列是延迟队列。 Executors.newScheduledThreadPool(20); //总结：小程序使用这些快捷方法创建线程池没问题，但是对于服务端长期运行的程序，需要如下通过利用ThreadPoolExecutor的构造函数创建线程池。 /** * Executors.newXXXThreadPool(),这种方式创建线程池，会使用无界的任务队列，为了避免OOM，应该手动使用ThreadPoolExecutor的构造方法指定队列的最大长度。 * 还要明确拒绝任务时的策略，行为。也就是任务对了沾满的时候，这里在submit()新的任务， * public interface RejectedExecutionHandler&#123; * void rejectedExecution(Runnable r,ThreadPoolExecutor executor); * &#125; * 线程池提供了常见的拒绝策略。 * 1、new ThreadPoolExecutor.DiscardOldestPolicy():丢弃执行队列中最老的任务，尝试为当前提交的任务腾出位置 * 2、new ThreadPoolExecutor.DiscardPolicy()：什么都不做，直接忽略 * 3、new ThreadPoolExecutor.AbortPolicy();抛出RejectedExecutionException * 4、new ThreadPoolExecutor.CallerRunsPolicy();直接由提交任务者执行这个任务 * 线程池默认的拒绝行为是：AbortPolicy，也就是抛出RejectedExecutionException， * 如果不关心任务被拒绝，则可以使用DiscardPolicy，这样多余的任务可以悄悄的被忽略。 */ //Executors的方法也是使用ThreadPoolExecutor的构造方法创建的线程池。 new ThreadPoolExecutor(2, 20, 0L, SECONDS, new ArrayBlockingQueue&lt;&gt;(512), new ThreadPoolExecutor.DiscardOldestPolicy()); ExecutorService executorService = Executors.newCachedThreadPool(); new ThreadPoolExecutor.DiscardPolicy(); new ThreadPoolExecutor.AbortPolicy(); new ThreadPoolExecutor.CallerRunsPolicy(); new ThreadPoolExecutor.DiscardOldestPolicy(); //executorService=Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 20; i++) &#123; executorService.execute(new WorkTask()); &#125; Future&lt;Object&gt; future = executorService.submit(() -&gt; &#123; //这个异常会在调用Future.get()的时候传递给调用者 throw new RuntimeException(\"exception in call\"); &#125;); try &#123; Object result = future.get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; //exception in Callable.call(); e.printStackTrace(); &#125; finally &#123; executorService.shutdown(); &#125; /** * 线程池的常用场景 * 1、如何正确的构造线程池 * 2、构造一个工作阻塞队列 * 3、设置拒绝策略 * 设置 */ int poolSize = Runtime.getRuntime().availableProcessors() * 2; /** * BlockingQueue只是一个接口，有好多实现类 * 1、ArrayBlockingQueue:底层基于数组实现的阻塞队列 * 2、LinkedBlockingQueue：底层基于链表实现的阻塞队列 * 3、LinkedBlockingDeque：底层基于双端链表实现的阻塞队列 * 4、DelayQueue：延迟队列 * 5、PriorityBlockingQueue:基于优先队列，底层是堆实现的。 * 6、SynchronousQueue:同步队列,队列内只存储一个元素。公平模式。 */ BlockingQueue&lt;Object&gt; queue = new ArrayBlockingQueue&lt;&gt;(512); RejectedExecutionHandler policy = new ThreadPoolExecutor.DiscardPolicy();// executorService = new ThreadPoolExecutor(poolSize, poolSize,// 0, TimeUnit.SECONDS,// queue,// policy); System.out.println(poolSize); &#125; class ThreadPoolExecutor1 &#123; /** * 线程池长期维持的线程数，即使没有任务，也不会回收 */ int corePoolSize; /** * 线程数的上限，比如一般是Integer.MAX_VALUE */ int maximumPoolSize; /** * 超过corePoolSize的线程，会有一个活跃时间，超过这个时间，多余线程会被回收 */ long keepAliveTime; TimeUnit unit; /** * 任务的排队队列，也就是排期队列。 */ BlockingQueue&lt;Runnable&gt; workQueue; /** * 创建线程的渠道 */ ThreadFactory threadFactory; /** * 拒绝策略，也就是任务队列也塞满了，新来的任务需要被拒绝。 */ RejectedExecutionHandler handler; &#125; 6、ThreadLocal​ 线程本地存储。ThreadLocal的作用是提供线程内的局部变量，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或者一个公共变量的传递复杂度。 6.1、ThreadLocalMap 每个线程中都有一个自己的ThreadLocalMap类对象，可以将线程自己的对象保持到其中，各管各的，线程可以正确的访问到自己的对象。 将一个公用的ThreadLocal静态实例作为key，将不同对象的引用保存到不同线程的ThreadLocalMap中，然后在线程执行的各处通过这个静态ThreadLocal实例的get()方法取的自己线程保存的对象，避免了将这个对象作为参数传递的麻烦。 ThreadLocalMap就是线程里的一个属性，在Thread类定义 1ThreadLocal.ThreadLocalMap threadLocals=null; 最常见的ThreadLocal使用场景，解决数据库连接，Session管理 12345678910111213private static final ThreadLocal local=new ThreadLocal(); public static Session getSession()&#123; Session s = (Session) local.get(); try&#123; if(s==null)&#123; s= getSession()； local.set(s); &#125; &#125;catch (HibernateException e)&#123; &#125; return s; &#125; 二、Dubbo的知识点dubbo是一款高性能，轻量级的开源java RPC框架，提供3大核心功能： 面向接口的远程方法调用 智能容错和负载均衡 服务自动注册和发现 dubbo是一个分布式服务框架，用来提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理。 1、什么是RPC？RPC：Remote Procedure Call，一个远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如说两个不同的服务A，B部署在两台不同的机器上，那么服务A如果想要调用服务B中的某个方法怎么解决？使用HTTP请求当然可以，但是可能会比较慢，而且一些优化做的并不好，RPC的出现就是为了解决这个问题。 RPC的原理： 服务消费方client调用以本地调用方式调用服务 client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体。 client stub找到服务地址，并将消息发送到服务端 server stub收到消息后进行解码 server stub根据解码结果调用本地的服务 本地服务执行并结果返回给server stub server stub将返回结果打包成消息并发送到消费方 client stub接收到消息，并进行解码 服务消费方得到最终结果。 2、为什么要用dubbo？dubbo和SOA分布式架构的流行有着关系。SOA面向服务的架构(Service Oriented Architecture)，就是把工程按照业务逻辑拆分成服务层，表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务就行。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者Provieder和服务使用者Consumer 3、开发分布式及程序，可以直接基于HTTP接口进行通信，但是为什么要使用dubbo？dubbo的四个特点： 负载均衡：同一个服务部署在不同的机器时该调用哪一台机器上的服务 服务调用链路生成：随着系统的发展，服务越来越多，服务间依赖关系变得错综复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系，dubbo可以为我们解决服务之间是如何互相调用的。 服务访问压力以及时长统计，资源调度和治理：基于访问压力实时管理集群容量，提供集群利用率。 服务降级：某个服务挂掉之后调用备用服务。 dubbo除了用在分布式系统中，也可以应用在微服务系统中，但是由于springboot cloud在微服务中更加广泛，所以一般提dubbo大部分在分布式系统的情况。 4、什么是分布式？分布式或者说SOA分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单拆分成订单系统，商品系统，登录系统等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。 5、为什么要分布式？从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。 另外，将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。把整个系统拆分成不同的服务、系统，然后每个服务，系统单独部署在一台服务器上，是不是很大成都提升了系统性能。 6、dubbo的架构 Provider：暴露服务的服务提供方 Consumer：调用远程服务的服务消费方 Registry：服务注册与发现的注册中心 Monitor：统计服务的调用次数和调用时间的监控中心 Container：服务运行容器 6.1、调用关系说明： Container服务容器负责启动，加载，运行服务提供者Provider。 服务提供者Provider在启动的是偶，向注册中心Registry注册自己提供的服务 服务消费者Consumer在启动的时候，向注册中心Registry订阅自己所需要的服务 注册中心Registry返回服务提供者Provider地址列表给消费者Consumer，如果有变更，注册中心Registry将基于长连接keep-alive推送变更数据给消费者Consumer 服务消费者Consumer，从服务提供者Provider地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 dubbo提供了4中负载均衡算法： 权重随机算法：RandomLoadBalance 最少活跃调用数算法：LeastActiveLoadBalance 一致性哈希算法：ConsistentHashLoadBalance 加权轮询算法：RoundRobinLoadBalance 服务消费者Consumer和服务提供这Provider，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心Monitor。 6.2、总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者Provider和服务消费者Consumer只在启动的时候与注册中心Registry交互，注册中心不转发请求，压力小。 监控中心Monitor负责统计各个服务调动次数，调用时间等，统计现在内存汇总后每分钟一次发送到监控中心服务器，并以报表的形式显示。 注册中心Registry，服务提供这Provider，服务消费者Consumer三者之间均为长连接Keep-alive，监控中心Monitor除外 注册中心Registry通过长连接感知服务提供者Provider的存在，服务提供者挂了，注册中心立即将推送事件通知消费者Consumer。 注册中心和监控中心全部挂了，不影响已经运行的服务提供者Provider和Consumer，消费者在本地缓存了提供者列表。 注册中心和监控中心都是可选的，服务消费者Consumer可以直接连服务提供者Provider 服务提供者Provider无状态，任意一台挂掉，不影响使用。 服务提供者Provider全部挂了，服务消费者Consumer应用将无法使用，并无限次重连等待服务提供者恢复。 6.3、Zookeeper挂了与dubbo直连的情况在实际生产中，如果zookeeper注册中心挂了，一段时间内服务消费者Consumer还是能够调用服务提供者Provider服务的，实际上Consumer使用了本地缓存进行通讯，Consumer本地缓存了提供者列表。 dubbo的健壮性: 监控中心挂了不影响使用","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"双指针","date":"2017-09-01T01:23:00.000Z","path":"2017/09/01/双指针/","text":"算法中的双指针使用双指针主要用来遍历数组，两个指针指向不同的元素，从而协同完成任务。 1、有序数组的Tow SumLeetCode的TowSum 题目描述：有序数组，找出两个数组，让和为target 算法思想：使用双指针，一个指针指向值较小的元素，一个指针指向值较大的元素，指向值较小元素的指针从头向后开始遍历，指向较大的指针从后向前开始遍历。 如果两个指针指向元素的和sum==target，返回结果位置 如果sum&gt;target，则移动较大的指针 如果sum&lt;target，则移动较小的指针 1234567891011121314public int[] twoSum(int[] numbers,int target)&#123; int i=0,j=numbers.length-1; while(i&lt;j)&#123; int sum=numbers[i]+numbers[j]; if(sum==target)&#123; return new int[]&#123;i+1,j+1&#125;; &#125;else if(sum&lt;target)&#123; i++; &#125;else&#123; j--; &#125; &#125; return null;&#125; 2、两数的平方和LeetCode 题目描述：判断一个数是否为两个数的平方和 算法思想：还是使用两个指针，一个指针为0，一个指针为和的sqrt 如果两个指针指向元素的和powSum==target，返回结果位置 如果powSum&gt;target，则移动较大的指针 如果powSum&lt;target，则移动较小的指针 1234567891011121314public boolean judgeSquerSum(int target)&#123; int i=0,j=(int)Math.sqrt(target); while(i&lt;=j)&#123;//此处有等号，不然target=2的时候，没有等号就返回false int sum=i*i+j*j; if(sum==target)&#123; return true; &#125;else if(sum&lt;target)&#123; i++; &#125;else&#123; j--; &#125; &#125; return false;&#125; 判断链表是否有环12345678910111213141516public class Solution &#123; public boolean hasCycle(ListNode head) &#123; if(head==null)&#123; return false; &#125; ListNode slow=head,fast=head;//声明两个临时指针，都指向head while(fast!=null&amp;&amp;fast.next!=null)&#123; slow=slow.next; fast=fast.next.next; if(slow==fast)&#123; return true; &#125; &#125; return false; &#125;&#125;","link":"","tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://victorblog.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}]},{"title":"Redis的使用","date":"2017-08-21T02:06:00.000Z","path":"2017/08/21/一、Redis简介/","text":"一、简介Redis是速度非常快的非关系型(NoSQL)内存键值数据库。可以存储键和5种不同类型的值之间的映射。 key的类型只能为字符串，value支持6中类型：String,Hash,List,Set,zset,HyperLogLog Redis支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 二、数据类型 数据类型 可以存储的值 操作 String 字符串，整数和浮点数 对整个字符串或者字符串的其中一部分执行操作，对整数和浮点数执行自增或者自减操作。 List 列表 从两端压入或者弹出元素，对单个或者多个元素进行修剪，只保留一个范围内的元素。 Set 无序集合 添加，获取，移除单个元素，检查一个元素是否存在于集合中，计算交集，并集，差集，从集合里面随机获取元素，去重。 Hash 包含键值对的无序散列表 添加，获取，移除单个键值对，获取素有键值对，检查某个键是否存在。 Zset 有序集合 添加，获取，删除元素，根据分值范围或者成员来获取元素计算一个键的排名 有点类似公司用的Wtable，每次用col key去获取数据。获取的数据里，有数据的，有字符串的，有对象的。 三、数据结构1、字典dictht是一个散列表结构，使用拉链法解决哈希冲突。 12345678910111213141516typedef struct dictht&#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125;dictht;typedef struct dictEntry&#123; void *key; union&#123; void *val; uint64_t u64; double d; &#125;v; struct dictEntry *next;&#125;dictEntry; Redis的字典dict中包含两个哈希表dictht，只是为了方便进行rehash操作。在扩容的时候，将其中一个dictht上的键值对rehash到另一个dictht上面，完成之后释放空间并交换两个dictht的角色。 rehash操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的rehash操作给服务器带来过大的负担。 渐进式rehash通过记录dict的rehashidx完成，他从0开始，然后每执行一次rehash都会递增。 2、跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。 在查找时候，从上层指针开始查找，找到对应的区间之后再到下一层去查找。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快，因为不需要进行旋转等操作来维护平衡性。 更容易实现 支持无锁操作。 四、Redis使用场景1、计数器可以对String进行自增自减运算，从而实现计数器功能。 Redis这种内存型数据库的读写性能非常高，很适合存储频繁读写的及数量。 2、缓存将热点数据存放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 3、查找表例如DNS的记录就很适合使用Redis进行存储。 查找表和缓存类似，也是利用Redis快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠地数据来源。 4、消息队列List是一个双向链表，可以通过lpush和rpop写入和读取消息 不过最好使用kafka，RabbitMQ等消息中间件。 5、Session缓存可以使用Redis来同一个存储多台应用服务器的Session信息。 当应用服务器不在存储用户的会话信息，也就不再具有状态。一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 6、分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。 可以使用Redis子代的SETNX命令实现分布式锁，除此之外，还可以使用官方提供的RedLock分布式锁实现。 7、其他Set可以实现交集，并集等操作，从而实现共同好友等功能 。 Zset可以shixi9an有序性操作，从而实现排行榜等功能。 五、Redis与Memcached两个都是非关系型内存键值数据库。主要区别如下： 1、数据类型Memcached只支持string类型，而Redis支持5种不同的数据类型，可更灵活的解决问题。 2、数据持久化Redis支持3种持久化策略：RDB快照和AOF日志，RDB和AOF混合型，而Memcached不支持持久化。 3、分布式Memcached不支持分布式，只能通过咋客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要现在客户端计算一次数据所在的节点。 Redis Cluster实现了分布式的支持。 4、内存管理机制 在Redis中，并不是所有数据都是一直存在内存里，可以将一些很久没用的value交换到磁盘，而Memcached的数据则会一直在内存中。 Memcached将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高。比如块代销为128b，只存储100b的数据，那么剩下的28b就浪费掉了。 六、Redis的key过期时间Redis可以为每个key设置过期时间，当key过期时，会自动删除该key。 对于散列表这种容器，只能为整个key设置过期时间(整个散列表)，而不能为key里面的单个元素设置过期时间。 七、数据淘汰策略Redis可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Redis具有6种淘汰策略： 策略 描述 volatile-LRU 当内存使用量超出时，此种策略，从已经设置过期时间的数据集中选出最近最少使用的数据淘汰 volatile-TTL 当内存使用量超出时，此种策略，从已经设置过期时间的数据集中选出将要过期的数据淘汰 volatile-Random 当内存使用量超出时，此种策略，从已经设置过期时间的数据中任意选择数据淘汰 allkeys-LRU 当内存使用量超出时，此种策略，从所有数据中挑选最近最少使用的数据淘汰。 allkeys-Random 当内存使用量超出时，此种策略，从所有数据中选任意数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，处于对性能和内存消耗的考虑，Redis的淘汰算法实际实现上并非针对素有key，而是抽样一小部分并且从中选出被淘汰的key。 使用Redis缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用allkeys-LRU淘汰策略，将最近最少使用的数据淘汰。 Redis4.0引入了volatile-LFU,allkeys-LFU淘汰策略，LFU策略通过统计访问频率，将访问频率最少的键值对淘汰。 八、持久化Redis是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 1、RDB持久化 将某个时间点的所有数据都存到硬盘上 可以将快照复制到其他服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 2、AOF持久化将写命令添加到AOF文件(Append Only File)的末尾。 使用AOF持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统决定何时同步 always选项会严重降低服务器的性能。 everysec选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且Redis每秒执行一次同步对服务器性能几乎没有任何影响。 no选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF文件也会越来越大。Redis提供了一种将AOF重写特性，能够去除AOF文件中的冗余写命令。 九、事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其他客户端的命令请求。也就是事务的ACID。 事务中的多个命令被一次性发给服务器，而不是一条一条的发送，这种方式叫流水线，它可以减少客户端和服务器之间的网络通信次数从而提升性能。 Redis最简单的事务实现方式是使用MULTI和EXEC命令来将事务操作包围起来。 十、事件Redis服务器是一个事件驱动程序。 1、文件事件","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Java编码规范","date":"2017-08-04T02:06:00.000Z","path":"2017/08/04/一、Java编码规范/","text":"一、Java编码规范1、命名风格 代码中的命名不能以_，$开始和结束 代码中的命名严禁使用拼音与英文混合，不允许使用中文的方式 常量命名全部大写，单词之间下划线隔开。 POJO类中布尔类型，都不要加is，否则框架解析出现序列化错误。 对于Service和DAO类，基于SOA理念，暴露出服务一定是接口。 内部的实现类用Impl。比如CacheServiceImpl是CacheService接口。 枚举类名要加上Enum后缀，枚举成员名称要全部大写，单词之间有下划线隔开。 123public enum DealStatusEnum&#123; SUCCESS,UNKOWN_REASON&#125; 如果用到了设计模式，建议在类名中体现出具体设计模式 1public class LoginProxy 2、代码风格 左右小括号和字符之间不能有空格 单行字符数限制不超过120个，超出需要换行。 123sb.append(\"zi\").append(\"xin\") .append(\"victor\") .append(\"victor\") 方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之前或者不同的语义之间插入一个空行。 相同的业务逻辑和语义之间不需要插入一个空行 if/for/while/switch/do等括号之间必须加空格 方法参数在定义和传入时，多个参数逗号后边必须加空格 1method(\"a\", \"b\", \"c\"); 3、OOP规范 所有的重写方法，必须加上@Override注解；注：是为了让编译器帮你发现错误 相同参数类型，相同业务含义，才可以使用java的可变参数，避免使用Object可变参数。 Object的equals方法容易抛出空指针异常，应使用常量或者确定有值得对象来调用equals 1\"test\".equals(object); 所有的POJO类属性必须使用包装数据类型；RPC方法的返回值和参数必须使用包装数据类型 注意：为了避免在POJO转换过程中失败。 定义DO/DTO/VO等POJO类时，不要设定任何属性默认值。 注意：因为POJO原则转换过程中避免失败。 类内方法定义顺序依次是：public方法/protected方法&gt;&gt;private方法&gt;&gt;getter/setter方法 注意：把最关注的方法放在前面 getter/setter方法中，不要增加业务逻辑，增加排查问题的难度 慎用Object的clone方法来拷贝对象，深度复制需要自己实现。 循环体内，字符串的连接方式，使用StringBuilder的append方法进行扩展 拒绝巨型方法，把方法拆解到尽可能符合单一责任原则的粒度，方便维护及复用。 4、集合处理 只要重写equals，就必须重写hashCode()，Set，Map对象作为键的情况小重写hashCode,equals ArrayList的subList结果不可强转成ArrayList，否则会抛出ClassCastException强制类型转换异常。 使用工具类Arrays.asList()把数组转换为集合时，不能使用其修改集合相关的方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Spring Bean生命周期","date":"2017-07-25T07:28:26.000Z","path":"2017/07/25/Spring-Bean生命周期/","text":"一、Spring Bean生命周期1、实例化 实例化一个Bean，也就是我们常说的new 2、IOC依赖注入 按照ApplicationContext上下文对实例化的Bean进行配置，也就是IOC注入 3、setBeanName实现 如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String id)方法，传递的就是Spring配置文件中的Bean的id值 4、BeanFactoryAware实现 如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory(BeanFactory)传递的是Spring工厂自身。可以用这个方式来获取其他Bean只需要在Spring配置文件中配置一个普通的Bean就可以。 5、ApplicationContextAware实现 如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文。比步骤4更好点，因为ApplicationContext是BeanFactory的子接口，有更多的实现方法。 6、postProcessBeforeInitialization接口实现-初始化预处理 如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessBeforeInitialization(Object obj,String s)方法，BeanPostProcessor经常被用来是Bean内容的更改，并且由于这个是在Bean初始化结束时调用的方法，可以被应用在内存或者缓存技术 7、init-method 如果Bean在Spring配置文件中配置了init-method属性会自动调用其配置的初始化方法。对应的注解是@PostConstruct 8、postProcessAfterInitialization 如果这个Bean关联了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj,String s)方法。以上工作完成以后，就可以应用这个Bean了，那这个Bean是一个Singleton单例的，所以一般情况下我们调用同一个id的Bean会是在内容地址相同的实例，当然可以在Spring配置文件中Bean的作用范围为ProtoType，GlobalSession等非Singleton。 9、Destroy过期自动清理阶段 当Bean不在需要的时候，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用destroy()方法。 10、destroy-method自配置清理 如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用它配置的销毁方法。对应的注解是@PreDestroy 11、总结 bean的配置文件中，bean标签有2个重要属性 1&lt;bean id=\"person\" class=\"com.bean.Person\" init-method=\"init\" destroy-method=\"destroy\"/&gt; init-method 对应的注解为@PostConstruct destroy-method 对应的注解为@PreDestroy 二、Spring依赖注入4种方式1、构造方法注入123public PersonServiceImpl(String name)&#123; this.name=name;&#125; 123&lt;bean id=\"personServiceImpl\" class=\"com.service.impl.PersonServiceImpl\"&gt; &lt;constructor-arg value=\"name\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 2、setter方法注入123456789public class Person&#123; private String name; public String getName()&#123; return name; &#125; public void setName(String name)&#123; this.name=name; &#125;&#125; 123&lt;bean id=\"person\" class=\"com.bean.Person\"&gt; &lt;property name=\"name\" value=\"tom\"&gt;&lt;/property&gt;&lt;/bean&gt; 3、静态工厂注入静态工厂，就是通过静态工厂的方法来获取自己需要的对象，为了让Spring管理所有对象，我们不能直接通过”类.静态方法()”的方式来获取对象，仍然是通过Spring注入的形式获取 123456789101112131415//静态工厂public class ServiceFactory&#123; public static final PersonService getStaticServieFactoryImpl()&#123; return new StaticFactoryServiceImpl(); &#125;&#125;public class PersonController&#123; //注入的对象 private PersonService staticFactory; //注入对象的set方法 public void setStaticFactoryService(ServiceFactory factory)&#123; this.staticFactory=factory; &#125;&#125; 123456&lt;bean name=\"personController\" class=\"com.controller.PersonController\"&gt; &lt;!--使用静态工厂的方法注入对象，对应下面的配置文件--&gt; &lt;property name=\"staticFactory\" ref=\"factory\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--用来获取对象的方式，从工厂类中获取静态方法,factory-method用来指定调用哪个工厂方法--&gt;&lt;bean name=\"factory\" class=\"com.service.ServiceFactory\" factory-method=\"getStaticServiceFactoryImpl\"&gt;&lt;/bean&gt; 4、实例工厂实例工厂就是获取对象的方法不是静态的，需要先new工厂类。再调用普通实例的方法 123456789101112public class FactoryService&#123; public FactoryDao getFactoryDaoImpl()&#123; return new FactoryDaoImpl(); &#125;&#125;public class PersonController&#123; private FactoryDao factoryDao; public void setFactoryDao(FactoryDao dao)&#123; this.factoryDao=dao; &#125;&#125; 123456&lt;bean nam=\"personController\" class=\"com.controller.PersonController\"&gt; &lt;!--使用实例工厂的方法注入对象，对应配置如下--&gt; &lt;property name=\"factoryDao\" ref=\"factoryDao\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--获取对象的方式从工厂类中获取实例方法--&gt;&lt;bean name=\"daoFactory\" factory-bean=\"daoFactory\" factory-method=\"getFactoryDaoImpl\"&gt;&lt;/bean&gt; 三、@Order注解使用 使用@Order控制配置类的加载顺序 SpringBoot加载Bean的时候，有用到@Order注解 通过@Order指定执行顺序，值越小，越先执行。 @Order注解常用来定义的AOP先于事务执行。 如果@Order不标注数字，默认最低优先级，因为其默认值是int最大值。 1234567891011121314151617181920212223@Configuration@Component(\"xxxConfig\")@Order(Ordered.HIGHEST_PRECEDENCE)public class XXXConfig &#123; @Value(\"$&#123;xxx.key.path&#125;\") private String xxxKey; @Value(\"$&#123;xxx.config.path&#125;\") private String xxxConfig; @PostConstruct public void init() &#123; try &#123; File file = ResourceUtils.getFile(xxxKey); xxxKey = file.getAbsolutePath(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; ............. &#125;&#125;","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Java的常识总结","date":"2017-07-03T02:06:00.000Z","path":"2017/07/03/Java面试/","text":"一、Java的常识总结1、final、finally、finalize的区别 final：修饰类、方法、变量 修饰类：类不能被其他类继承 修饰方法：方法锁定，防止其他继承类进行更改 修饰变量：表示为常量，只能被赋值一次，不能更改，而且必须要初始化。 finally：只有与finally对应的try语句块得到执行的情况下，finally语句块才能执行。如果在执行try之前，方法发生异常，则finally不会执行。 finalize：是java.lang.Object定义的，每一个方法都有finalize方法 方法是在gc启动的时候，被调用。凡是new出来的对象，gc都能搞定，一般情况下我们又不会用new以外的方式去创建对象，所以一般是不需要程序员去实现finalize。 2、HashMap与Hashtable 线程是否安全：HashMap非线程安全，Hashtable线程安全。Hashtable的方法内部都是由synchronized修饰。保证线程安全建议使用ConcurrentHashMap HashMap可以用null作为一个key键，但是只有一个，value为null可以有一个或者多个。Hashtable如果put进去的key为null就会报空指针。 HashMap默认的初始化为16，每次扩容会变为原来的2倍。JDK1.8以后的HashMap解决哈希冲突的时候，链表的长度默认值为8，如果超过8的时候，链表会转化为红黑树，用来减少搜索时间。 3、HashMap和HashSet​ HashSet底层是基于HashMap实现。 HashMap HashSet 实现Map接口 实现Set接口 存key-value 存Object 调用put()添加 调用add()添加 HashSet检查重复：当add的时候，HashSet会先计算对象的hashcode值来判断对象加入的位置。 也即是先判断hashcode值，如果hashcode相同，则调用equals()方法，如果为true，则相同,HashsSet就不会加入操作成功。 HashMap多线程操作导致死循环问题，因为多线程并发情况下Rehash会造成元素之间形成一个循环链表。 4、ConcurrentHashMapJDK1.8的ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构和1.8的HashMap类似，数组+链表/红黑二叉树。 当链表长度超过8，链表（查找复杂度O(N)）将转换为红黑树（查找复杂度O(longN)） synchronized只锁定当前链表或者红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升了N倍。 5、线程线程是一个比进程更小的执行单位，一个进程在其执行的过程中可以产生多个线程。 与进程不同的是，类的多个线程共享进程的堆Heap和方法区Method Area资源。 每个线程都有自己的程序计数器PC，虚拟机栈Stack，本地方法栈Native Method Stack。 一个进程中可以有多个线程，多个线程共享进程的堆和方法区（JDK1.8以后的没有方法区，从JVM移到本地成为元空间MetaSpace） 程序计数器PC：字节码解释器通过改变程序计数器来读取指令，主要是为了线程切换后能恢复到正确的执行位置。 虚拟机栈和本地方法栈 虚拟机栈：每个java方法在执行的同时会创建一个栈帧用来存储局部变量表，操作数栈，常量池引用的信息。 本地方法栈：虚拟机栈是为Java方法也就是字节码服务，本地方法栈为虚拟机用到的Native方法服务。 堆和方法区 是所有线程共享的资源，堆是进程中最大的一块内存，主要用来存放新创建的对象，所有对象都在这里分配内存，用来存放新创建的对象。 方法区：用来存放已被加载的类信息，常量，静态变量。 6、sleep()方法和wait()方法wait()方法是java.lang.Object里的方法，用来线程间交互/通信 sleep()是thread的方法。sleep方法没有释放锁，wait会释放锁，导致其他进程占用锁。 wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的notify()或者notifyAll()方法。 sleep()方法执行完后，线程会自动苏醒。 7、start()方法和run()方法Thread thread=new Thread();线程就进入了新建装填，调用了start()方法，就会启动一个线程进入就绪状态。当分配到时间片就可以运行了。 start()：执行线程相应的准备工作，然后自动执行run()方法的内容。 直接执行run()，会把run()方法当成一个main主线程的普通方法去执行，并不会再某个线程中执行它。 run方法只是thread的一个普通方法调用，还是在主线程里执行。 8、synchronized关键字synchronized关键字可以保证被修饰的方法、代码块在任何时刻都只有一个线程执行。 JDK1.6之前，synchronized属于重量锁，因为使用监视器monitor，它是依赖于操作系统Mutex lock。需要进行线程切换从用户态到和心态。 JDK1.6之后，引入了锁优化，自选锁，锁消除，锁粗化，偏向锁。等减少了锁操作的开销。 9、synchronized和ReentrantLock都是可重入锁：自己可以再次获取自己的内部锁，比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当再次想要获取这个对象的锁还是可以获取的，如果不可重入，就会死锁。 ReentrantLock是JDK层面实现的，需要用lock()和unlock()方法配合try/finally语句块。 ReentrantLock增加的高级功能： 等待可中断：lock.lockInterruptibly(); 可实现公平锁：ReentrantLock(boolean fair)://值为true，则为公平锁。 可实现选择性通知：需要借助Condition接口和newCondition()，来实现线程通信 和wait()与notify()/notifyAll()原理类似。而Condition的singnalAll()方法会唤醒所有线程。 10、volatile关键字Java的内存模型总是从主存（共享内存）读取变量，保证内存可见性CAS。 防止指令重排序。 二、HTTP请求头http请求头，http客户程序（浏览器），向服务器发送请求的时候必须指明请求类型（GET或者POST），如果必要还可以选择发送其他的请求头。 Accept：浏览器可以接受的MIME类型 Accept-Charset：浏览器可接受的字符集 Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经过gzip编码的HTML页面，许多情形下可以减少5到10倍的下载时间 Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时使用。 Authorization：授权信息，通常出现在服务器发送的token中 Connection：表示是否需要持久连接，如果Servlet看到这里的值为Keep-Alive，或者请求使用的是Http1.1（默认为持久连接） Cookie：最重要的请求头信息之一 Host：初始URL中的主机和端口 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面 User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"AOP的使用","date":"2017-07-03T02:06:00.000Z","path":"2017/07/03/AOP的使用/","text":"一、实际开发中AOP的使用背景，为了打印controller的接口信息，以及传参等到log日志文件，方便后期查找问题。 1、首先定义一个注解类Log首先在项目的结构下，创建一个package叫做annotation包 然后新建一个注解类@interface,在IDEA中的new-&gt;class中选Annotation项目 12345@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Log &#123; String value() default \"\";&#125; 2、指定@Log注解使用的切面在项目的结构下，新建一个package叫做aop 然后新建一个类LogAspect 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131/** * @Description:日志切面 * @Author: VictorDan * @Date: 19-7-30 上午10:07 * @Version: 1.0 */@Component@Aspectpublic class LogAspect &#123; public final static Logger logger = LoggerFactory.getLogger(LogAspect.class); /** * controller层切点 */ @Pointcut(\"@annotation(com.anjuke.ai.annotation.Log)\") public void controllerAspect()&#123; &#125; /** * 前置通知，用于拦截controller层记录用户的操作 * @param joinPoint 切点 * @throws ClassNotFoundException */ @Before(\"controllerAspect()\") public void before(JoinPoint joinPoint) throws Exception &#123; HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); logger.info(\"请求IP：&#123;&#125;\",request.getRemoteAddr()); logger.info(\"请求路径：&#123;&#125;\",request.getRequestURL()); logger.info(\"请求方式：&#123;&#125;\",request.getMethod()); logger.info(\"方法描述：&#123;&#125;\",getMethodDescription(joinPoint)); &#125; /** * 环绕通知，用于方法执行结束，返回值的打印 * @param joinPoint * @return * @throws Throwable */ @Around(\"controllerAspect()\") public Object around(ProceedingJoinPoint joinPoint) throws Throwable &#123; long startTime=System.currentTimeMillis(); Object[] args = joinPoint.getArgs(); Object proceed = joinPoint.proceed(args); String result = \"null\"; if (proceed != null) &#123; result = proceed.toString(); if (result.length() &gt; 90) &#123; result = result.substring(0, 90); &#125; &#125; long endTime=System.currentTimeMillis(); logger.info(\"执行时间：&#123;&#125;ms\",endTime-startTime); logger.info(\"返回值：&#123;&#125;\\n\\t\", result); return proceed; &#125; /** * 异常通知，用来拦截controller层的异常日志 * @param ex */ @AfterThrowing(throwing = \"ex\",pointcut = \"controllerAspect()\") public void afterThrowing(Throwable ex)&#123; logger.error(\"发生异常：&#123;&#125;\",ex.toString()); &#125; /** * 获取注解中对方法的描述信息 * @param point 切点 * @return 方法描述 * @throws ClassNotFoundException */ private String getMethodDescription(JoinPoint point) throws ClassNotFoundException &#123; String targetName = point.getTarget().getClass().getName(); String methodName = point.getSignature().getName(); Object[] args = point.getArgs(); Class&lt;?&gt; targetClass = Class.forName(targetName); Method[] methods = targetClass.getMethods(); String description=\"\"; for (Method method:methods) &#123; if(method.getName().equals(methodName))&#123; Class&lt;?&gt;[] clazz = method.getParameterTypes(); if(clazz.length==args.length)&#123; description=method.getAnnotation(Log.class).value(); break; &#125; &#125; &#125; return description; &#125; /** * @RequestBody的数据，只能通过流来读取，而controller读取过request的数据，流就会关闭，所以log打印不出。 * @param httpServletRequest * @return */ private String getRequestData(HttpServletRequest httpServletRequest)&#123; HttpServletRequestWrapper httpServletRequestWrapper = new HttpServletRequestWrapper(httpServletRequest); StringBuilder sb = new StringBuilder(); BufferedReader reader = null; InputStreamReader inputStreamReader=null; ServletInputStream servletInputStream =null; try &#123; servletInputStream = httpServletRequestWrapper.getInputStream(); inputStreamReader=new InputStreamReader (servletInputStream, Charset.forName(\"UTF-8\")); reader = new BufferedReader(inputStreamReader); String line = \"\"; while ((line = reader.readLine()) != null) &#123; sb.append(line); &#125; &#125; catch (IOException e) &#123; logger.error(\"IO异常：&#123;&#125;\",e.getMessage()); &#125;finally &#123; try &#123; if(servletInputStream!=null)&#123; servletInputStream.close(); &#125; if(inputStreamReader!=null)&#123; inputStreamReader.close(); &#125; if(reader!=null)&#123; reader.close(); &#125; &#125; catch (IOException e) &#123; logger.error(\"IO异常：&#123;&#125;\",e.getMessage()); &#125; &#125; return sb.toString (); &#125;&#125; 3、在Controller中方法的注解使用就行1234567891011@PostMapping(\"/key-data\") @Log(value = \"关键数据\") public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; getKeyDataDaily(@RequestBody KeyDataDailyRequest request) &#123; BrokerUserSession brokerUserSession = getBrokerUserSession(); Long brokerId = brokerUserSession.getUserId(); request.setBrokerId(brokerId); Map&lt;String, Object&gt; data = service.getKeyDataDaily(request); boolean flag = service.isThirtyDailyPmPropZero(request); data.put(\"isZero\",flag); return resultOK(data); &#125;","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Java数据结构","date":"2017-07-02T02:06:00.000Z","path":"2017/07/02/Java版数据结构/","text":"1、数据结构的特性 数据结构 优点 缺点 数组 插入快，如果知道下标，可以非常快速的存取 删除慢，大小固定 有序数组 比无序的数组查找快 删除和插入慢，大小固定 栈 先进后出的存取方式 存取其他项很慢 队列 先进先出的存取方式 存取其他项很慢 二叉树 查找、插入、删除都快（如果树保持平衡） 删除算法复杂 红-黑树 查找、插入、删除都快。树总是平衡的 算法复杂 2-3-4树 查找、插入、删除都快、树总是平衡的。类似的树对磁盘存储有用 算法复杂 哈希表 如果关键字已知，则存储极快，插入快 删除满，如果不知道关键字则存储很慢，对存储空间使用不充分 堆 插入、删除快。对大数据项的存取很快 对其他数据项存取慢 图 对现实世界建模 有些算法慢且复杂 1.1、数组 创建数组 12345/*Java中数组作为对象处理。创建数组使用new关键字一旦创建数组，数组大小便不可改变*/int[] arr=new int[10]; 访问数组数据项 12/*数组数据项通过方括号的下标来访问*/arr[0]=100; 数组初始化 1int[] arr=&#123;1,2,3,4,5&#125;; 1.2、有序数组 有序数组特点 有序数组按照数组元素一定的顺序排列，方便使用二分查找来查找数组中特定的元素。有序数组提高了查询效率，并没有提高删除和插入元素的效率。 1.3、查找算法 线性查找 查找的时候，将要查找的数一个个的与数组中的元素比较，直到找到要找的数。 二分查找（折半查找） 不断的将有序数组进行对半分割，每次拿中间位置的数和要查找的数进行比较： target&lt;middle,target在数组的前半段。 target&gt;middle,target在数组的后半段 target==middle,target就是中间数 12345678910111213141516171819202122/*二分查找索引（前提：有序）*/public int binarySearch(long value)&#123; int middle=0; int left=0; int right=size-1; while(true)&#123; middle=(left+right)/2; if(arr[middle]==value)&#123; return middle; &#125;else if(left&gt;right)&#123; return -1;//不存在 &#125;else&#123; if(arr[middle]&gt;value)&#123; right=middle-1;//在左半部分 &#125;else&#123; left=middle+1;//在右半部分 &#125; &#125; &#125;&#125; 1.4、栈和队列​ 栈和队列都是抽象数据类型（Abstract Data Type，ADT），既可以用数组实现，又可以用链表实现。 1.4.1、栈123456789101112131415161718192021222324252627public class TestStack&#123; private long[] arr; private int top; public TestStack()&#123; arr=new long[100]; top=-1; &#125; public TestStack(int maxSize)&#123; arr=new long[maxSize]; top=-1; &#125; public void push(long value)&#123; arr[++top]=value; &#125; public long pop()&#123; return arr[top--]; &#125; public long peek()&#123; return arr[top]; &#125; public boolean isEmpty()&#123; return (top==-1); &#125; public boolean isFull()&#123; return (top==arr.length-1); &#125;&#125; 1.4.2、链表 链结点：一个链结点（Link，结点）是某个类的对象，该对象中包含对下一个链结点引用的字段（next） 1234567891011/*结点定义*/public class Link&#123; public long data;//data域 public Link next;//next Link in list public Link(long data)&#123; this.data=data; &#125; public void displayLink()&#123; System.out.println(data+\"---&gt;\"); &#125;&#125; 单链表（LinkList） 123456789101112131415161718192021222324252627public class LinkList&#123; private Link first;//定义头结点 public LinkList()&#123; this.first=null; &#125; public void insertFirst(long value)&#123; Link newLink=new Link(value); newLink.next=first; first=newLink; &#125; public Link deleteFirst()&#123; if(first==null)&#123; return null; &#125; Link temp=first; first=first.next; return temp; &#125; public void displayList()&#123; Link current=first; while(current!=null)&#123; current.displayLink(); current=current.next &#125; System.out.println(); &#125;&#125;","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"算法与数据结构","date":"2017-07-01T02:06:00.000Z","path":"2017/07/01/算法与数据结构/","text":"一、线性表1、线性表的顺序存储12345678const LIST_INIT_SIZE=100;//表初始分配空间const LISTINCREMENT=10;//空间分配增量typedef struct&#123; ElemType *elem;//存储空间 int length;//当前长度 int listsize;//当前存储容量 int LISTINCREMENT;//可增加存储空间（可能初始分配存储空间会用完，可以增加一个10，从100变为110）&#125;SqList; 1、线性表的初始化（创建一个空的线性表）12345678910111213Status InitList_Sq(SqList &amp;L)&#123; //构造空表L，1、首先是要申请存储空间 L.elem=(ElemType *)malloc(LIST_INIT_SIZE*sizeof(ElemType)); //因为申请存储空间需要系统分配，内存够用的时候才能申请到相应的存储空间 if(!L.elem)&#123; exit(OVERFLOW); &#125; //2、然后把表长度置为0 L.length=0; //3、然后把初始的存储空间赋给它 L.listsize=LIST_INIT_SIZE; return OK;&#125;//InitList_Sq 注意：ElemType为数据类型，比如可以为int类型。 1L.elem=(int *)malloc(LIST_INIT_SIZE*sizeof(int)); 线性表有两种结构： 物理结构：也就是存储结构 逻辑结构：也就是你定义的结构，可以为顺序结构，线性结构。 顺序存储结构：它的逻辑结构与物理结构是相同的，也就是用连续的存储单元。只需要定义一组连续的存储单元就行，然后把这个连续的存储单元的基地址，用某个变量来标注它。就比如：ElemType *elem，然后每个元素的地址系统都可以自己计算出来的。顺序存储结构，基地址知道了，就可以随机的存取。你取任何一个元素访问的时间是相同的。 在顺序存储结构中，定义的方式不仅仅是一个数组。此处定义采用的是指针的方法。因为基地址可以用一个指针来记住它。 2、删除线性表中的第i个元素123456789101112131415Status ListDelete_Sq(SqList &amp;L,int i,ElemType &amp;e)&#123; //删除L中第i个元素，后面的元素前移 if((i&lt;1)||(i&gt;L.length))&#123; return ERROR; &#125; p=&amp;L.Elem[i-1];//p是当前这个元素的地址。 e=*p;//*p是，p指向当前这个单元的内容，也就是i-1元素的内容放在e这个变量里 q=L.Elem+L.length-1;//L.Elem是L的基地址,在加上L.length-1就是最后一个元素的地址，所以q其实是指向最后一个元素。 //（q=L.Elem+L.length-1这里只是计算地址的一个过程）,p指针是指向第i-1个元素，q指针指向最后一个元素 //从第i个元素开始到最后一个元素都往前移 for(++p;p&lt;=q;++p)*(p-1)=*p;//p&lt;=q，也即是p指针在q之前。第一个++p的指针加加是，在做for语句的时候，一旦做循环p指针先加1，也就是移动到了第i个位置。在进行for的时候，仅仅执行一次。第2个++p，是每次for循环执行完之后都要再+1。 //*(p-1)=*p;，意思是把p指针所指向的内容往前移，移到p-1。也就是元素往前移，都是用星号，目的是取元素里的内容 --L.length; return OK;&#125;//ListDelete_Sq 注：第i个元素的下标是i-1，因为数组是从0开始的。 3、顺序表的应用例1、设A=(a1,a2,a3,…am),B=(b1,b2,b3,…bn)为两个线性表，试写出比较A,B大小的算法。 比较原则：首先去掉A,B两个集合的最大前缀子集之后，如果A,B为空，则A=B；如果A空B不空，A&lt;B；如果B空A不空，则A&gt;B;如果A和B均不空，则首元素大者为大。 分析：A,B看成是顺序表，从前往后一次比较他们两个相同下标的元素。 123456789101112int compare(SqList A,SqList B)&#123; //若A&lt;B，返回-1，A=B返回0，A&gt;B返回1 int j=0; while(i&lt;A.length&amp;&amp;j&lt;B.length)&#123; if(A.elem[j]&lt;B.elem[j])return -1; else if(A.elem[j]&gt;B.elem[j])return 1; else j++; &#125; if(A.length==B.length)reutrn 0; else if(A.length&lt;B.length)return -1; else return 1;&#125; 例2、设计一个算法，用尽可能少的辅助空间将顺序表中前m个元素和后n个元素进行整体互换。即将线性表,也就是O(m+n)空间复杂度 (a1,a2,…am,b1,b2,…,bn)—–&gt;(b1,b2,….,bn,a1,a2,…,am) 如果两两交换的话，要考虑m，n的大小。 参考算法1：取一个临时空间，将b1放入临时空间， 把a1到am看成一个整体搬迁后移一个位置，如此b2,b3,直到bn。 12345678910void exchange(SqList &amp;L,int m,int n)&#123; //线性表分成两个部分后，两部分倒置 for(int i=0;i&lt;n;i++)&#123;//第一个for把b1到bn一个个放到临时存储空间w int w=L.Elem[i+m];//i+m=0+m也即是b1的下标 for(j=m;j&gt;=1;j--) L.Elem[i+j]=L.Elem[i+j-1];//然后把a1到am元素整体往后移动一个位置。 L.Elem[i]=w; &#125;&#125;算法特点：牺牲时间节省空间，时间复杂度O(m*n) 参考算法2：如果另外申请空间m+n个存储单元，将b，a分别写入，时间复杂度将为m+n,也就是：牺牲空间节省时间。 1234567891011void exchange(SqList &amp;L,int m,int n)&#123; SqList List;//为m+n长度 //存放b1,b2,...bn for(int i=0;i&lt;n;i++)&#123; List.Elem[i]=L.Elem[i]; &#125; //存放a1,a2,am for(int i=m;i&lt;=L.length-1;i++)&#123; List.Elem[i]=L.Elem[i]; &#125;&#125; 2、线性链表的表示、实现、操作 顺序表的局限：插入、删除时要移动大量的元素，耗费大量时间。 链式表示：用一组任意的存储单元存储线性表 存储单元不要求连续：物理结构不反应逻辑结构 不可以随机存取，但插入和删除方便 需要两个域：一个表示数据本身；一个表示数据元素间的先后关联。合起来是一个结点。 S结点中表示关联的部分为指针域，内部存放指针或者链。结点链接成一个链表。 2、线性链表的表示、实现、操作3、循环链表的定义1、线性链表（单链表）的定义：1234typedef struct LNode&#123; ElemType data; struct LNode *next;//指针域指向的是下一个结点，它指向的自然也就是结点类型。&#125;LNode,*LinkList//这个LinkList是一个指针 带头结点的单链表：头结点：data里没有数据。比如把表里元素都删除。总不能只剩下一个L吧。头结点的增加仅仅是为了运算的方便。做题的时候，要看清楚是不是带头结点的链表，默认都是带头结点的链表。 2、线性表的基本操作：1、初始化线性链表也就是创建一个仅仅有头结点的空表。 123456Status InitList_L(LinkList &amp;L)&#123;//LinkList，就是刚刚在struct结构体定义的*LinkList //建立头结点，其next为空 L=(LinkList)malloc(sizeof(LNode)); L-&gt;next=NULL; return OK; &#125; 2、GetElem在单链表中的实现GetElem_L(L,i,&amp;e)也就是把第i个元素取出来，赋给变量e。 思想：可以使用一个指针p指向第一个元素，然后再用一个计数器flag=1。 或者让指针p指向头结点，可以把计数器flag=0；然后指针不断的往后，计数器flag要不断的+1。 12345678910111213Status GetElem_L(LinkList L,int i,ElemType &amp;e)&#123;//使用&amp;e，是因为e的值会发生变化。因为把获取到的值，赋给了e。 //L为带头结点的单链表的头指针 //当第i个元素存在时，将值返回给e，OK,否则ERROR LNode *p=L-&gt;next;//L-&gt;next指向第一个元素，相当于是p指向第一个元素 int flag=1; while(p&amp;&amp;flag&lt;i)&#123;//while条件中的p，p是不为空的意思。因为p是一个指针，while(p)表示，p指针存在，也就是p指向的元素是存在的，并且flag是小于i的 p=p-&gt;next;//那么我就把p-&gt;next赋给p，也就是p指针后移 ++flag; &#125; if(!p||flag&gt;i)return ERROR;//比如可能有这种情况，链表只有5个元素，但是i给你的是6，这时候也就是p不存在了，自然就是error了 e=p-&gt;data; return OK; &#125;//GetElem_L 注：顺序表与单链表在取第i个元素的操作上有什么不同？ 顺序表：随机存取 3、插入操作：在第i个结点之前插入一个元素e1234567891011121314151617Status ListInsert_L(LinkList &amp;L,int i,ElemType e)&#123; //在线性表的第i个元素之前插入一个元素e p=(LinkList)malloc(sizeof(LNode)); if(!p)exit(OVERFLOW); p-&gt;data=e; q=L; flag=0; while(q&amp;&amp;flag&lt;i-1)&#123; q=q-&gt;next; ++flag; &#125; if(!q||flag&gt;i-1)return ERROR; P-&gt;next=q-&gt;next; q-&gt;next=p; return OK;&#125;//ListInsert_L时间复杂度分析：O(n) 总结：只要是把值赋给e的操作，都是要用Elem &amp;e，如果只是单纯把e赋给别的，就用Elem e好了。 12345678LNode* LocateElem_L(LinkList L,ElemType e)&#123; //在L中找到第一个值和e相同的结点，返回其地址，若不存在，返回空值 if(!L)return NULL; p=L; while(p&amp;&amp;p-&gt;data!=e)p=p-&gt;next;//只有当!p也就是p不存在，或者是p-&gt;data=e才循环结束。也就是返回p。 return p;&#125;时间复杂度为O(n) 例题1：将两个有序链表归并为一个新的有序链表。 分析：将Lb中的元素插入到La，使其有序。 pa-&gt;data &gt;pb-&gt;data:将pb插入到pa的前面。插入过程中，注意临时指针的使用。 声明两个指针，pa指向La的第一个元素，pb指向Lb的第一个元素。 还要额外增加两个辅助变量来记录。比如用p来记录pa前面那个元素。 然后用q来记录pb前面的那个元素。 1234567891011121314151617181920212223242526void mergeList(LinkList &amp;La,LinkList &amp;Lb)&#123; //归并两个非递减单链表La，Lb，形成新的La pa=La-&gt;next; pb=Lb-&gt;next; p=La;//p其实就是pb的前面那个元素,是一个存储单元 while(pa&amp;&amp;pb)&#123;//当pa存在，并且pb存在 if(pa-&gt;data&lt;=pb-&gt;data)&#123;//如果pa的data域小于pb的data域，让p和pa存在先后关系。就是说把pa赋给p，pa赋给pa的下一个。 p=pa; pa=pa-&gt;next; &#125; else&#123;//否则就要插入了。 t=pb;//让t来存pb的前面那个元素 pb=pb-&gt;next; /* 以下三句是做插入操作。因为pb的data比pa大，所以前插入，因为此 时的pa已经是next了所以先连后面的，用t-&gt;next连到pa，然后p是pa前 一个元素，则p的next指针域连到t。然后再把p移动到t的位置，因为此时 t的位置是新插入的元素，你原来的元素已经不是p的位置了，此时新插入 的元素在pa之前。总之，始终要保证p在pa的前一个元素。 */ t-&gt;next=pa; p-&gt;next=t; p=t; &#125; if(pb) //如果此时pa=La-&gt;next已经为NULL不存在了那就直接把pb插到pa的后面 p-&gt;next=pb; &#125;&#125;时间复杂度为O(n) 4、双向循环链表的定义、操作5、线性表的典型应用二、栈和队列1、栈的抽象数据类型 栈：一个只能在栈顶进行插入和删除的线性表，其特征为FIFO。 栈的表示： 顺序栈：栈的顺序存储 优点：处理方便 缺点：数组大小固定，容易造成内存资源的浪费。 链栈：栈的动态存储 优点：能动态改变链表的长度，有效利用内存资源 缺点：处理较为复杂 1.1、顺序栈的表示和实现123456789101112131415161718#define STACK_INIT_SIZE 100;#define STACKINCREMENT 10;//可增长的长度。当100个存储空间用完以后，可增加。typedef struct&#123; SElemType *base;//base表示将要申请的这一组存储空间的基地址。 SElemType *top; int stacksize;&#125;SqStack;/**stacksize:表示栈当前可以使用的最大容量。base:栈底top：栈顶栈顶指针指向栈顶元素的下一个位置。也就是下次压栈时元素所放的位置。*/top指向的是下一个元素将要存放的位置。top减一指向弹栈时下一个元素的取值位置。栈空的条件：top=base;栈满的条件：top-base&gt;=stacksize;因为每次top都是指向栈顶元素的下一个位置。那么出栈的时候，top先-1，然后到了栈顶元素。 123456789101112//初始化栈Status InitStack(SqStack &amp;S)&#123;//这里使用&amp;S，是因为S发生变化，因为给S申请存储空间了。 //构造一个空栈 //1、首先申请一块连续的存储空间，base地址是你申请存储空间的时候，系统给的地址。 S.base=(SElemType *)malloc(STACK_INIT_SIZE*sizeof(SElemType)); if(!S.base)exit(OVERFLOW); //2、初始化的空栈的时候，top指针等于base指针。 S.top=S.base; //3、初始化栈的大小 S.stacksize=STACK_INIT_SIZE; return OK;&#125;//InitStack 1234567891011121314//压栈Status Push(SqStack &amp;S,SElemType e)&#123;//这里使用&amp;S，因为要插入e到栈S里，S发生变化了，所以要用&amp;，e不用加&amp;是因为，e只是插入，并没有给e赋值，而让e发生变化 //元素e插入到栈中S if(S.top-S.base&gt;=S.stacksize)//栈满 &#123; newbase=(SElemType *)realloc(S.base,(S.stacksize+STACKINCREMENT)*sizeof(SElemType));//重新申请存储空间 if(!newbase)exit(OVERFLOW);//如果不能申请 else S.base=newbase;//可以申请则把新的基地址赋给原来的base S.top=S.base+S.stacksize; S.stacksize+=STACKINCREMENT; &#125;//if *S.top++=e;//因为top永远指向栈顶元素的下一个。这个等价于*S.top=e，然后S.top++; return OK; &#125;//Push 12345678//出栈Status Pop(SqStack &amp;S,SElemType &amp;e)&#123; //因为出栈，然后再把元素赋值给e，e发生变化了，所以要用&amp;e //从栈顶读取数据放入e内，栈中下一元素所在位置成为新的栈顶 if(S.top==S.base)return ERROR;//栈空 e=*--S.top;//因为top栈顶指针永远指向栈顶元素的下一个位置，所以出栈的时候，你要先进行减1，到栈顶位置，然后再取栈顶元素的数据。然后赋值给e return OK;&#125; 注：top的操作顺序和值的变化 压栈：value-&gt;top;top++;//首先是把值赋给top后，然后top再++,指向当前压栈元素的下一个 弹栈：top–;top-&gt;value;//首先是top先–，指向栈顶元素，然后再取值。 1.2、链栈的结构和表示1234567//定义栈结构typedef struc stack_node&#123; ElemType data; struct stack_node *next;&#125;LinkStack*;LinkStack stk;//stk标识这个栈的栈顶元素，也就是头指针，标识栈顶的作用，所以并不需要定义一个top指针。//栈底：最先入栈的 1234567891011//入栈pushStatus Push(LinkStack &amp;stk,ElemType x)&#123;//stk表示这个栈的栈顶元素也就是头指针。标识栈顶位置的作用。 LinkStack top;//此处top无任何意义，只是一个标识，用p，q都可以，仅仅是一个即将要入栈的x的存储空间而已，是一个过渡的辅助变量而已。因为我们已经用stk表示这个栈了，也就是说stk标识这个栈的栈顶元素的位置了。起到了标识栈顶位置的作用。 //我要插入x，肯定要给x申请存储空间，因为top是指向当前x元素的 top=malloc(sizeof(LinkStack)); top-&gt;data=x; //自然要把top-&gt;next连接到stk上，指向栈顶指针。 top-&gt;next=*stk; //然后此时栈顶元素的指针就不是stk了，而是刚刚入栈的元素了。也就是把top赋给stk成为新的栈顶指针。 *stk=top;&#125; 1234567//出栈：相当于把单链表的表头元素给删除掉Status Pop(LinkStack &amp;stk,ElemType &amp;e)&#123;//因为pop之后，你栈顶元素出来之后，stk变化了，所以我们要在stk变量前面加上一个&amp;字符。因为pop出来这个元素后，赋值给e这个变量，e也发生变化了，所有也加上&amp; LinkStack p=stk;//用临时指针p先记住stk的位置 p=p-&gt;next; free(p);//这些是不带头结点的操作。 //如果是带头结点的话，stk指向的是表头一个空的，stk-&gt;next才是指向真正的栈顶元素。&#125; 2、队列123456789//初始化空队列QInitQueue(&amp;Q);//Q为空则trueQueueEmpty(Q);//入队，x加入，成为Q的新队尾EnQueue(&amp;Q,x);//出队，删除队头元素，然后把对头元素赋给xDeQueue(&amp;Q,&amp;x);GetHead(Q,&amp;x) 2.1、顺序队列1234567#define MaxSize 50typedef struct Node&#123; ElemType data[MaxSize]; int front,rear;&#125;SqQueue;//队空条件：Q.front==Q.rear==0; 2.2、循环队列12345初始：Q.front=Q.rear=0;队首指针进1：Q.front=(Q.front+1)%MaxSize;队尾指针进1：Q.rear=(Q.rear+1)%MaxSize;队列长度：(Q.rear+MaxSize-Q.front)%MaxSize;队满条件：(Q.rear+1)%MaxSize==Q.front; 队列初始化 123void InitQueue(&amp;Q)&#123; Q.rear=Q.front=0;//初始化队首、队尾指针&#125; 判队空 1234bool isEmpty(Q)&#123; if(Q.rear==Q.front)return true; else return false;&#125; 入队 1234567bool EnQueue(SqQueue &amp;Q,ElemType x)&#123; if((Q.rear+1)%MaxSize==Q.front)return false;//队满 Q.data[Q.rear]=x; //入队从队尾 Q.rear=(Q.rear+1)%MaxSize;//队尾指针加1 return true;&#125; 出队 1234567bool DeQueue(SqQueue &amp;Q,ElemType &amp;x)&#123; if(Q.rear==Q.front)return false;//出队之前，先判断队是否为空 x=Q.data[Q.front]; //出队从队头开始 Q.front=(Q.front+1)%MaxSize;//队头指针加1取模 return true;&#125; 2.3、链式队列12345678//结点定义typedef struct Node&#123; ElemType data; struct LinNode *next;&#125;LinkNode;typedef struct Q&#123; LinkNode *front,*rear;&#125;LinkQueue; 初始化 1234void InitQueue(LinkQueue &amp;Q)&#123; Q.front=Q.rear=(LinkNode*)malloc(sizeof(LinkNode));//建立头结点 Q.front-&gt;next=NULL;&#125; 判队空 1234bool isEmpty(LinQueue Q)&#123; if(Q,front==Q.rear)return true; else return false;&#125; 入队 1234567void EnQueue(LinkQueue &amp;Q,ElemType x)&#123; s=(LinkNode *)malloc(sizeof(LinkNode)); s-&gt;data=x; s-&gt;next=NULL; Q.rear-&gt;next=s;//将新节点插入到队尾 Q.rear=s;//队尾指针移动到新的结点s&#125; 出队 123456789bool DeQueue(LinkQueue &amp;Q,ElemType &amp;x)&#123; if(Q.front==Q.rear)return false;//空队 p=Q.front-&gt;next;//因为有头结点，所以对头应该是next，然后给p临时变量 x=p-&gt;data; Q.front-&gt;next=p-&gt;next;//然后让头结点，指向头结点的下一个结点。 if(Q.rear==p)Q.rear=Q.front;//如果队列中只有一个结点，则队头等于队尾，为空了。 free(p); return true;&#125; 三、二叉树12345678910111213141516171819202122232425262728293031323334353637383940414243//二叉树的链式存储结构typedef struct BiTNode&#123; ElemType data; struct BiTNode *lchild,*rchild;&#125;BiTNode,*BiTree;//先序遍历void PreOrder(BiTree T)&#123; if(T!=NULL)&#123; visit(T); PreOrder(T-&gt;lchild); PreOrder(T-&gt;rchild); &#125;&#125;//中序遍历void InOrder(BiTree T)&#123; if(T!=NULL)&#123; InOrder(T-&gt;lchild); visit(T); InOrder(T-&gt;rchild); &#125;&#125;//后续遍历void PostOrder(BiTree T)&#123; if(T!=NULL)&#123; PostOrder(T-&gt;lchild); PostOrder(T-&gt;rchild); visit(T); &#125;&#125;//层次遍历void LevelOrder(BiTree T)&#123; InitQueue(Q); BiTree p;//p是遍历指针 EnQueue(Q,T);//将根结点入队 while(!isEmpty(Q))&#123;//队列不空 DeQueue(Q,p);//队头元素出队 visit(p);//访问当前p所指向的结点 if(p-&gt;lchild!=NULL) EnQueue(Q,p-&gt;lchild); if(p-&gt;rchild!=NULL) EnQueue(Q,p-&gt;rchild); &#125;&#125; 四、图 连通：在无向图中，从顶点v到顶点w有路径存在 连通图：图中任意两个顶点都是连通的。 连通分量：无向图中的极大连通子图，极大也就是要包含所有的边。 极小连通子图：既要保证图连通，也要保证边数最少。 强连通：有向图，顶点v到顶点w，与顶点w到顶点v之间都有路径。，为强连通的。 强连通图：邮箱途中，任意一对顶点都是连通的。 边权：图的每条边标上数值，数值为权值 网：带边权的图为网。 1、图的存储","link":"","tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://victorblog.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}]},{"title":"操作系统","date":"2017-06-21T02:06:00.000Z","path":"2017/06/21/一、操作系统概述/","text":"一、操作系统概述1、指令权限与工作模式处理器能处理的指令分为： 特权指令：需要特殊权限才可以使用的指令，通常涉及系统安全。 非特权指令：任何情况下都可以使用的指令。 处理器的控制寄存器中用专门的位标识当前的工作模式： 核心态(管态)：已获得特殊权限的状态，能够执行所有指令。 用户态(目态)：只能指令非特权指令。 事实上，现代处理器通常具有更多层权限和工作模式。从高到低划分为特权级0(管态)，特权级1，特权级2，特权级3(目态)。 2、内存空间与操作系统的内核处理器处于核心态时可以将内存的不同区段分别设置为两种类型： 内核(系统)空间：其中存储的程序将运行在核心态上，从而可以使用所有指令，且可以访问所有内存空间数据。 用户空间：其中存储的程序将运行在用户态上，于是只能使用非特权指令，且只能访问用户空间数据。 操作系统通常在启动时指定内核空间，并将其核心程序放置在内核空间内。操作系统的这一部分叫内核，操作系统的其他部分叫核外部分。 3、操作系统的内核操作系统的内核通常包含以下程序和数据： 与硬件密切相关的操作 关键数据结构 基本中断处理程序 使用频繁的功能模块 操作系统的内核具有两个基本特点： 常驻内存 运行在核心态 4、固件操作系统的启动依赖于一组称作固件（Firmware）的特殊软件。 固件存储在只读存储器ROM中，也可以将部分程序或者数据存储在硬盘中。 与PC启动相关的固件有BIOS和UEFI，他们在计算机启动时首先被运行，并运行基本硬件的检查，装入操作系统引导程序，提供基本硬件驱动程序和中断处理程序等。 5、基本输入输出系统(BIOS)BIOS包含3个部分： POST自检程序：计算机启动后识别硬件配置、自检、初始化。 基本启动程序：在存储介质中查找有效的主引导程序，如果找到，将它读入内存[0000:7C00]处并开始执行，从而引导操作系统启动。 基本硬件驱动程序以及中断处理程序：用来在未加载操作系统时识别和控制基本IO设置(键盘、鼠标、显卡、硬盘等) 硬盘的主引导记录MBR 5.1、基于BIOS的操作系统启动过程 硬件自检 加载驱动 基本启动程序 主引导程序(440字节) 引导程序(512字节) 操作系统装载程序 操作系统 操作系统装载程序Loader的作用： 进一步检查和配置系统硬件。 建立内核空间，装入操作系统内核，并初始化数据。 加载用户登录程序和人机交互环境。 6、统一的可扩展固件接口UEFIIntel公司在1997年新推出的高性能处理器，计划设计一种可扩展的、标准化的固件接口规划EFI(Extensible Firmware Interface)，用来计算机系统的启动以及提供与操作系统的接口。在2006年EFI发展成UEFI(Unified EFI)。 UEFI的特点： 驱动程序环境DXE 磁盘管理采用GUID分区方法 UEFI应用程序 6.1、基于UEFI的操作系统启动过程UEFI体系包含2部分内容： 平台初始化框架：包含了支持系统启动的底层程序，包含：安全检查、硬件自检和初始化、DXE、启动管理器。 UEFI映像(UEFI Image):一组在框架的支持下可执行的程序，包括启动管理器、操作系统装载程序、临时操作系统装载程序、EFI应用程序、设备驱动程序。 UEFI固件的启动过程： 安全检查 Pre-EFI 加载DXE 启动管理器 临时操作系统装载程序 EFI应用程序 操作系统装载程序 操作系统 7、GUID硬盘分区方式UEFI采用GUID硬盘分区方式，优点是： 扇区块号为64位，消除2TB限制。 磁盘分区数不受4个限制，最多划分128个分区 分区扇区数为64位，消除2TB限制 自带备份，增加可靠性 采用CRC32校验，增加完整性 每个分区都有唯一标识GUID 8、操作系统的用户接口–命令接口命令接口是人机交互的主要途径 8.1、按命令接口在操作系统内的实现方式，分为 内部命令：内核完成的命令 外部命令：由其他应用程序完成的命令 8.2、按命令接口的使用方式，分为： 脱机命令：预先编写好，送交执行后无法立即反馈的命令接口形式。 比如批处理系统中的JCL 联机命令：交互式执行，能够立即得到反馈的命令接口形式。 比如分时操作系统的交互式命令接口。又可细分为字符命令、菜单命令和图标命令。 9、操作系统的用户接口–程序接口(系统调用)9.1、系统调用的简介程序接口是程序员在编写应用程序时使用操作系统功能的接口。目前，程序接口通常通过系统调用System Call方式实现。 系统调用在不同的上下文中，可能表示2种含义： 一组由操作系统设计者编写的内核中的子程序，用来封装程序员可能使用到的操作系统功能。 程序员在编程中对这些子程序进行调用的行为 系统调用所涉及的子程序属于操作系统内核的一部分，保存在内存空间中，应用程序不能直接访问，于是需要通过特殊方式才能实现系统调用。 9.2、系统调用的实现： 放置调用号标识 访管指令：发出表示系统调用请求的中断或者陷阱 处理器保护程序现场，转向中断处理（内核态） 中断处理程序识别中断为系统调用请求，于是转向内核的访管指令处理程序 访管指令处理程序在地址入口表中查找所请求的调用号对应的内核子程序的入口地址。 从查找到的地址开始执行内核子程序(系统调用)并得到结果。 恢复应用程序现场，返回用户态。 9.3、系统调用与用户子程序对比 对比项目 系统调用 用户子程序 运行环境 内核态（管态） 用户态(目态) 访问方式 访管指令（中断或者陷阱） 直接跳转、返回 与主程序关系 与主程序分开、独立 同一进程地址空间 共享性 可以在多个进程间共享 同一进程内部调用 二、进程管理1、程序与系统的工作流程程序应具体2个基本特点： 顺序性 可再现性 系统的工作流程分为顺序执行和并发执行 顺序执行的工作方式具有2个特点： 封闭性：独占全部资源，不受其他程序影响 可再现性：重复运行的结果相同 并发执行的工作方式中，多道程序 微观上看，轮流执行 宏观上看，同时执行 并发执行的工作方式有3个特点： 随机性：运行时情景随机 不可再现性：重复运行结果可能不同 相互制约：资源争夺和协作关系 2、进程Dijkstra：一道程序在一个数据集上的一次执行过程，叫一个进程Process IBM：程序的运行过程成为任务Task 进程和任务都是运行着程序的描述，二者可以互换使用，不加区别。 进程的主要特征： 动态性：具有生命周期，状态不断变化 并发性：可以并发执行 独立性：互相独立，只能访问自身的内存空间 结构性：不同的进程可抽象出相同的属性，从而抽象结构相同 异步性：运行时情况不可预知 3、进程的动态性进程的3种状态 就绪：能够运行但却未在被处理器处理的状态 运行：正在被处理器处理的状态 阻塞：由于各种原因(IO操作、资源缺乏等)暂时无法运行的状态。 4、进程信息的描述–PCB进程控制块PCB：是操作系统用来描述进程信息的数据结构，由3部分组成： 基本信息部分： 进程名：通常是程序名 进程标识符PID：操作系统用来识别进程的唯一标识 用户标识：创建进程的用户标识 进程状态：运行、就绪或者阻塞 管理信息部分： 程序和数据地址：程序和数据在内存中的位置信息 IO操作相关参数：进程进行IO操作时的参数 进程通信信息：用来进程间通信的消息队列指针等。 控制信息部分： 现场信息：进程离开运行状态时，保存进程现场以备恢复。 调度参数：与进程调度相关的参考信息，如优先级、运行时间等 同步、互斥信号量：用来保证进程间同步、互斥关系的信息。 5、PCB队列和进程管理操作系统用PCB来描述进程信息，并利用2种PCB队列来组织这些PCB: 就绪队列：就绪进程的PCB组成的队列，按照进程性质或者优先级，系统中可以有多个就绪队列，用来提高进程调度算法选择进程的效率。在多处理器系统中，就绪队列也叫请求队列，每个处理器都有独立的请求队列。 等待队列：阻塞进程的PCB组成队列，按照阻塞原因，系统中可以有多个等待对了。方便在阻塞原因解除的时候统一调整进程状态。 6、原语改变进程状态操作不应该被打断，也就是具有原子性。 原语是特殊的程序段，这样的程序段中的所有指令要么全部执行要么一个都不执行。原语就是具有原子性的程序段。 原语可以被看做广义的指令，指令都具有原子性 原语中的指令若执行不成功，系统需要回退到原语执行前的状态。 定义原语是为了保证系统运行的一致性。 7、进程控制–创建进程控制通过一组原语来实现，包括进程的创建、撤销、阻塞、唤醒和切换。 1、进程的创建原语Create用来创建一个进程Create可能在以下情况下被使用： 按照作业调度程序的作业步规定 按照用户提交的命令 用户程序使用了创建进程的系统调用比如Unix的fork Create被使用时完成以下操作： 建立PCB 生成进程标识符PID 初始化进程和PCB 将PCB加入适当的就绪队列 8、进程控制–撤销1、进程的撤销原语Destroy用来结束一个进程Destroy可能在以下情况下被使用： 进程执行完毕 进程出错而结束 父进程结束 人为终止 Destroy被使用时完成以下操作： 进程资源回收 删除进程PCB 9、进程控制–阻塞1、进程的阻塞原语Block用来让一个进程进入阻塞状态Block在阻塞原因发生时被使用。 Block被使用时完成以下操作： 修改进程状态为阻塞状态 将进程的现场保存，以备恢复 将进程PCB加入合适的等待队列 10、进程控制–唤醒1、进程的唤醒原语Wakeup用来让一个进程从阻塞状态进入就绪状态Wakeup在阻塞原因解除时被使用。 Wakeup被使用时完成以下操作： 将进程PCB从等待队列中移除 修改进程状态为就绪状态 将进程PCB加入合适的就绪队列 阻塞原因解除后，进程未必可以直接运行，因此合理的唤醒原语并不会将阻塞状态直接修改为运行状态。 11、进程控制–切换进程切换使处理器的控制权在进程之间专员，无法通过操作系统内核独立完成，必须借助硬件来实现。 进程：系统调用（系统调用中断），外部因素（IO中断，异常），调度维护（计时器中断），然后进程进入中断、异常处理机制。然后在通过内核功能，控制原语，进程调度等。 12、相关进程间的制约关系–间接制约间接制约关系：仅由于使用相同的资源而造成的制约。 比如：2个进程p1和p2分别使用同一台打印机，逐行打印各自文本。 1、间接制约与临界资源 在操作系统的控制下，进程对系统资源的访问逻辑上需要经历3个步骤：申请、使用、归还。 某些系统资源，在进程从申请到归还的周期中只能允许此进程独占，否则将造成运行错误。则称这样的资源为临界资源。 常见的临界资源有：打印机，存储单元，文件等。 间接制约实质上是一组并发进程共享某种临界资源的制约关系。 2、间接制约与护持关系 临界区Critical Section/Critical Region：是指进程对应的程序中访问临界资源的一段程序代码，即进程从申请资源开始到归还资源为止的一段程序代码。 对于一组(在某资源上)存在间接制约关系的并发进程，可以通过建立一种机制来避免制约关系导致的运行错误：当一个进程在这种临界资源对应的临界区内执行时，其他要求进入相关临界区的进程必须等待。 上面的机制得以建立，则称这些并发进程为互斥进程，它们之间建立了护持关系。 3、临界区管理实现互斥关系的关键是对临界区的执行进行有效控制。 临界区管理的准则有： 空闲让进：如果进程请求进入临界区，而没有其他进程在相关临界区内执行，则允许进入。 忙则等待：如果有进程在临界区内执行，则其他要求进入相关临界区的进程都需要等待。 有限等待：对于要求进入临界区的进程，等待的时间应该是有限的 让权等待：当一个进程离开临界区时，应该让请求进入相关临界区的下一个进程进入临界区执行。 4、互斥关系的实现–加锁机制加锁机制由3个要素组成：锁变量key，加锁操作lock(key)和解锁操作unlock(key) 12345678lock(key)&#123; while(key==1); key=1;&#125;unlock(key)&#123; key=0;&#125; 这个机制中，锁变量key=1时表示上锁状态，key=0为未上锁。 当进程申请资源时需要对资源加锁，若资源已经上锁，则上锁操作 将进行反复测试也就是反复等待，直到资源被解锁了；当进程归还资源的时候需要对资源解锁。 高级语言，软件实现的加锁机制并不能实现真正的加锁，因为加锁操作内部仍可能被打断。 加锁机制的缺陷： 存在忙等待现象，浪费了处理器时间 存在饥饿现象，进程等待时间可能无限长 多个锁变量的加锁操作可能导致进程死锁 5、互斥关系的实现–信号量机制信号量机制由3个要素组成：信号量s，原语p和原语v 也就是所谓的pv操作 12345678910111213141516struct semaphore&#123; int value;//信号量的值 PCB *bq;/由于该信号量而阻塞的进程队列PCB队列，(阻塞队列/等待队列)&#125;p(s)&#123; s.value=s.value-1; if(s.value&lt;0) blocked(s);&#125;v(s)&#123; s.value=s.value+1; if(s.value&lt;=0) wakeup(s);&#125; 信号量机制中，s.value&gt;0表示资源就绪，否则就表示有其他进程在使用或者请求使用资源。 进程请求资源时需要使用原语p，如果资源不空闲，则进入由于s而导致的等待队列。直到正在使用资源的进程归还资源时，使用原语v将它唤醒。 信号量机制相比于加锁机制的优势： 信号量机制把等待资源的进程设置为阻塞状态，而不是忙等待，提高了处理器的利用率。 信号量机制中由操作系统原语wakeup唤醒进程，可以采取一定策略，比如短作业优先，而不是像加锁机制一样，抢先者得到处理器，这样可以避免饥饿现象的发生。 13、相关进程间的制约关系–直接制约直接制约关系：由于进程协作关系造成的制约。 某任务需要通过循环使用read、move、write，3个操作将源磁盘上的数据文件，复制到目标磁盘。一次操作处理一个文件。 如果这3个操作在同一个进程中顺序执行，则当进行IO操作read,write的时候，进程就处在阻塞状态了。 如果这3个操作，分别建立进程，并发执行，则会节约运行时间。 然后，由于并发执行的时候这3个操作执行顺序的随机性，可能会造成重复拷贝文件或者遗漏文件的错误。 1、直接制约与同步关系 对于一组存在直接制约关系的并发进程，为了避免制约关系导致的运行错误，应该让各个进程中代码的运行顺序服从其内在要求，需要建立进程间单向或者相互的依赖关系。 如果一组进程中的每一个进程都要与其他至少一个进程建立了单向或者相互的依赖关系，则把这组进程叫同步进程，这组进程之间建立了同步关系。 2、同步关系的实现–信号量机制 比如有代码段L1对L2有依赖，可以将L2的一次执行看做一种资源的归还，而L1的执行，需要申请使用这种资源。 于是，可以类似用信号量机制来实现同步关系：在L1执行之前先执行原语p，而在L2执行后执行原语v。 与互斥关系不同的是，L1使用资源后并不归还。 3、文件拷贝问题–同步关系的实现 read中的代码段L1 L1将文件src的内容存到缓冲区buf1 然后Move中的代码段L2 L2读出buf1中的内容 然后Move中的代码段L3，再把读出的内容写到buf2 最后write中的代码段L4，再从buf2中读的内容写到别的文件 初始化的时候：先S1=1,S2=0,S3=0,S4=0，P他们刚开始只能S1进行使用处理器资源，其他的都加入到阻塞队列了。 首先Read进程是p(S1),因为S1初始化必须为1，P操作，先是做减1操作，然后再做判断S1是否小于0，如果小于0，先加入到阻塞队列。 为了顺序执行，我们先把S1初始化为1，刚开始S1是1，减1操作后，S1变为0了。此时就是Read进程的L1代码段在使用处理器资源。 当L1的代码段执行完后，需要进行Move了，此时也就是L1已经让出处理器资源，则需要先把S2给唤醒，需要V以下S2。唤醒S2之后S2做加1操作，然后再P(S2)就得到了处理器资源，然后等到P(S2)减1操作，执行完后，等同理。 14、进程同步总结 控制并发进程的互斥，同步关系，保证他们能够正确执行，称作进程同步，实现集成同步的方法叫做进程同步机制。 操作系统需要为程序提供进程同步机制，而当需要处理互斥/同步关系时才决定如何使用这些机制。 用于实现互斥关系，针对间接制约的进程同步关系：加锁机制、信号量机制 用于实现同步关系，针对直接制约的进程同步关系：信号量机制。 除了加锁机制、信号量机制之外，常用的进程同步机制还有：标志位机制，管程机制等。 15、生产者、消费者问题用两个进程Pro和Con分别表示生产者和消费者，用数组B[]表示最大库存为k的仓库， 1234567891011121314151617slot=k;//正信号量表示还有多少个空位item=0;//需要消费的Pro()&#123; 生产货物; p(slot); 查找B中任意空位in; 将货物放入B[in]; v(item);&#125;Con()&#123; p(item); 查找B中任意存有货物的位置out; 取出获取B[out]; v(slot); 消费货物；&#125; 16、从信号量到进程通信 进程的独立性：不同进程无法互相访问代码和数据。 进程需要相互写作必须能够通信（也就是共享受）。 信号量就是一种常见的共享受–低级通信 信号量数据结构的存放位置并不是yoghurt进程的私有空间，而是内核空间，信号量使用前需要向内核申请，用来获得信号量标识。 1、共享存储区通信 共享存储区通信：进程根据需要向内核申请位于内核空间的共享存储区，并将此区域映射到发送进程与接收进程的地址空间，使得发送，接收进程可以通过共享存储区进行通信。 共享存储区中的数据存取没有固定模式，需要注意根据实际情况处理同步/互斥关系。 2、信箱通信 信箱通信：在内核空间中建立专门区域作为信箱，信箱划分有限个信格，每个信格可存放一条信息（信件）。每个信箱可由多个发送进程与接收进程共享 ，发送进程用send操作将信息放入信格中，接收进程用receive操作从信格中取出信息。 与共享存储区通信相比，信箱通信的数据存取模式固定，其实现是一个多生产者消费者的PC问题。 3、消息缓冲通信 消息缓冲通信：内核空间中为每个进程配备一个消息队列MQ，并记录在进程的PCB中。发送进程通过send操作将信息封到一个消息缓冲区中，并追加到接收进程的消息队列中。接收进程通过receive操作收取自身消息队中的信息。 消息缓冲区包含： 发送进程的标识pid 正文大小size 正文data 向下指针next 与信箱通信比较，消息缓冲通信中的消息队列属于进程。消息缓冲通信是一种直接通信，当接收进程没有开启的时候，通信不成功。 4、管道通信 管道通信：通信双方共享一个文件，发送进程向文件中写入数据，接收进程从中读出数据，从而实现通信。共享的文件叫做管道。 管道通信中的共享数据并不是存放在内核空间中，而是存放在管道(文件)中，需要注意维护数据的安全问题。 管道通信中的管道文件，和共享存储区类似，并没有固定数据存取模式，所以需要注意根据实际情况处理互斥同步关系。 管道通信可以使用远程文件作为管道，所以，这样通信方式不局限于单机系统。 17、线程把进程细化成若干个可以独立运行的实体，每一个实体叫做一个线程Thread。 线程和进程的比较 对比项目 进程 线程 独立拥有的资源 处理器控制权和其他资源 处理器控制权 内存访问 不同享 同进程内共享 切换时的开销 大 小 动态性，并发性，处理同步/互斥 二者相同 二者相同 引入线程减小了系统的基本工作单位粒度，目的是： 实现进程内部的并发执行，提供并行程度 减少处理器切换带来的开销 简化进程通信方式 1、线程的控制与类型对线程实施管理，控制的模块称作线程包 根据线程包的实现方式，将线程划分为： 用户级线程：由用户态的线程包来管控，而内核中不识别这种线程 优点：切换时不需要进入到内核 缺点：某一个线程在内核上阻塞的时候，会导致全部进程阻塞。 系统级线程：使用内核提供的线程包来管控 优点：某线程在内核上阻塞时，同进程的其他线程仍然可以运行。 缺点：切换时不需要进入内核。 2、进程的常用细化方法将进程细化为线程需要根据实际应用的需要，主要的细化方法有： 分派/处理模型（Dispatcher/Worker Model） 由一个分派线程Dispatcher负责接收工作需求，然后将工作任务分派给工作线程Worker来完成动作。 队列模型Team Model 将进程细化为具有独立关系的线程，各自完成独立的任务 管道模型Pipeline Model 将完整进程中有序的各执行步骤作为线程，依次顺序运行（流水线） 三、调度调度Scheduling是管理的一种方法，一种决策。目的是通过一种合理的，有效的安排方式，提供资源(比如工作，人力 ，车辆等)的利用率。 操作系统中关注的调度问题有4种： 作业调度(宏观)：考虑何时执行哪些作业(比如Hive的数据抽取脚本) 进程调度(微观)：考虑将哪个就绪进程转换为运行状态 交换调度(中级)：考虑将进程如何在内/外存储器之间交换 设备调度：考虑让哪个进程唤醒使用设备。 1、调度的性能指标评价调度的效果，常使用一组性能指标： 1、周转时间和平均周转时间从处理过程的完成时间来进行评价(批处理系统)，处理过程Ji的周转时间；Ti=Ji的完成时刻-Ji的提交时刻 一批(n个)处理过程的平均周转时间：T=∑Ti/n 2、带权周转时间周转时间并不直接体现某处理过程耗时的合理性。 处理过程Ji的带权周转时间：Twi=Ti/Tri 一批(n个)处理过程的平均带权周转时间：Tw=∑Twi/n 3、响应时间从处理过程的响应速度来进行评价，常用在分时系统，实时系统 处理过程Ji的响应时间： Ri=Ji第一次对用户做出响应的时刻-Ji的提交时刻 4、其他性能指标 公平合理：安排公平，用户满意 资源利用率(并行度)：资源闲置的情况 吞吐量：单位时间完成处理过程的数量 2、作业调度算法1、先到先服务FCFS(First Come First Served)思想：排队 特点： 公平合理 算法简单，容易实现 服务质量欠佳(不利于短作业) 2、短作业优先SJF(Shortest Job First)思想：减少共同等待时间 特点： 实现最小的平均周转时间 吞吐量大 存在饥饿现象 算法简单，但实现困难 3、高响应比优先HRN(Highest Response-ratio Next)思想：通过考虑作业以等待的时间解决SJF的饥饿问题 响应比R=作业等待时间/作业大小 作业等待时间=系统当前时间-作业提交时刻 3、进程调度方式进程调度：在采用并发执行方式的系统中，决定正在运行的进程何时将处理器使用权交给就绪队列中的哪个进程。 进程调度方式指操作系统何时实行调度，也就是处理器使用权何时可能切换，可分为： 非抢占式：除非进程由于自身原因或者外部(非操作系统的)原因交出处理器使用权，否则一直运行直到结束，非剥夺的 抢占式：操作系统定期检查系统状态(定期调度)，按照某种原则主动将进程暂停并将处理器使用权交给下个进程。剥夺式 4、进程调度算法进程调度算法考虑调度进行时的行为，也就是如何判断是否真正发生处理器使用权切换，以及切换时选择哪个进程。 1、先来先服务FCFS 调度时不修改就绪队列，并且总是选择队首。 FCFS基于非抢占调度方式 FCFS进程调度的优缺点类似FCFS的作业调度。（公平，容易实现，服务质量欠佳） 2、时间片轮转RR（Round-Robin） RR基于抢占调度方式，它的实现依赖与计时器中断。 进程每次被选中时分配一个时间片，调度时判断其时间片是否用尽，如果用尽，则将其移到就绪队列末尾，否则不修改就绪队列。并且总是选择队首。 时间片长度为T，如果就绪队列中已经存在了n个进程，则提交新进程的最长响应时间：R=Txn 应用RR算法的关键是根据实际需要确定T的值： T太长则响应时间太长 T太短则进程切换太频繁，系统开销加大。 3、优先级算法Priority 为每个进程赋予一个优先数，调度时总是选择优先数最高的进程。 可基于抢占式调度，也可以基于非抢占式调度： 抢占式调度：可以保证总是执行高优先级进程 非抢占式调度：仅能保证当前一个进程阻塞或者结束时总能选择优先级最高的进程。 优先级调度的关键是进程优先数的配置。按照优先数配置的特点，优先级算法分为： 静态优先级算法：进程优先数在进程创建时确定，并不可改变。 动态优先级算法：进程优先数在创建进程时被赋初始值，运行中可以根据系统状态改变。 通过不同的优先数配置策略，优先级算法可以灵活的体现各种算法思想FCFS，SJF 经验：IO繁忙型的进程应该被赋予高优先级用来提供系统的并行度。 5、实时系统的进程调度实时系统需要对周期性或者非周期性发生的时间作出处理，其对处理正确性和时效性要求十分苛刻。 不同类型的时间对于时效性的要求各异，可表达为以下时间参数： 就绪时限：事件发生时到相应处理进程被创建的时间长度要求。 开始时限：事件发生时到相应处理进程第一次运行的时间长度要求 完成时限：事件发生时到相应处理进程完成的时间长度要求。 处理时间：某些特殊事件要求定期触发处理过程。 5、可调度以及条件 对于一组事件，如果存在某种处理方式，让其中每个事件的时限要求都可以得到满足，就说明这组事件是可调度的Schedulable。否则，如果无论用何种方式，都至少有一个事件无法满足时限要求，则说明这组事件是不可调度的。 可调度的一个必要条件：u&gt;r，u表示系统单位时间内可以处理的事件数（处理能力），r表示单位事件内到达的事件数。 四、死锁一组进程都处于阻塞状态，其中的任一进程阻塞状态的解除都依赖与另一进程的后续操作 。这种系统状态叫做死锁DeadLock。 1、死锁的分类： 资源死锁：由于相互等待临界资源而导致的死锁 通信死锁：由于相互等待通信导致的死锁 控制死锁：由于系统或者用户的特殊控制导致的死锁 2、死锁的特点 偶发性：由于特殊的并发执行顺序导致，是小概率事件 非消耗：死锁的相关进程处于阻塞状态，不占用处理器 程序无关：并非个别程序设计的错误，而是系统运行的错误。 3、死锁的原因资源死锁的根本原因是可用资源的数量小于所需资源的数量。 但不可能靠无限制的增加资源数量来根本性的解决死锁问题。 死锁的发生的必要条件： 互斥条件：死锁的相关进程都是互斥的 不剥夺条件：当进程申请资源时不能剥夺其他进程正在使用的资源 请求与保持(部分分配)条件：进程在申请资源不成功的时候仍然保持已经申请的资源。 环路等待条件：进程相互等待关系形成环路。 4、死锁的预防通过在资源分配的时候采取一系列的限制措施，来破坏4个必要条件之一，就可以避免死锁的发生。 互斥条件的破坏 方法：不允许申请临界资源 缺点：临界资源需要转化为非临界资源才能被申请和使用，大部分情况下无法实现。 不剥夺条件的破坏 方法：允许进程申请资源时剥夺其他进程正在使用的资源 缺点：被剥夺资源的进程不可预期的打断，系统需要处理回退。缺乏公平性，反复剥夺与被剥夺 请求与保持条件的破坏 方法1：资源暂时归还 进程申请资源不成功时，暂时归还已经分配的资源。 缺点：系统需要处理回退，反复申请与归还。 方法2：静态分配 进程需要一次性申请完所有可能使用的资源 缺点：难以预知需要使用的资源，降低资源利用率。 环路等待条件的破坏 方法1：单请求方式 资源申请不允许嵌套，但可以一次申请多个，归还旧的资源后方可以申请新资源。 缺点：同时需要多个资源时只能实行小范围内的静态分配。 方法2：按序分配 对资源编号，进程只允许申请比当前已拥有资源编号更大的资源。 缺点：如果使用大编号资源的段内需要使用小编号资源，则只能实行小范围内的静态分配。而且，编号管理也困难。 5、死锁的经典问题哲学家用餐问题 6、死锁的避免进程可以自由的动态的，申请资源，但是分配资源的时候对可能发生的死锁，进行预测，来决定是否立即分配。 死锁的避免具有以下特点： 原理上是通过预测潜在的死锁 不影响程序设计 临界资源分配并非空闲让进 7、银行家算法 初始化，计算资源可用量Avaiable和当前需求矩阵Need， Available=Total-∑Used Need=Max-Used 合法检查，检查资源申请与需求矩阵的相容性，未通过则报错，Request&lt;=Need 资源检查，检查请求的资源是否可用，未通过则阻塞进程P。Request&lt;=Avaiable 预分配，嘉定Request得到满足 Available=Available-Request Used=Used+Request Need=Need-Request 安全状态检查，检查预分配后的状态是否为安全状态。 某系统有4类资源A,B,C,D，数量分别是8,10,9,12。当前有5个进程P1，P2，P3，P4，P5。最大需求矩阵Max和当前分配矩阵Used如下：$$Max = \\left[ \\matrix{ 4 &amp; 6 &amp; 3 &amp; 8\\ 3 &amp; 3 &amp; 5 &amp; 2\\ 6 &amp; 6 &amp; 0 &amp; 9\\ 3 &amp; 4 &amp; 8 &amp; 5\\ 4 &amp; 3 &amp; 2 &amp; 2 } \\right]$$ $$Used = \\left[ \\matrix{ 1 &amp; 1 &amp; 0 &amp; 2\\ 2 &amp; 2 &amp; 4 &amp; 0\\ 0 &amp; 5 &amp; 0 &amp; 1\\ 1 &amp; 1 &amp; 1 &amp; 5\\ 2 &amp; 0 &amp; 2 &amp; 2 } \\right]$$ 在当前状态下，如果进程P1申请request=(1,0,1,0)，系统能否分配？ 资源总量(8,10,9,12) 进程 Max Used Need Avaiable Finished A B C D A B C D A B C D 1 1 1 2 p1 4 6 3 8 1 1 0 2 p2 3 3 5 2 2 2 4 0 p3 6 6 0 9 0 5 0 1 p4 3 4 8 5 1 1 1 5 p5 4 3 2 2 2 0 2 2 在当前状态下，如果进程P3申请request=(1,0,0,1),系统能否分配？ 五、存储管理1、存储器的类型 外存储器、磁带，离线存储，容量大，速度慢 外存储器，磁盘，光盘等，主板外 cache主存储器，主板类（存储器管理） 寄存器cache，cpu内，速度块，容量小 2、程序的编码与装入 程序（高级语言） 123456int a=10;main()&#123; int b=a; cout&lt;&lt;b&lt;&lt;a; cout&lt;\"hello\";&#125; 编译链接 程序（机器码） 123450~1 102~8 hello.9 ???? (将[0~1]压栈)10~13 ???? (cout&lt;&lt;[esp]&lt;&lt;[0~1];)14~17 ???? 装入(count&lt;&lt;[2~8];) 运行时内存 数据段：全局变量和敞亮 代码段：指令存放处 堆栈：局部变量存放处（由指令动态操作） 3、逻辑地址与物理地址​ 程序在运行的时候被装入到内存中，其中的指令与数据的位置用地址值表示，地址值的变现形式可分为： 逻辑（虚拟）地址：指令或者数据在程序中的编号（位置） 物理地址：指令或者数据内存中的位置 物理地址 逻辑地址 程序（机器码） 10000~10001 0~1 10 10002~10008 2~8 hello 10009 9 (将[0~1]压栈；) 10010~10013 10~13 （cout&lt;&lt;[esp]&lt;&lt;[0~1];） 10014~10017 14~17 (cout&lt;&lt;[2~8];) 4、重定位（地址转换）​ 若程序中的变量与指令用逻辑地址加以引用，则为了在运行的时候正确访问这些变量或者指令，需要进行重定位（或者叫地址转换），来确定实际的物理地址。 ​ 根据地址转换操作的时机，重定位分为： 静态重定位：操作系统在装入程序时候修改其中所引用的逻辑地址值为物理地址值–由操作系统实现。 动态重定位：在运行过程中需要做访问操作的时候才进行地址转换–由cpu的存储管理单元MMU自动实现，操作系统只登记MMU进行重定位时需要的相关信息。 个人理解，Java的反射机制，可以在运行的时候，获取Class信息，想必底层实现原理就是利用的动态重定位吧。 5、静态重定位与动态重定位的特点对比： 静态重定位 动态重定位 硬件支持 不需要 需要 操作系统的工作 装入时修改程序 登记MMU所需要信息 程序装入 被修改 直接装入 程序运行时移动 困难 容易 程序的不连续装入 困难 容易 其他技术支持（动态链接、虚拟存储等） 难以实现 容易实现 6、存储器管理的关键问题​ 存储器管理主要是对用户空间进行管理，目的是为了提供主存储器的利用率，方便用户对主存储器的使用。 ​ 存储器管理的关键问题： 1、存储空间的分配与回收 设计合理的数据结构以记录存储空间的分配情况 设计合理的算法来提高存储器的利用率（装入尽可能多的程序） 2、重定位方式的确定 决定采用静态重定位或者动态重定位，并进行相应的工作 3、实现保护与共享 存储空间保护使得各进程不能访问彼此的存储空间，主要方法有： 利用出其里的特权级，防止跨级访问 利用MMU的界限寄存器规则，防止同级访问 操作系统设置和检查存储区域的保护键与当前进程的键以及操作是否匹配 存储空间共享： 提供进程间共享数据的途径 4、实现虚拟存储技术7、存储器管理的方法1、从程序的角度分为： 非分段管理：将进程的数据、代码、堆栈作为一个完整的逻辑空间 分段管理：将进程的数据、代码、堆栈、各自作为独立的逻辑空间 2、从内存分配的角度分为： 分区管理：一个逻辑空间对应物理空间中一个分区，连续分配 分页管理：一个逻辑空间对应物理空间中多个分页，可不连续分配。 段页式管理：比较先进复杂的综合方法。 8、单一连续区存储管理​ 操作系统占用内核空间之外，将整个用户空间看做一个内存区域，一次只能装入一道用户程序，只有用户区中的程序运行完毕方可以装入下一道程序。 也就是说，一个内存，有系统区，还有用户区。系统区，存储一个操作系统信息。 常用静态重定位 管理简单 不需要复杂硬件支持 只允许单任务 为了实现单一连续区存储管理，或者其他任何存储管理的方法，操作系统采用的请求表来记录请求装入内存的程序信息。 程序标识 需要的空间长度（虚拟地址空间大小） A 50K B 90K C 130K D 10K E 165K 9、固定分区存储管理​ 操作系统占用内核空间之外，将整个用户空间划分为若干个固定大小的区域（大小可不相等）。多道程序分别装入不同的分区内，一道程序只能装入一个分区，而一个分区也只能分配给一道程序。 ​ 为了实现固定分区存储管理，操作系统必须记录各分区的位置和使用情况，采用关键数据结构–分区说明表（DPT） 区号 长度 起始地址 状态 1 75K 32K 0 2 30K 107K 0 3 140K 137K 0 4 11K 277K 0 ​ 固定分区存储管理一般采用静态重定位，配合界限寄存器法防止各进程彼此访问存储区域（存储保护）。 ​ 利用MMU的界限寄存器规则，防止同级间房屋内，操作系统设置和检查存储区域的保护建与当前进程的键是否匹配。 ​ 固定分区存储管理的特点： 支持多道程序设计 并发执行进程数受到分区数限制 程序可用空间受分区大小限制 存储区存在“碎片” 长作业优先、输入队列法 10、可变分区存储管理​ 在程序装入时选择一个空闲区域并在其中动态创建一个分区来装入程序。 ​ 为了实现可变分区存储管理，操作系统需要记录空闲内存区域的位置，可采用以下两种数据结构： 1、可用表类似分区说明表DPT，但是其中的记录是变化的，并且状态1表示空闲，而状态0表示记录无效（预留着存储位置） 起始地址 长度 状态 152K 10K 1 172K 116K 1 0 0 0 缺点：可用表的长度受到了限制，从而导致表示的空闲区个数受到限制，影响并发的进程个数 2、空闲区链表用链表的形式来记录空闲区 ps：怪不得面试的时候，让手写LRU算法，而LRU算法也是用链表实现的。 12345struct FreeNode&#123; long start;//分区的起始地址 long length;//分区的长度 struct FreeNode *next;//向下指针&#125; 3、可变分区存储管理空间分配算法当程序请求装入的时候，进行空间分配的算法： 最先适应法FF：依次查找空闲区，选择收个足够大的空闲区装入程序。 最佳适应法BF：在所有足够装入程序的空闲区中选择最小者 最坏适应法WF就：在所有足够装入程序的空闲区中选择最大者 4、空间分配算法优缺点：三种算法的优势和缺点： 最先适应法FF：分配速度最快，但是堆内存使用没有规划。 最佳适应法BF：找到的空闲区与请求最匹配，对大程序的装入有充分的准备，但是容易产生外碎片。 最坏适应法WF：不容易产生外碎片，但是容易造成大程序装入的失败。","link":"","tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://victorblog.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}]},{"title":"计算机网络","date":"2017-06-11T02:06:00.000Z","path":"2017/06/11/计算机网络/","text":"一、计算机网络1、计算机网络的常识 分布式处理：计算机网络中的某个计算机系统负荷过重，可以将它处理的某个复杂任务分配给网络中的其他计算机系统，来利用空闲计算机资源来提高整个系统的利用率。 负载均衡：将工作任务均衡的分配给计算机网络中的各台计算机 广域网（WAN）：长距离通信，是因特网的核心，连接广域网的各个结点的交换机的链路。 局域网（LAN）：局域网使用广播技术，广域网使用交换技术。 广播式网络：所有联网的计算机共享一个公共通信信道。当有一台计算利用共享信道发送报文分组，所有其他的计算都会接收到这个分组。 点对点网络：每条物理线路连接一对计算机。乳沟两台通信计算机直接没哟直连线路，它们之间的报文分组通信就需要中间结点的接收，存储和转发。 使用采用分组存储转发与路由选择机制是点对点与广播式网路的重要区别。 广域网基本都是点对点网络。，广域网中的无线、卫星通信网络采用的是广播的通信方式。 2、计算机网络性能指标 带宽（BandWidth）：表示网络的通信线路传送数据能力，单位b/s，比特每秒 时延（Delay）：表示数据（一个报文、分组）从网络的一端传到另一端所需要的总时间。 发送时延=分组长度/信道宽度 传播时延=信道长度/传播速度 处理时延 排队时延 注：一般做题，排队时延/处理时延可忽略。提高数据的发送速度只是为了减少数据的发送时延 往返时延（RTT）：从发送端发送数据开始，到发送端收到来自接收端确认总共的时延。 吞吐量（ThroughPut）：单位时间里通过网络（信道，接口）的数据量。受到网络的带宽和额定速率限制。 速率：连接在计算机网络上的主机在数字信道上传送数据的速率，也叫比特率b/s,bit/s,bps，Kb/s,Mb/s,Gb/s（G=10^9），一般我们把速率叫带宽。 3、网络资源网络资源包括：硬件资源、软件资源、数据资源 4、通信子网和资源子网 资源子网：负责全网的数据处理业务，负责向网络用户提供各种网络资源和网络服务 计算机系统 终端 联网外部设备 软件资源和信息资源 通信子网：完成网络数据传输，转发的通信处理任务 通信控制处理机 通信线路 通信设备：网桥、交换机、路由器 注：通信子网对应OSI的下3层：物理层、数据链路层、网络层 5、分组交换分组交换：把数据分成大小相当的小数据片，每个片都要加上控制信息（比如目的地址），因此传送数据的总开销比较大。分组交换信道利用率高 传播时延=信道距离/传播速度 6、计算机网络与分布式计算机系统区别 分布式系统：整个系统中的各个计算机对用户都是透明的。用户通过输入命令就可以运行程序，但是用户并不知道哪一台计算机在为他运行程序。是操作系统为用户选择一台最合适的计算机来运行其程序，并将运行的结果传送到合适的地方。 计算机网络：用户必须在要运行程序的计算机上先登录，然后按照计算机的地址，将程序通过计算机网络传送到计算机上运行。最后，根据用户的命令将结果传送到指定的计算机。 二、物理层1、通信双方的交互方式 单工通信：只有一个方向的通信而没有反方向的交互，只需要一条信道。例：无线电广播、电视广播 半双工通信：通信的双方都可以发送或接收信息，但是任何一方都不能同时发送和接收。需要两条信道。 全双工通信：通信双方可以同时发送和接收信息，也需要两条信道。 2、电路交换、报文交换、分组交换 电路交换：在进行数据传输前，两个节点之间必须建立一条专用的物理通信路径，建立连接，数据传输，释放连接。 在数据传输过程中，用户始终占用端到端的固定传输带宽。 报文交换：数据交换的单位是报文，报文携带目标地址、源地址等信息。报文交换采用是存储转发的传输方式。 报文交换主要用在早期电报通信网，现在很少使用。 分组交换：和报文交换一样，页才用了存储转发方式。解决了报文交换中大报文的传输问题。分组交换限制了每次传输的数据块大小上限，把大的数据块划分成了合理的小数据块，再加上源地址，目标地址，编号信息等。构成分组Packet。 3、物理层设备 中继器 也叫转发器，将信号整形并放大再转发出去，来消除信号由于经过一长段电缆，因为噪声或者其他因素导致信号失真和衰减。 注：放大器和中继器的区别 放大器放大的是模拟信号，是将衰减信号放大。 中继器放大的是数字信号，是将衰减的信号整形再生。 中继器没有存储和转发的功能，所以是不能连接两个速度不同的网段，中继器两端的网段一定是同一个协议。 集线器Hub 是一个多端口的中继器，工作物理层。Hub工作时，当一个端口接收到数字信号后，由于信号从端口到Hub的传输过程中有了衰减，Hub是将信号进行整形放大。 Hub在网络中只起到信号放大和转发作用，只是为了扩大网络的传输范围，不具备信号的定向传送能力。只是一个标准的共享式设备。","link":"","tags":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://victorblog.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}]},{"title":"Kafka的应用","date":"2017-03-04T02:44:34.000Z","path":"2017/03/04/Kafka的应用/","text":"一、Kafka的应用1、Spark/Spark Streaming 实时计算充当消费者，消费Kafka的消息。也就是说Spark大约有90%的数据会来自kafka。 2、kafka有两种消息发送模式2.1、点对点（一对一）模式。只能一个生产者，一个消费者这样，消费者消费一条数据，然后表示接收到了，然后再回复一个确认收到，则消费一条数据，消息队列就删掉一条数据。如果想要多个消费者消费消息，就不行了。 2.2、发布订阅（publish/subscribe ）​ 发布订阅有两种，一种是生产者把消息发送到队列里，然后主动推送消息给消费者，有点类似你订阅的微信公众号，只要有新的消息就会推送给消费者。但是这种机制有一个缺点就是，比如你生产者生产消息的速度是100M/s，但是，你多个消费者可能处理消息的速度都不一样，有的50M/s，有的200M/s，而直接推送消息这种方式，处理速度慢的消费者就会直接崩掉。而处理速度快的又会造成很大的资源浪费。第二种是消费者主动拉取数据，会根据自身数据处理速度拉取。但是有问题，消费者要隔一段时间的轮询消息队列，看有没有新的消息产生。假如某种情况下，这样也容易造成一些资源浪费。kafka的发布订阅方式是消费者主动拉取数据。 3、Kafka中的相关定义3.1、Offset​ 作用就是记录消费位，比如你消费到第6条，broker挂了，它会保存记录到zookeeper 里，kafka的0.9版本之前都是保存在zk中，之后就直接保存在kafka里也就是本地存储。不管你保存在zk也好，也是本地kafka集群也好，offset都是用来管理消息消费偏移量的为什么要改保存到kafka集群本地呢？你zk存的好好的，首先消费者本身跟kafka直接连接的，我在kafka里面获取数据的同时，还要维护跟zk的连接，首先消费者是以拉取的方式获取消息的，本身拉取速度是非常快的，可能你消费者一秒要拉取消息好几次，还要跟zk这边打交道好几次，这样太频繁了，就会影响效率，而zk本身是来提供各大框架之间润滑技的作用，那你此时这么高并发的请求，存到zk里面也不好，后来把offset存到本地，这个本地不是本地磁盘，而是存到kafka本地。kafka存数据是存在topic 里面，kafka存消息是存到磁盘上的，默认保留七天，配置文件里写的是168个小时 3.2、topic​ 看成是一个消息队列，因为生产者消费者面向的都是一个topic。 partition分区 为了实现扩展性，一个非常大的topic主题，会分布到多个broker（服务器）上。一个topic可以分为多个分区partition，而每个partition是一个有序队列。 replication是副本 为保证集群中，当某个节点发生故障时，保证节点上的数据不丢失，而且kafka还能继续工作，kafka提供了副本机制，也就是一个topic的每一个分区，都有许多个副本。也就是一个leader和许多个follower。当然follower肯定不能再同一个broker，不然副本毫无意义。 leader副本 leader是每个分区多个副本的主，生产者发送数据的对象，以及消费者消费数据的对象都是leader。每个分区多个副本的从，实时的从leader主副本同步数据，保持和leader数据同步，当某个leader分区挂了，某一个follwer从副本会成为新的leader分区启动kafka之前要先启动zk，因为kafka依赖于zk 消费者组consumer group 也就是同一个消费者组里面的消费者，消费topic不同分区，也就是不能消费同一个分区的数据 3.3、关于leader何时向follwer发送ack​ 有两种想法，一是，半数以上的follwer同步完成数据，leader发送ack，但是这种解决方案，至少需要2n+1台服务器。容易造成资源浪费，还有很多重复的副本。第二种解决方案是，所有的服务器都同步完成，也就是总共有n+1台服务器，但是这样仍然有一个问题，就是，假如同步过程中，有一个follwer可能由于网络问题，长时间的没有和leader同步数据，这样就要一直等下去，容易造成资源浪费，因此就出现了ISR机制。kafka使用的是第二种方案​ leader维护一个ISR（in sync replication set）叫做和follwer保持同步的集合，当ISR中的follwer和leader同步数据完成后，leader就会发出ack给follower。如果follwer长时间没有向leader同步数据，则这个follwer就会被踢出ISR，replica.lag.time.max.ms这个参数决定follwer的这个长时间的最大限度（默认值是10000ms，也就是10s之内同步完，否则被踢出isr）。如果leader挂了，就会选择follwer作为新的leader。选举follwer作为新的leader的时候，是看消息条数最多的，作为新的。因为数据越多的丢数据越少。 ack的应答机制 对某些不重要的数据，可靠性要求不是很高的数据，能够容忍少量丢失数据。所以，没必要等ISR中的的follwer都同步完数据。Kafka提供了3种可靠性级别，可以让用户使用的时候，对可靠性和延迟进行权衡。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"Kafka的总结","date":"2017-03-04T02:06:00.000Z","path":"2017/03/04/Kafka总结/","text":"1、Kafka有哪些特点？ 高吞吐量、低延迟：Kafka每秒可以处理几十万条消息，它的延迟最低只有几ms，每个topic可以分多个partition，Consumer group对partition进行consume操作。 可扩展性：Kafka集群支持热扩展 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失。 容错性：允许集群中节点失败，加入副本数为n，则允许n-1个节点失败 高并发：支持数千个客户端同时读写 2、哪些场景下会选择Kafka？ 日志收集：公司用Kafka手机各种服务的log，通过Kafka以统一接口服务的方式开放给各种consumer，比如hadoop，HBase，Spark，Flink等 消息系统：解耦生产者和消费者，缓存消息 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，比如浏览网页，搜索，点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者落到HDFS，数据仓库中做离线分析和挖掘。 运营指标：Kafka经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 流式处理：spark streaming和Flink 3、Kafka分区的目的？partition分区对于kafka集群的好处是：实现负载均衡。分区对于消费者来说，可以提高并发度，提高效率。 4、Kafka是如何做到消息的有序性？kafka中的每个partition中的消息在写入的时候都是有序的，而且单独一个partition只能由一个消费者去消费。可以在里面保证消息的顺序性，但是分区之间的消息不保证有序的。 5、Kafka是如何保证数据可靠性和一致性？5.1、数据可靠性1、Topic分区副本​ 在kafka0.8.0之前，kafka是没有副本partition的概念的，那时人们只会用Kafka存储一些不重要的数据，因为没有副本，数据可能会丢失。但是随着业务发展，支持副本的功能越来越强大，为了保证数据的可靠性，kafka从0.8.0版本开始引入了分区副本。也就是每个分区可以人为的配置几个副本，比如在创建主题的时候指定replication-factor，也可以在Broker级别进行配置default.replication.factor，一般会设置为3个副本。 ​ kafka可以保证单个分区里的消息是有序的，分袂可以在线可用，可以离线不可用。在多个分区副本里面有一个副本是leader，其余的副本是follower，所有的读写操作都是经过leader进行的，同时follower会定期的去leader上复制数据。当leader挂了，其中一个follower会重新成为新的leader，通过分区副本，引入了数据冗余，同时提供了kafka的数据可靠性。 ​ kafka的分区副本架构是kafka可靠性保证的核心，把消息写入多个副本可以让kafka在发生崩溃的时候仍能保证消息的持久性。 2、Producer往Broker发送消息​ 如果我们要往kafka对应的topic发送消息，我们需要通过Producer完成，kafka主题对应了多个分区partition，每个分区下面又对应了多个副本。为了让用户设置数据可靠性，Kafka在Producer里面提供了消息确认机制。也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功。可以在定义Producer时通过acks参数指定。这个acks参数支持3个值： acks=0：表示如果生产者能够通过网络把消息发送出去，那么就任务消息已经成功写入kafka。这种情况下还是有可能发生错误，比如发送的对象不能被序列化或者网卡发生故障，但如果是分区离线或者整个集群长时间不可用，那么就不会收到任何错误。在acks=0模式下的运行速度是非常快的，很多基准测试都是这个模式，你可以得到惊人的吞吐量和带宽利用率，不过这种模式，一定会丢失一些消息。 acks=1:意味着如果leader在收到消息并把消息写到分区数据文件，并不一定同步到磁盘上时，会返回确认或者错误响应。这个模式下，如果发生正常的leader选举，生产者会在选举时收到一个LeaderNotAvaiableException异常，如果生产者能恰当的处理这个错误，它会重试发送消息，最终消息会安全到达新的Leader里。不过在这个模式下仍然可能丢失数据，比如消息已经成功写入Leader，但是在消息被赋值到follower副本之前Leader发生崩溃。 acks=all:这个和旧版本参数request.required.acks=-1的意义一样：意味着Leader在返回确认或者错误响应之前，会等待所有同步副本都受到消息，如果和min.insync.replicas参数结合起来。就可以决定在返回确认前至少有多少个副本能够收到消息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。 Producer发送消息还可以选择同步，producer.type=sync默认是同步。也可以配置成异步producer..type=async模式。如果设置成异步，虽然会极大的提高消息发送性能，但是这样会增加丢数据的风险。如果需要确保消息的可靠性，必须将producer.type=sync。 3、Leader选举​ ISR（in-sync replicas）列表。每个分区的leader会维护一个ISR列表，ISR列表里面就是follower副本的Broker编号，之后跟的上Leader的follower副本才能加入到ISR里面，这个是通过replica.lag.time.max.ms参数配置的，只有ISR里的成员才有被选举为Leader的可能。 ​ 当Leader 挂了，而且unclean.leader.election.enable=false情况下，kafka会从ISR列表中选择第一个follower作为新的leader，因为这个分区拥有最新的已经commit的消息，通过这个可以保证赢commit的消息的数据可靠性。 ​ 为了保证数据的可靠性，至少需要配置以下几个参数： producer级别：acks=all(旧版本request.required.acks=-1)，同时发生模式改为同步producer.type=sync topic级别：设置replication.factor&gt;=3,并且min.insync.replicas&gt;=2 broker级别：关闭不完全的leader选举，unclean.leader.election.enable=false; 5.2、数据一致性​ 此处的数据一致性是不论新的Leader还是新选举的Leader，consumer都能督导一样的数据。 ​ 假如分区的副本partition为3，其中副本0是leader，副本1和副本2是follower，并且在ISR列表里面，虽然副本0已经写入了Message4，但是Consumer只能读到Message2。因为所有的ISR都同步了Message2，只有High Water Mark以上的消息才支持Consumer读取，而High Water Mark取决于ISR列表里面的偏移量最小的分区。类似木桶效应。 ​ 这样做的原因是还没有被足够多副本赋值的消息被认为是不安全的，如果Leader发生崩溃，另一个副本成为新Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，就可能会破坏一致性。试想，一个消费者从当前Leader也就是副本0读取并处理了Message4，这个时候Leader挂了，选举了副本1作为新的Leader，此时另一个消费者再去从新的Leader读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。 ​ 引入High Water Mark机制，会导致Broker间的消息赋值因为某些原因变慢，那么消息达到消费者的时间也会随之边长，因为我们会先等待消息复制完毕。延迟时间可以通过参数replica.lag.time.max.ms参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。 6、ISR、OSR、AR是什么？ ISR：In-Sync Replicas副本同步队列 OSR：Out-of-Sync Replicas AR：Assigned Replicas所有副本 ISR由leader维护，follower从leader同步数据有一些延迟，超过相应的阈值会把follower剔除ISR，存入到OSR列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。 7、LEO、HW、LSO、LW等分别代表什么？ LEO：是LogEndOffset的简称，代表当前日志文件中最后一条 HW：水位或者水印watermark，也叫作高水位High Watermark。通常被用在流式处理领域，比如Flink，Spark等。用来表示元素或者事件在基于事件层面上的进度。在Kafka中，水位的概念反而与时间无关，而是和位置信息有关。严格而说，它表示的位置信息，也就是位移offset，取partition对应的ISR中最小的LEO为HW，Consumer最多只能消费到HW所在的位置上一条消息。 LSO：是LastStableOffset简称，对未完成的事务而言，LSO的值等于事务中第一条消息的位置firstUnstableOffset，对已完成的事务而言，它的值和HW相同。 LW：Low Watermark低水位，代表AR集合中最小的logStartOffset值。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://victorblog.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"消息队列","date":"2017-03-01T02:06:00.000Z","path":"2017/03/01/消息队列/","text":"大数据技术之Kafka1、Kafka概述1.1、消息队列 1.2、为什么要用消息队列？ 解耦：也即是客户端A跟B不用直连，通过中间件。达到解耦 冗余：消息队列可以进行数据备份，它本身可以缓冲数据 扩展性：消息队列可以进行集群 灵活性&amp;峰值处理能力：机器扩展多了，峰值处理能力自然提高 可恢复性：因为你数据可备份的，消息丢了一个，可以恢复的 顺序保证：队列先进先出，它有顺序。 缓冲：均衡两边的速度，这边数据冗余了。可以进行备份缓冲。 异步通信：比如你A挂了，并不会影响B 1.3三种消息队列对比：1.3.1、ActiveMQ，RabbitMQ，Kafka ActiveMQ中，消费者消费完消息后，队列Queue中，这条消息立即被删除。 而ActiveMQ，它的队列是存在内存中。普通的消息队列是存在队列Queue中，然后存在内存中。Kafka是持久化到磁盘中，Kafka是以partition为单位进行消费的，而普通消息队列是一条一条进行消费的。 RabbitMQ多了一段Exchange，路由交换机。有4中工作机制。** ​ 当producer生产了3条消息，每个队列中不会都拿到这3条消息的，它是根据路由机制进行匹配的，有4中路由机制，fanout，direct，topic，headers。4种路由机制。 ​ topic是以点为分隔符进行通配，*是匹配一个单词，#是匹配多个单词。 Kafka，当消费者消费完消息后，不会被立即删除，它有避免重复消费，和避免内存溢出，Kafka是以文件的方式进行存储的，存储在磁盘中，所以永远不会内存溢出。可能会存在磁盘满了，磁盘满了，可以加集群，然后进行备份，就是partition可以备份到另一台机器中。在kafka中是以zookeeper来管理集群的。 kafka消费是以partition为单位进行消费的。所以所，kafka适合数据量更大的时候的处理。 而RabbitMQ它的消息存在内存中，如果消息超过长度，就会内存溢出。而kafka不会，消息存在磁盘中。 1.3.2、ActiveMQ，RabbitMQ，Kafka应用场景​ 或者你这个库存当时没有了，可以调用别的地方的库存，然后重新分配库存。但是kafka就没有这样的好处。 订单太多，可能会导致内存溢出，RabbitMQ中有一个partition是网络分区，用来做网络集群的，与kafka的partition不同，因此可以做集群部署。 1.4、什么是Kafka流式计算中，Kafka用来缓存数据，Storm通过消费Kafka的数据进行计算。 Apache Kafka是一个开源消息系统，由Scala写成 Kafka是由LinkedIn公司开发 Kafka是一个分布式消息队列，kafka对消息保存时根据Topic进行归类，发送消息者叫Producer 消息接收者叫Consumer。此外Kafka集群有多个Kafka实例组成，每个实例(server)叫broker。 不管是kafka集群，还是consumer都依赖与zookeeper集群保存一些meta信息，保证系统可用性。 1.5、Kafka架构 partition分区：作负载均衡用。 replication：备份。 一个消费者可以消费多个topic，同一个组里面的消费者，不能消费同一个partition分区的数据。但是你不同的组里面，还是可以消费同一个partition分区的数据。 Consumer A与Consumer B属于通过一个消费者组，Partition0被Consumer A消费，就不能被Consumer B消费。 zookeeper：Consumer的集群依赖的，kafka集群的构建依赖于zookeeper，消费者也需要，上次消费到哪了，也要给zookeeper存一份。 1.6、kafka的专业术语 massage： kafka中最基本的传递对象，有固定格式。 topic： 一类消息，如page view，click行为等。 producer： 产生信息的主体，可以是服务器日志信息等。 consumer： 消费producer产生话题消息的主体。 broker： 消息处理结点，多个broker组成kafka集群。 partition： topic的物理分组，每个partition都是一个有序队列。 segment： 多个大小相等的段组成了一个partition。 offset： 一个连续的用于定位被追加到分区的每一个消息的序列号，最大值为64位的long大小，19位数字字符长度","link":"","tags":[{"name":"分布式中间件","slug":"分布式中间件","permalink":"https://victorblog.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"什么是埋点？","date":"2017-02-04T02:06:00.000Z","path":"2017/02/04/什么是埋点？/","text":"一、数据流程 数据生产-数据采集-数据处理-数据分析和挖掘-数据驱动/用户反馈-产品优化/迭代。 数据采集，顾名思义采集相应的数据，是整个数据流的起点，采集的全不全、对不对，直接决定数据广度和质量，影响后续所有的环节。 在数据采集失效性、完整性不好的公司，经常会有业务方发现数据发生的大幅度变化，追其所以时发现是数据采集的问。而另一方面，采集什么数据才能有效的得到数据分析结论，才能有效的进行推荐，就需要提前规划埋点。 当前数据采集普遍遇到的几个问题： 实时性，对于工具性产品在无网条件下的数据，无法实时上报； 完整性，由于用户隐私协议&amp;欧盟通用数据保护条例的，部分数据无法采集； 异常，android_id、idfa、idfv 随版本升级变化或无法获取。 二、数据埋点接下来用5w2h的思路来看埋点。 1、埋点是什么？ 所谓“埋点”，是数据采集领域（尤其是用户行为数据采集领域）的术语，指的是针对特定用户行为或事件进行捕获、处理和发送的相关技术及其实施过程。比如用户某个icon点击次数、观看某个视频的时长等等。 埋点的技术实质，是先监听软件应用运行过程中的事件，当需要关注的事件发生时进行判断和捕获。 特别注意需要明确事件发生时间点、判别条件，这里如果遇到不清楚的，需要和开发沟通清楚，避免采集数据与理想存在差异。例如：期望采集某个app的某个广告的有效曝光数，有效曝光的判别条件是停留时长超过1秒且有效加载出广告内容。 2、埋点是谁的工作？ 现在公司通常都会有数据产品经理或业务线数据分析师，结合版本迭代过程进行埋点规划。如果是代码埋点，还需要开发完成相应的埋点代码。 3、 在什么时间点&amp;在哪里埋点呢？ 埋点是目的导向。 在产品规划时就要思考数据埋点问题，如果在产品外发后再考虑怎么埋点，就会导致前期版本用户的数据无法收集，想要看某个数据时就会非常无奈，只有等到新版本完善来弥补。 思考要埋哪些点、埋点的形式，需要紧密结合产品迭代的方向、运营需求，并和数据开发等进行充分沟通以确认： 埋点能够得到想要的数据解决/支持； 能够得到当前版本的复盘情况； 后续版本的数据支撑。 通常的沟通过程以 埋点文档为载体；数据埋点评审为终结。 当前版本的复盘情况： 新版本功能使用情况，是否符合预期； 新功能上线后对其他功能点的影响？是否为整体均有积极作用； 版本运营活动目标群体的特征获取; 新增商业化目标的监测…… 后续版本的数据支撑： 规划方向的用户行为分析 画像特征分析 4、怎么埋点呢？4.1 、埋点技术：代码埋点、可视化埋点、无埋点接着第一节：埋点是什么？来看下埋点技术层面的区分：代码埋点、可视化埋点和无埋点。 （1）代码埋点​ 以为需要监测网站上/app上用户的行为，是需要在网页/app中加上一些代码的，当用户触发相应行为时，进行数据上报，也就是代码埋点。这样的代码，在网站上叫监测代码，在app中叫SDK（Software Development Kit）。市场上的第三方数据采集均支持代码埋点，GA, GrowingIO，神策等。 优点：可以详细的设置某一个事件自定义属性； 缺点：时间、人力成本大，数据传输的时效性。 （2）可视化埋点​ 利用可视化交互手段，数据产品/数据分析师可以通过可视化界面（管理后台连接设备） 配置事件，如下是腾讯移动分析的可视化埋点界面。可视化埋点仍需要先配置相关事件，再采集。 优点：埋点只需业务同学接入，无需开发支持； 缺点：仅支持客户端行为。 （3）无埋点 无埋点是指开发人员集成采集 SDK 后，SDK 便直接开始捕捉和监测用户在应用里的所有行为，并全部上报，不需要开发人员添加额外代码。 数据分析师/数据产品 通过管理后台的圈选功能来选出自己关注的用户行为，并给出事件命名。之后就可以结合时间属性、用户属性、事件进行分析了。所以无埋点并不是真的不用埋点了。目前市场第三方工具GrowingIO支持无埋点全量行为数据抓取 优点： 无需开发，业务人员埋点即可； 支持先上报数据，后进行埋点。 无埋点和可视化埋点均不需要开发支持，仅数据业务同学进行设置即可。但两者数据上报-埋点设置存在加大的差异：无埋点支持在数据上报之后再进行埋点设置，因而数据采集/上报的量远大于可视化埋点。 5、各种埋点场景&amp;埋点建议 客户端数据：页面点击数据，比如：tab栏的点击，某个icon的点击（各入口点击对比使用情况，统计页面点击行为的转化漏斗）。 服务端数据：安装数据，下载后安装情况；内容数据，比如某个视频内容 曝光/展示/播放数据；搜索内容。 5.1、以视频产品为例的一次埋点过程： 明确产品动态，梳理数据需求； 当前为一个视频社区软件，增加了舞蹈跟拍功能，用户可以根据不用的舞蹈来进行拍摄（运营同学对舞蹈进行了分类，主打几个舞蹈），目的是为了给用户提供低成本创造视频内容的方式。 基于上述的产品目的，期望能了解： 该功能的使用情况（uv，pv，使用过程漏斗）; 生产的视频情况（视频数,视频的互动情况），是否能实现促进内容生产带动社区氛围的目标。 5.2、数据需求转化为指标&amp;埋点，并与数据开发进行讨论； 功能使用uv、pv； 对其他拍摄功能的影响； 可以服务端打点，也可以客户端打点，但因为视频社区的基于内容的互动行为基本都在服务端，所以建议服务端打点。 拍摄流程的转化漏斗；拍摄流程主要是页面的点击过程，故使用客户端埋点，并记录uv，pv。 跟拍视频的播放、点赞、评论、分享、关注、二次被跟拍的情况； 跟拍舞蹈的类型，明确用户是否偏向于某个类型的舞蹈跟拍； 服务端，基于内容的互动行为基本都在服务端。 5.3、版本上线； 按照预期进行数据分析，产品迭代复盘。数据分析过程，注意查看是否与预期相符，是否有优化点。","link":"","tags":[{"name":"杂文","slug":"杂文","permalink":"https://victorblog.github.io/tags/%E6%9D%82%E6%96%87/"}]},{"title":"SSM项目的构建","date":"2016-10-03T02:06:00.000Z","path":"2016/10/03/SSM项目的构建/","text":"一、SSM项目的构建1、创建一个maven工程 eclipse创建maven工程后，pom.xml报错：web.xml is missing and is set to true 解决问题：右击项目–&gt;java EE tools–&gt;Generate Deployment Descriptor Stub。 系统会自动创建一个web.xml文件放到src/main/webapp/WEB_INF下。 注：xml文件格式化快捷键，Ctrl+Shift+F,为了避免和搜狗拼音的简体繁体冲突，可以勾掉搜狗的快捷键。 2、引入项目依赖的jar包 spring springmvc mybatis 数据库连接池，驱动包 其他 1、引入Spring相关包的操作1、maven中央仓库2、引入springmvc的包123456&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 3、引入Spring-jdbc的包123456&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;4.3.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 4、引入Spring面向切面编程模块Spring aspects123456&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-aspects --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.3.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 2、引入MyBatis相关包的操作1、引入mybatis的包123456&lt;!-- https://mvnrepository.com/artifact/org.mybatis/mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.2&lt;/version&gt;&lt;/dependency&gt; 2、引入mybatis整合Spring的适配包123456&lt;!-- https://mvnrepository.com/artifact/org.mybatis/mybatis-spring --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 3、引入数据库连接池以及驱动1、引入c3p0数据库连接池的包123456&lt;!-- https://mvnrepository.com/artifact/c3p0/c3p0 --&gt;&lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt;&lt;/dependency&gt; 2、引入mysql8.0.15的驱动包查看本地mysql的版本，引入相应的驱动包 首先进入mysql 12mysql -uroot -prootstatus; 123456&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt;&lt;/dependency&gt; 4、其他模块由于要开发web项目，先把常用的，jstl,servlet-api,junit先引入进来 1、引入jstl包123456&lt;!-- https://mvnrepository.com/artifact/jstl/jstl --&gt;&lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 2、引入servlet-api包1234567&lt;!-- https://mvnrepository.com/artifact/javax.servlet/servlet-api --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 我们知道tomcat服务器里面有servlet包，但是不引入的话jsp页面会报错，所以加入provided标签表示人家已经提供的。也即是当项目发布到服务器上的时候，这个包会被自动剔除掉。 3、引入junit单元测试包1234567&lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 3、引入BootStrap前端框架bootstrap中文官网 1、下载bootstrap然后解压。2、在webapp目录下建立一个folder命名为叫static文件夹 3、把解压的bootstrap-3.3.7-dist复制到新建的static目录下4、新建一个index.jsp引入bootstrap样式和基本文件bootstrap官网 12&lt;link href=\"static/bootstrap-3.3.7-dist/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;&lt;script src=\"static/bootstrap-3.3.7-dist/js/bootstrap.min.js\"&gt;&lt;/script&gt; 4、编写ssm整合的关键配置文件1、web.xml配置文件的编写1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" id=\"WebApp_ID\" version=\"2.5\"&gt; &lt;display-name&gt;ssm-crud&lt;/display-name&gt; &lt;!-- 1、启动spring的容器(快捷键：alt+/) --&gt; &lt;!-- needed for ContextLoaderListener --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- Bootstraps the root web application context before servlet initialization --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 2、springmvc的前端控制器，拦截所有请求 --&gt; &lt;!-- The front controller of this Spring Web application, responsible for handling all application requests --&gt; &lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 可以不指定配置文件路径，但是必须要在与web.xml同级目录下 有一个springmvc配置文件，与&lt;servlet-name&gt;标签属性名-servlet.xml的配置文件 eg:dispatcherServlet-servlet.xml &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;location&lt;/param-value&gt; &lt;/init-param&gt; --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;!-- 让它拦截所有请求 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- 3、配上springmvc中带的字符编码过滤器 Ctrl+Shift+T， 输入CharacterEncodingFilter,打开对应的class文件。 当过滤器有多个，这个肯定有先后顺序的。 ******注意：一定要把字符编码过滤器放在所有过滤器之前。****** --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;!-- 右键选择，copy qualified name复制全类名 --&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;!-- 在初始化参数的时候，给它指定有一个属性，这个属性在CharacterEncodingFilter 叫encoding的属性，指定我们要用的字符编码集用utf-8 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- springmvc4中的CharacterEncodingFilter， 多了两个属性：forceRequestEncoding，forceResponseEncoding --&gt; &lt;!-- 设置这下面两个属性都为true，让它强制请求和响应编码为utf-8 --&gt; &lt;init-param&gt; &lt;param-name&gt;forceRequestEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceResponseEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;!-- 过滤所有请求 --&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- 4、使用Rest风格的URI，而提交页面的请求， 是发不出PUT，DELETE这样的请求的。所以还需要一个过滤器HiddenHttpMethodFilter 它会将页面的post请求转为指定的delete或者put请求。 --&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;/web-app&gt; 2、springmvc配置文件的编写123456789101112131415161718192021222324252627282930313233&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.3.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd\"&gt; &lt;!--SpringMVC的配置文件，包含网站跳转逻辑的控制配置 --&gt; &lt;!-- 1、配置包扫描，让它只扫描@Controller注解的 --&gt; &lt;context:component-scan base-package=\"com.victor\" use-default-filters=\"false\"&gt; &lt;!-- 让它只扫描控制器 --&gt; &lt;context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt; &lt;/context:component-scan&gt; &lt;!-- 2、配置视图解析器，方便页面返回解析 --&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/views/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 两个标准配置 --&gt; &lt;!-- 3、将springmvc不能处理的请求交给tomcat 这样实现了动态，静态资源都能访问成功了 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 4、能支持springmvc更高级的一些功能，比如JSR303教研， 快捷的ajax，更重要的是来映射动态请求--&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt;&lt;/beans&gt; 3、spring的配置文件1、applicationContext.xml配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.3.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.3.xsd\"&gt; &lt;!-- 1、在spring的配置文件中，让Spring不扫控制器。 --&gt; &lt;context:component-scan base-package=\"com.victor\"&gt; &lt;context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt; &lt;/context:component-scan&gt; &lt;!-- spring的配置文件：这里主要配置和业务逻辑有关的 /ssm-crud/src/main/resources/dbconfig.properties--&gt; &lt;!-- 数据源，事务控制 ，xxx--&gt; &lt;!-- ================================一、数据源配置============================================== --&gt; &lt;!--2、context:property-placeholder用来引入外部配置文件 --&gt; &lt;context:property-placeholder location=\"classpath:dbconfig.properties\"/&gt; &lt;!--3、配置c3p0数据源(用alt+/快捷键进行补全) --&gt; &lt;bean id=\"pooledDataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;!--这些数据库设置，我们都不是写死的， 因此创建一个dbconfig.properties文件 为了不跟其他配置文件混乱，跟数据源有关的都加上jdbc.的前缀， --&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.jdbcUrl&#125;\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driverClass&#125;\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.user&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- ========================================================================================== --&gt; &lt;!-- =========================二、MyBatis整合配置====================================================== --&gt; &lt;!-- 4、配置mybatis的整合 --&gt; &lt;!-- 可以帮我们创建sqlSessionFactory --&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;!-- 指定mybatis全局配置文件的位置--&gt; &lt;property name=\"configLocation\" value=\"classpath:mybatis-config.xml\"&gt;&lt;/property&gt; &lt;!-- 指定mybatis的数据源引用文件 --&gt; &lt;property name=\"dataSource\" ref=\"pooledDataSource\"&gt;&lt;/property&gt; &lt;!-- 指定mybatis的mapper文件的位置 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath:mapper/*.xml\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 5、配置扫描器，将mybatis接口的实现加入到ioc容器中 因为我们知道mybatis的接口的实现是一个代理对象，需要加到ioc容器中 --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;!-- 扫描所有的dao接口的实现，加入到ioc容器中--&gt; &lt;property name=\"basePackage\" value=\"com.victor.crud.dao\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- ========================================================================================== --&gt; &lt;!-- ===============================三、事务控制的配置（非常重要）======================================================= --&gt; &lt;!-- 6、事务控制的配置 --&gt; &lt;!-- 数据源的开启关闭回滚操作，我们用事务管理器来做 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 控制住数据源 --&gt; &lt;!-- 告诉项目中用的数据源是哪个 --&gt; &lt;property name=\"dataSource\" ref=\"pooledDataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 7、开启基于注解的事务 ，或者使用xml配置形式的事务 一般我们推荐：比较重要的事务都是使用配置xml文件的形式。 --&gt; &lt;aop:config&gt; &lt;!-- 切入点表达式（也就是想要切入到哪些里面进行事务控制） 首先是返回值，返回值类型：*为所有 com.victor.crud.service..*(..):表示service包下的所有类，所有方法都来控制事务,括号里面的双点：表示这个方法里的参数任意多也行 ..：表示即使这个包下还有子包也行 --&gt; &lt;aop:pointcut expression=\"execution(* com.victor.crud.service..*(..))\" id=\"txPoint\"/&gt; &lt;!-- 配置事务增强(需要引入tx名称空间) --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"txPoint\"/&gt; &lt;/aop:config&gt; &lt;!--8、配置事务增强(也就是事务如何切入) --&gt; &lt;!-- 关键是跟事务管理器又产生什么联系的呢？ 其实是我们事务增强配置的时候有一个属性transaction-manager， 它的值默认取值叫transactionManager，而我们的事务管理器的id正好是transactionManager 如果事务管理器的id改了，也一定要把id复制粘贴到事务增强处。 意思就是我们用这个事务管理器来控制事务，控制事务的细节，切哪些方法在&lt;aop:point&gt;标签配置处指定。 哪些方法切入以后该怎么办，在&lt;tx:advice&gt;标签配置处指定。 --&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;!-- *表示这个切入点切入的所有方法，都是事务方法 --&gt; &lt;tx:method name=\"*\"/&gt; &lt;!-- 表示以get开始的所有方法(我们可以认为以get开始的方法都是查询，设置一个属性read-only=true,来进行优化)--&gt; &lt;tx:method name=\"get*\" read-only=\"true\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- ========================================================================================== --&gt; &lt;!--Spring配置文件的核心点： 数据源，与mybatis的整合，事务控制 --&gt;&lt;/beans&gt; 2、dbconfig.properties文件的写法由于mysql8，driverClass=com.mysql.jdbc.Driver会报错 错误解决：Loading class com.mysql.jdbc.Driver&#39;. This is deprecated. The new driver class iscom.mysql.cj.jdb mysql其他版本:driverClass=com.mysql.jdbc.Driver mysql8以上版本:driverClass=com.mysql.cj.jdbc.Driver 1234jdbc.jdbcUrl=jdbc:mysql://localhost:3306/java?useSSL=falsejdbc.driverClass=com.mysql.cj.jdbc.Driverjdbc.user=victorjdbc.password=root 错误解决：java.sql.SQLNonTransientConnectionException: Public Key Retrieval is not allowed 由于mysql8版本： 1jdbcUrl=jdbc:mysql://localhost:3306/java?useSSL=false&amp;allowPublicKeyRetrieval=true 4、mybatis的配置文件1、mybatis-cfg.xml1234567891011121314&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- 开启驼峰命名规则 --&gt; &lt;settings&gt; &lt;setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/&gt; &lt;/settings&gt; &lt;!-- 类型别名的配置 --&gt; &lt;typeAliases&gt; &lt;package name=\"com.victor.crud.bean\"/&gt; &lt;/typeAliases&gt;&lt;/configuration&gt; 2、利用MyBatis Generator逆向工程MyBatis Generator的官网 引入mybatis generator的依赖 123456&lt;!-- https://mvnrepository.com/artifact/org.mybatis.generator/mybatis-generator-core --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt;&lt;/dependency&gt; 给当前工程中创建一个mbg.xml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC \"-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\" \"http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\"&gt;&lt;generatorConfiguration&gt; &lt;context id=\"DB2Tables\" targetRuntime=\"MyBatis3\"&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=\"com.mysql.cj.jdbc.Driver\" connectionURL=\"jdbc:mysql://localhost:3306/java?useSSL=false\" userId=\"victor\" password=\"root\"&gt; &lt;/jdbcConnection&gt; &lt;!-- Java的大数据类型解析 --&gt; &lt;javaTypeResolver &gt; &lt;property name=\"forceBigDecimals\" value=\"false\" /&gt; &lt;/javaTypeResolver&gt; &lt;!-- 指定javaBean生成的位置 windows系统的写法：targetProject=\".\\src\\main\\java\":表示当前工程下的main的java工程下。macOS、linux系统的写法：targetProject=\"./src/main/java\" --&gt; &lt;javaModelGenerator targetPackage=\"com.victor.crud.bean\" targetProject=\"./src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;property name=\"trimStrings\" value=\"true\" /&gt; &lt;/javaModelGenerator&gt; &lt;!-- 指定sql映射文件生成的位置 --&gt; &lt;sqlMapGenerator targetPackage=\"mapper\" targetProject=\"./src/main/resources\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 指定dao接口生成的位置，mapper接口 --&gt; &lt;javaClientGenerator type=\"XMLMAPPER\" targetPackage=\"com.victor.crud.dao\" targetProject=\"./src/main/java\"&gt; &lt;property name=\"enableSubPackages\" value=\"true\" /&gt; &lt;/javaClientGenerator&gt; &lt;!-- table标签指定每个表的生成策略 tableName=\"tbl_emp\":指定表名，必须的属性。 domainObjectName=\"Employee\"：指定生成的javabean的类名 --&gt; &lt;table tableName=\"tbl_emp\" domainObjectName=\"Employee\"&gt;&lt;/table&gt; &lt;table tableName=\"tbl_dept\" domainObjectName=\"Department\"&gt;&lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 如何生成呢 因为我们是java程序+xml配置文件的生成方式 生成逆向工程 运行这个测试类main方法生成就行。 12345678910111213141516171819202122232425262728293031package com.victor.crud.test;import java.io.File;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.mybatis.generator.api.MyBatisGenerator;import org.mybatis.generator.config.Configuration;import org.mybatis.generator.config.xml.ConfigurationParser;import org.mybatis.generator.exception.XMLParserException;import org.mybatis.generator.internal.DefaultShellCallback;public class MBGTest &#123; public static void main(String[] args) throws IOException, XMLParserException, Exception &#123; // TODO Auto-generated method stub List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; //指定当前项目下的mybatis-generator的配置文件 File configFile = new File(\"mbg.xml\"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null); System.out.println(\"mybatis-generator successfully!\"); &#125;&#125; 5、测试mapper1、插入批量数据 首先是applicationContext.xml修改 1234567&lt;!-- 配置一个可以执行批量操作的sqlSession --&gt;&lt;bean id=\"sqlSession\" class=\"org.mybatis.spring.SqlSessionTemplate\"&gt;&lt;!-- ref指向，mybatis配置好的sqlSessionFactory --&gt; &lt;constructor-arg name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\"&gt;&lt;/constructor-arg&gt; &lt;!-- mybatis执行器默认类型不是bathc批量的，我们改成批量的 --&gt; &lt;constructor-arg name=\"executorType\" value=\"BATCH\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 测试用例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.victor.crud.test;import java.util.UUID;import org.apache.ibatis.session.SqlSession;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import com.victor.crud.bean.Department;import com.victor.crud.bean.Employee;import com.victor.crud.dao.DepartmentMapper;import com.victor.crud.dao.EmployeeMapper;/** * 测试dao层的工作 * @author victor *推荐spring的项目就可以使用Spring的单元测试，可以自动注入我们需要的组件 *1、导入SpringTest模块的依赖 *2、使用@ContextConfiguration注解指定Spring配置文件的位置，可以自动帮我们创建ioc容器。 *3、@RunWith是Junit里的注解，可以指定在运行单元测试的时候，用哪个来运行。 *我们现在用Spring的单元测试模块，指定value值为SpringJUnit4ClassRunner.class *4、我们要用哪些组件，直接@Autowried要使用的哪些组件即可。 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations= &#123;\"classpath:applicationContext.xml\"&#125;)public class MapperTest &#123; @Autowired DepartmentMapper departmentMapper; @Autowired EmployeeMapper employeeMapper; //自动注入一个批量的SqlSession @Autowired SqlSession sqlSession; /** * 测试DepartmentMapper */ @Test public void testCRUD() &#123; /*原生的单元测试 * //1、创建SpringIOC容器 ApplicationContext ioc=new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //2、容器中获取mapper ioc.getBean(DepartmentMapper.class); */ System.out.println(departmentMapper); //1、插入几个部门// departmentMapper.insertSelective(new Department(null,\"技术部\"));// departmentMapper.insertSelective(new Department(null,\"测试部\")); //2、生成员工，测试员工插入// employeeMapper.insertSelective(new Employee(null, \"Tom\", \"M\", \"1397743321@qq.com\", 1)); //批量操作可以用个for循环，但是效率不高 /** * for()&#123; * employeeMapper.insertSelective(new Employeenew Employee(null, \"Tom\", \"M\", \"1397743321@qq.com\", 1)); * &#125; */ //3、批量插入多个员工；批量，使用可以执行批量操作的sqlSession EmployeeMapper mapper=sqlSession.getMapper(EmployeeMapper.class); for(int i=0;i&lt;1000;i++) &#123; String uid=UUID.randomUUID().toString().substring(0, 5)+i; mapper.insertSelective(new Employee(null,uid,\"M\",uid+\"@qq.com\",1)); &#125; System.out.println(\"BATCH Successfully!\"); &#125;&#125; 6、查询页面的展示1、访问index.jsp页面URI：/emps 2.index.jsp页面发送出查询员工列表请求123&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt;&lt;&lt;jsp:forward page=\"/emps\"&gt;&lt;/jsp:forward&gt; 3、EmployeeController来接受请求，查出员工数据 建立一个EmployeeController来处理员工的CRUD请求 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.victor.crud.controller;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import com.github.pagehelper.PageHelper;import com.github.pagehelper.PageInfo;import com.victor.crud.bean.Employee;import com.victor.crud.service.EmployeeService;/** * 处理员工的CRUD请求 * @author victor * */@Controllerpublic class EmployeeController &#123; @Autowired EmployeeService employeeService; /** * 查询员工数据(分页查询) * @return */ @RequestMapping(\"/emps\") public String getEmps(@RequestParam(value=\"pn\",defaultValue=\"1\")Integer pn, Model model) &#123; //这不是一个分页查询； //引入PageHelper分页插件 //在查询之前只需要调用 PageHelper.startPage(pn, 5);//从pn页开始查，每一页5条数据 //startPage后面紧跟的这个查询就是一个分页查询 List&lt;Employee&gt; emps=employeeService.getAll(); //用PageInfo包装查询后的结果，只要将pageInfo交给页面即可。 //pageInfo封装了详细的分析信息，包括有我们查询出来的数据。 PageInfo page = new PageInfo(emps,5);//传入连续显示的5页 //用Model，或者Map来给它里面添加数据，都会被带给页面，它会放到请求Request域中。 model.addAttribute(\"pageInfo\", page); //page对象的getNavigatepageNums可以拿到我们传入连续显示的信息 //page.getNavigatepageNums(); return \"list\"; &#125;&#125; 因为我们要使用分页功能，要在pom.xml引入PageHelper的依赖 PageHelper的官网 123456&lt;!-- 引入PageHelper分页插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.0.0&lt;/version&gt; &lt;/dependency&gt; 为了让pageHelper起作用，还要在mybatis-config.xml配置文件中配置 1234&lt;!-- 引入分页插件PageHelper --&gt; &lt;plugins&gt; &lt;plugin interceptor=\"com.github.pagehelper.PageInterceptor\"&gt;&lt;/plugin&gt; &lt;/plugins&gt; 建立EmployeeService来查询所有员工数据 1234567891011121314151617181920212223package com.victor.crud.service;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.victor.crud.bean.Employee;import com.victor.crud.dao.EmployeeMapper;@Servicepublic class EmployeeService &#123; @Autowired EmployeeMapper employeeMapper; /** * 查询所有员工数据 * @return */ public List&lt;Employee&gt; getAll()&#123; return employeeMapper.selectByExampleWithDept(null); &#125;&#125; 为了保证页面能获取数据，先进行SpringMVC请求的测试 建立一个MVCTest测试类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.victor.crud.test;import java.util.List;import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mock.web.MockHttpServletRequest;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import org.springframework.test.context.web.WebAppConfiguration;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.MvcResult;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;import org.springframework.test.web.servlet.setup.MockMvcBuilders;import org.springframework.ui.ModelMap;import org.springframework.web.context.WebApplicationContext;import org.springframework.web.servlet.ModelAndView;import com.github.pagehelper.PageInfo;import com.victor.crud.bean.Employee;/** * 使用Spring测试模块提供的测试请求功能，测试crud请求的正确性 * @author victor * */@RunWith(SpringJUnit4ClassRunner.class)@WebAppConfiguration/** * 测试Springmvc只有Spring配置文件是不够的，还要有springmvc的配置文件 *ssm-crud/src/main/webapp/WEB-INF/dispatcherServlet-servlet.xml * */@ContextConfiguration(locations= &#123;\"classpath:applicationContext.xml\",\"file:src/main/webapp/WEB-INF/dispatcherServlet-servlet.xml\"&#125;)public class MVCTest &#123; /** * @Autowired这个只能自动注入ioc容器里面的，WebApplicationContext是一个ioc容器怎么能自己@Autowired呢？ * 所以还需要一个注解@WebAppConfiguration,就可以把web的ioc容器拿过来。 */ @Autowired WebApplicationContext context;//传入SpringMVC的ioc容器 //虚拟mvc请求，获取到处理结果 MockMvc mockMvc; /** * 每次要用mockMvc，这个方法都要初始化以下 */ @Before public void initMockMvc() &#123; mockMvc=MockMvcBuilders.webAppContextSetup(context).build();//mockMvc来模拟mvc请求发送。 &#125; /** * 测试会报一个错：java.lang.NoClassDefFoundError: javax/servlet/SessionCookieConfig * 原因是Spring4.0测试的时候，需要servlet3.0的支持，修改pom.xml文件引入新的servlet就行。 * @throws Exception */ @Test public void testPage() throws Exception &#123; //模拟请求拿到返回值 MvcResult result= mockMvc.perform(MockMvcRequestBuilders.get(\"/emps\").param(\"pn\", \"1\")) .andReturn(); /**在Controller中的model.addAttribute(\"pageInfo\", page);放在请求域中 * 请求成功以后，请求域中会有pageInfo：我们可以取出pageInfo来进行验证是否正确。 */ ModelAndView modelAndView = result.getModelAndView(); ModelMap modelMap = modelAndView.getModelMap(); System.out.println(modelMap.toString()); System.out.println(\"打印modelMap完毕===============\"); MockHttpServletRequest request = result.getRequest(); PageInfo pi=(PageInfo) request.getAttribute(\"pageInfo\"); System.out.println(\"当前页码：\"+pi.getPageNum()); System.out.println(\"总页码：\"+pi.getPages()); System.out.println(\"总记录数：\"+pi.getTotal()); System.out.println(\"在页面需要连续显示的页码：\"); int[] navigatepageNums = pi.getNavigatepageNums();//ctrl+1然后enter快速生成返回值对象 for(int i:navigatepageNums) &#123; System.out.println(\" \"+i); &#125; //获取员工数据 List&lt;Employee&gt; list = pi.getList(); for(Employee employee:list) &#123; System.out.println(\"ID：\"+employee.getEmpId()+\"==&gt;Name:\"+employee.getEmpName()); &#125; &#125;&#125; 测试的时候会报错 错误信息：java.lang.NoClassDefFoundError: javax/servlet/SessionCookieConfig 解决问题：原因是Spring4.0测试的时候，需要servlet3.0的支持，修改pom.xml文件引入新的servlet就行。 修改pom.xml配置文件,引入javax.servlet-api3.0.1的依赖 123456&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 4、来到list.jsp页面进行展示bootstrap的官方文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt;&lt;%@taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\"%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;员工列表&lt;/title&gt;&lt;% pageContext.setAttribute(\"APP_PATH\", request.getContextPath());%&gt;&lt;!-- web路径：不以/开始的相对路径，找资源，是以当前资源的路径为基准。相对路径容易出问题。以/开始的相对路径，找资源，是以服务器的路径为标准(http://localhost:3306);需要加上项目名 也就是加上项目名http://localhost:3306/crud 而加/以后，项目名，我们也不是写死的，可以写上一小段java代码 --&gt;&lt;!-- 引入jQuery--&gt;&lt;script src=\"http://libs.baidu.com/jquery/2.1.4/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 引入样式 --&gt;&lt;link href=\"$&#123;APP_PATH&#125;/static/bootstrap-3.3.7-dist/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;&lt;script src=\"$&#123;APP_PATH&#125;/static/bootstrap-3.3.7-dist/js/bootstrap.min.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 搭建显示页面 --&gt; &lt;div class=\"container\"&gt; &lt;!-- 标题行 --&gt; &lt;div class=\"row\"&gt; &lt;div class=\"col-md-12\"&gt; &lt;h1&gt;SSM-CRUD&lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- 按钮 --&gt; &lt;div class=\"row\"&gt; &lt;div class=\"col-md-4 col-md-offset-8\"&gt; &lt;button class=\"btn btn-primary\"&gt; &lt;span class=\"glyphicon glyphicon-search\" aria-hidden=\"true\"&gt;&lt;/span&gt;查询 &lt;/button&gt; &lt;button class=\"btn btn-success\"&gt; &lt;span class=\"glyphicon glyphicon-plus\" aria-hidden=\"true\"&gt;&lt;/span&gt;新增 &lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;!--显示表格数据 --&gt; &lt;div class=\"row\"&gt; &lt;div class=\"col-md-12\"&gt; &lt;table class=\"table table-hover\"&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;empName&lt;/th&gt; &lt;th&gt;gender&lt;/th&gt; &lt;th&gt;email&lt;/th&gt; &lt;th&gt;deptName&lt;/th&gt; &lt;th&gt;操作&lt;/th&gt; &lt;/tr&gt; &lt;!-- 拿出pageInfo里面包含的list，这个list是我们查询到的数据 每一个就叫emp员工。然后取出的信息放在tr标签中 --&gt; &lt;c:forEach items=\"$&#123;pageInfo.list&#125;\" var=\"emp\"&gt; &lt;tr&gt; &lt;th&gt;$&#123;emp.empId&#125;&lt;/th&gt; &lt;th&gt;$&#123;emp.empName &#125;&lt;/th&gt; &lt;th&gt;$&#123;emp.gender==\"M\"?\"男\":\"女\"&#125;&lt;/th&gt; &lt;th&gt;$&#123;emp.email &#125;&lt;/th&gt; &lt;th&gt;$&#123;emp.department.deptName&#125;&lt;/th&gt; &lt;th&gt; &lt;button class=\"btn btn-info btn-sm\"&gt; &lt;span class=\"glyphicon glyphicon-pencil\" aria-hidden=\"true\"&gt;&lt;/span&gt;编辑 &lt;/button&gt; &lt;button class=\"btn btn-danger btn-sm\"&gt; &lt;span class=\"glyphicon glyphicon-trash\" aria-hidden=\"true\"&gt;&lt;/span&gt;删除 &lt;/button&gt; &lt;/th&gt; &lt;/tr&gt; &lt;/c:forEach&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;!-- 显示分页信息 --&gt; &lt;div class=\"row\"&gt; &lt;!-- 分页文字信息 --&gt; &lt;div class=\"col-md-6\"&gt;当前$&#123;pageInfo.pageNum&#125;页,总$&#123;pageInfo.pages &#125;,总$&#123;pageInfo.total &#125;条记录&lt;/div&gt; &lt;!-- 分页条信息 --&gt; &lt;div class=\"col-md-6\"&gt; &lt;nav aria-label=\"Page navigation\"&gt; &lt;ul class=\"pagination\"&gt; &lt;li&gt;&lt;a href=\"$&#123;APP_PATH&#125;/emps/?pn=1\"&gt;首页&lt;/a&gt;&lt;/li&gt; &lt;c:if test=\"$&#123;pageInfo.hasPreviousPage&#125;\"&gt; &lt;li&gt;&lt;a href=\"$&#123;APP_PATH&#125;/emps?pn=$&#123;pageInfo.pageNum-1&#125;\" aria-label=\"Previous\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;laquo;&lt;/span&gt; &lt;!-- 上一页 --&gt; &lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:forEach items=\"$&#123;pageInfo.navigatepageNums &#125;\" var=\"page_Num\"&gt; &lt;c:if test=\"$&#123;page_Num==pageInfo.pageNum &#125;\"&gt; &lt;li class=\"active\"&gt;&lt;a href=\"#\"&gt;$&#123;page_Num&#125;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test=\"$&#123;page_Num!=pageInfo.pageNum &#125;\"&gt; &lt;li class&gt;&lt;a href=\"$&#123;APP_PATH&#125;/emps/?pn=$&#123;page_Num&#125;\"&gt;$&#123;page_Num&#125;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:forEach&gt; &lt;c:if test=\"$&#123;pageInfo.hasNextPage &#125;\"&gt; &lt;li&gt;&lt;a href=\"$&#123;APP_PATH&#125;/emps?pn=$&#123;pageInfo.pageNum+1&#125;\" aria-label=\"Next\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;raquo;&lt;/span&gt; &lt;!-- 下一页 --&gt; &lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;li&gt;&lt;a href=\"$&#123;APP_PATH&#125;/emps?pn=$&#123;pageInfo.pages&#125;\"&gt;末页&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/nav&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 二、Lombok的使用1、引入依赖jar1234567&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.8&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; 2、使用注解 @Data,@ToString用在JavaBean 123456789101112@Data@ToStringpublic class MyLoadImg &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = \"id\") private Long id; private String userName; private String url; private String img; private Boolean flag;&#125; @Slf4j用来打印日志 1234567@Slf4jpublic class MyService&#123; @Test public void test()&#123; log.info(\"打印日志：&#123;&#125;\",hello); &#125;&#125;","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Maven作用域scope分类","date":"2016-09-20T10:30:47.000Z","path":"2016/09/20/Maven依赖的作用域scope/","text":"1、Maven中依赖作用域scope的分类Maven默认的依赖配置项中，scope的默认值是compile 1.1、compile默认是compile，也就是什么都不配置，就意味着compile，表示被依赖的项目需要参与当前项目的编译，后续的测试，运行周期也会使用到。是一个比较强的依赖，打包的时候也需要含进去。 1.2、runtime表示依赖项目无需参与项目的编译，不过后期的测试和运行周期需要参与。与compile相比，只是跳过编译而已。比较常见的是比较常见的如JSR-xxxx的实现，对应的API jar是compile的，具体实现是runtime的，compile只需要知道接口就足够了。oracle jdbc驱动架包就是一个很好的例子，一般scope为runntime。另外runntime的依赖通常和optional搭配使用，optional为true。我可以用A实现，也可以用B实现。 1.3、providedprovided意味着打包的时候可以不用包进去，别的设施(Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。 1.4、test该依赖仅仅参与测试相关的内容，包括测试用例的编译和执行，比如定性的Junit。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://victorblog.github.io/tags/Java/"}]},{"title":"Linux常用命令","date":"2015-11-01T02:06:00.000Z","path":"2015/11/01/Linux常用的操作命令/","text":"Linux常用的操作命令记住：Linux系统里一切都是文件 更新系统 12sudo apt updatesudo apt upgrade 查看历史命令 12#清除历史history -c 查看某个命令的所在位置 1which ps 查看某个命令的使用文档 1man ps 查看端口占用情况： 1lsof -i:8080 查看某个进程： 1ps aux | grep \"chrome\" 杀死某个进程： 1kill -9 pid 展示MD5： 1echo \"danruiqing\" | md5sum 查看系统时间： 12date +%sdate -d\"1 week ago\" +\"%F %H:%M:%S\" 查看路由表 1netstat -rn 路由表添加路由 12#添加路由sudo route add 10.249.7.38/32 dev wlp2s0 访问远程服务器 1ssh danruiqing@xxx.com -p 21 查看本地网络配置 1ifconfig -a maven清楚并忽略test 1mvn clean package -Dmaven.test.skip=true 查询命令的使用 12#grep \"123\"查询的结果作为后面的输入，管道grep \"123\" | grep \"45\" | grep \"tom\" 环境变量配置 1234567vim .bashrcexport JAVA_HOME=/usr/java/jdk1.8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/Libexport PATH=.$&#123;JAVA_HOME&#125;/bin:$PATH:wqsource .bashrc tar包的解压打包 12tar -zxvf xxx.tartar -zcvf java_demo.tar java_demo zip包的解压打包 12zip java_demo.zip java_demounzip -l example.zip 查看日志文件 12tail -f java.logcat .bashrc 修改hosts文件 1234#为了线下稳定访问服务，配置hostssudo gedit /etc/hosts#保存并重启网络sudo /etc/init.d/networking restart","link":"","tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://victorblog.github.io/tags/DevOps/"}]},{"title":"病毒与细菌","date":"2015-06-11T02:06:00.000Z","path":"2015/06/11/病毒与细菌/","text":"1、构造不同 细菌：原核生物的一种，主要特点：没有核膜，遗传物质分散在细胞质内一个相对固定的区域内，叫做核区，拟核。细菌的外边包裹着一层细胞壁，成分为多糖聚合而成，也叫肽聚糖 病毒：构造简单，外面是一层蛋白质，叫做病毒外壳，蛋白质外壳内部包裹着病毒的遗传物质，可以是DNA，RNA。病毒自己不能完成新陈代谢，也不能完成繁殖，需要寄生在其他细胞内完成。 2、繁殖方式不同2.1、细菌 细菌以无性方式进行繁殖，最主要的方式是以二分裂的无性繁殖方式。一个细菌细胞细胞壁横向分裂，形成两个子代细胞，在分裂的时候可以产生遗传重组。 单个细胞也会通过一下几种方式发生遗传变异：突变（细胞自身的遗传基因发生随机改变），转化（无修饰的DNA从一个细菌转移到溶液中另一个细菌中，并成功整合到该细菌DNA或者质粒上，让它具有新的特征），转染（病毒的或细菌的DNA，或者两者的DNA，通过噬菌体这种载体转移到另一个细菌中），细菌接合（一个细菌的DNA通过两细菌间形成的特殊的蛋白质结构，接合菌毛，转移到另一个细菌） 细菌可以通过这些方式获得基因片段，通过分裂，将重组的基因组传给后代 2.2、病毒 由于病毒是非细胞的，无法通过细胞分裂的方式来完成数量增长，他们是利用宿主细胞内的代谢工具来合成自身的拷贝，并完成病毒组装。 病毒并不是严格生物学意义上的繁殖，而且每次释放个体数量巨大，故而称为–繁殖。不同的病毒之间的生命周期的差异很大，但大致可以分成6个阶段：附着，入侵，脱壳，合成，组装，释放。 3、治疗手段不同3.1、细菌的治疗手段 很多致病细菌均可以通过抗生素治疗，现阶段人类经常用的有：青霉素，头孢类，万古霉素，红霉素，四环素都属于抗生素类，绝大多数病毒的感染我们束手无策。 3.2、病毒的治疗手段 盐酸吗啉胍（病毒灵） 能抑制病毒的DNA和RNA聚合酶，从而抑制病毒繁殖，在人胚肾细胞上，1%浓度对DNA病毒（腺病毒，疱疹病毒）和RNA病毒（埃可病毒）都有明显抑制作用，对病毒繁殖周期各个阶段均有抑制作用，对游离病毒颗粒无直接作用。 利巴韦林 利巴韦林是合成的核苷类抗病毒药，利巴韦林对呼吸道合胞病毒（RSV）具有选择性抑制作用。","link":"","tags":[{"name":"杂文","slug":"杂文","permalink":"https://victorblog.github.io/tags/%E6%9D%82%E6%96%87/"}]}]